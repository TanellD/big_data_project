{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0635285d-5589-4eeb-9091-9dd2143b3492",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Connect to Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4d77bfb-132e-422c-9933-69d0df5685dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/03 12:42:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/03 12:42:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/05/03 12:42:10 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/05/03 12:42:11 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "25/05/03 12:42:11 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Add here your team number teamx\n",
    "team = 19\n",
    "\n",
    "# location of your Hive database in HDFS\n",
    "# warehouse = \"project/hive/warehouse\"\n",
    "warehouse = \"/user/team19/project/warehouse\"\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"{} - spark ML\".format(team))\\\n",
    "        .master(\"yarn\")\\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hadoop-02.uni.innopolis.ru:9883\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse)\\\n",
    "        .config(\"spark.sql.avro.compression.codec\", \"snappy\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbd810cc-d172-43a6-b017-57203d8a6970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_402\"\n",
      "OpenJDK Runtime Environment (build 1.8.0_402-b06)\n",
      "OpenJDK 64-Bit Server VM (build 25.402-b06, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "# pip freeze\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab0a0875-0d8c-4f48-a65b-230e11c46d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://hadoop-01.uni.innopolis.ru:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>19 - spark ML</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f6f3bc9ea10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbc89ee-c475-47d1-b000-a800563a8870",
   "metadata": {},
   "source": [
    "# list Hive databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2631136-e462-4815-a807-d1bd4bc3f7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Database(name='default', catalog='spark_catalog', description='Default Hive database', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/apps/hive/warehouse'), Database(name='retake1', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team36/retakedb1'), Database(name='root_db', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/root/root_db'), Database(name='show', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team36/data2'), Database(name='team0_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team0/project/hive/warehouse'), Database(name='team11_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team11/project/hive/warehouse'), Database(name='team12_hive_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team12/project/hive/warehouse'), Database(name='team12_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team12/project/hive/warehouse'), Database(name='team13_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team13/project/hive/warehouse'), Database(name='team14_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team14/project/hive/warehouse'), Database(name='team15_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team15/project/hive/warehouse'), Database(name='team16_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team16/project/hive/warehouse'), Database(name='team17_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team17/project/hive/warehouse'), Database(name='team18_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team18/project/hive/warehouse'), Database(name='team19_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team19/project/hive/warehouse'), Database(name='team1_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team1/project/hive/warehouse'), Database(name='team20_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team20/project/hive/warehouse'), Database(name='team21_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team21/project/hive/warehouse'), Database(name='team21_projectdb_v2', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team21/project/hive/warehouse'), Database(name='team21_projectdb_v3', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team21/project/hive/warehouse'), Database(name='team22_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team22/project/hive/warehouse'), Database(name='team23_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/hive/warehouse'), Database(name='team24_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team24/project/hive/warehouse'), Database(name='team25_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team25/project/hive/warehouse'), Database(name='team26_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team26/project/hive/warehouse'), Database(name='team27_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team27/project/hive/warehouse'), Database(name='team28_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team28/project/hive/warehouse'), Database(name='team29_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team29/project/hive/warehouse'), Database(name='team2_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team2/project/hive/warehouse'), Database(name='team30_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team30/project/hive/warehouse'), Database(name='team31_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team31/project/hive/warehouse'), Database(name='team34_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team34/project/hive/warehouse'), Database(name='team36_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team36/project_table'), Database(name='team36db', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team36/hive_db'), Database(name='team37_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team37/project/hive/warehouse'), Database(name='team38_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team38/project/hive/warehouse'), Database(name='team39_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team39/project/hive/warehouse'), Database(name='team3_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team3/project/hive/warehouse'), Database(name='team4_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team4/project/hive/warehouse'), Database(name='team5_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team5/project/hive/warehouse'), Database(name='team6_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team6/project/hive/warehouse'), Database(name='team7_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team7/project/hive/warehouse'), Database(name='team8_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team8/project/hive/warehouse'), Database(name='team9_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team9/project/hive/warehouse'), Database(name='teamx_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team13/project/hive/warehouse'), Database(name='testdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/apps/hive/warehouse/testdb.db')]\n",
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|             default|\n",
      "|             retake1|\n",
      "|             root_db|\n",
      "|                show|\n",
      "|     team0_projectdb|\n",
      "|    team11_projectdb|\n",
      "|team12_hive_proje...|\n",
      "|    team12_projectdb|\n",
      "|    team13_projectdb|\n",
      "|    team14_projectdb|\n",
      "|    team15_projectdb|\n",
      "|    team16_projectdb|\n",
      "|    team17_projectdb|\n",
      "|    team18_projectdb|\n",
      "|    team19_projectdb|\n",
      "|     team1_projectdb|\n",
      "|    team20_projectdb|\n",
      "|    team21_projectdb|\n",
      "| team21_projectdb_v2|\n",
      "| team21_projectdb_v3|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(spark.catalog.listDatabases())\n",
    "spark.sql(\"SHOW DATABASES;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69a1fe4-d2d3-4fb0-9533-59f61a917d05",
   "metadata": {},
   "source": [
    "# Specify the input and output features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018f48d1-6fde-4ff5-b7ce-4eed719a7774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9560b235-3f77-4e59-a5fe-f32d9efd7d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|             default|\n",
      "|             retake1|\n",
      "|             root_db|\n",
      "|                show|\n",
      "|     team0_projectdb|\n",
      "|    team11_projectdb|\n",
      "|team12_hive_proje...|\n",
      "|    team12_projectdb|\n",
      "|    team13_projectdb|\n",
      "|    team14_projectdb|\n",
      "|    team15_projectdb|\n",
      "|    team16_projectdb|\n",
      "|    team17_projectdb|\n",
      "|    team18_projectdb|\n",
      "|    team19_projectdb|\n",
      "|     team1_projectdb|\n",
      "|    team20_projectdb|\n",
      "|    team21_projectdb|\n",
      "| team21_projectdb_v2|\n",
      "| team21_projectdb_v3|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5bf565-af4e-49d4-bf45-ca1e1baf971f",
   "metadata": {},
   "source": [
    "# Read hive tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a159eb78-3d54-4302-8937-9dae58a5dbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+----------------+--------------------+-----------+\n",
      "|       namespace|           tableName|isTemporary|\n",
      "+----------------+--------------------+-----------+\n",
      "|team19_projectdb|      hosts_bucketed|      false|\n",
      "|team19_projectdb|listings_partitioned|      false|\n",
      "|team19_projectdb|review_scores_buc...|      false|\n",
      "+----------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"USE team19_projectdb\").show()\n",
    "spark.sql(\"SHOW TABLES\").show()\n",
    "# spark.sql(\"SELECT * FROM <db_name>.<table_name>\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3376ccfd-3045-4c2d-9800-19ea2da702b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hosts_bucketed = spark.read.format(\"avro\").table('team19_projectdb.hosts_bucketed')\n",
    "listings_partitioned = spark.read.format(\"avro\").table('team19_projectdb.listings_partitioned')\n",
    "review_scores_bucketed = spark.read.format(\"avro\").table('team19_projectdb.review_scores_bucketed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4550d52e-bf88-4bb8-a79f-37540e6da717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['host_id',\n",
       " 'host_url',\n",
       " 'host_name',\n",
       " 'host_since',\n",
       " 'host_location',\n",
       " 'host_about',\n",
       " 'host_response_time',\n",
       " 'host_response_rate',\n",
       " 'host_acceptance_rate',\n",
       " 'host_listings_count',\n",
       " 'host_total_listings_count',\n",
       " 'host_verifications']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hosts_bucketed.show()\n",
    "hosts_bucketed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "656cc510-59bd-46c1-a2cd-d618ac9e0a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/03 12:42:33 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+---------+----------+-------------+----------+------------------+------------------+--------------------+-------------------+-------------------------+-------------------+\n",
      "|         host_id|            host_url|host_name|host_since|host_location|host_about|host_response_time|host_response_rate|host_acceptance_rate|host_listings_count|host_total_listings_count| host_verifications|\n",
      "+----------------+--------------------+---------+----------+-------------+----------+------------------+------------------+--------------------+-------------------+-------------------------+-------------------+\n",
      "|71592872.0000000|https://www.airbn...|      Ian|2016-05-12|         NULL|      NULL|within a few hours|       100.0000000|                NULL|          1.0000000|                1.0000000|email,phone,reviews|\n",
      "+----------------+--------------------+---------+----------+-------------+----------+------------------+------------------+--------------------+-------------------+-------------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "hosts_bucketed.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f40641c1-606e-4095-8e26-9a83e4eebd0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'listing_url',\n",
       " 'scrape_id',\n",
       " 'last_scraped',\n",
       " 'name',\n",
       " 'host_id',\n",
       " 'street',\n",
       " 'property_type',\n",
       " 'room_type',\n",
       " 'accommodates',\n",
       " 'bathrooms',\n",
       " 'bedrooms',\n",
       " 'beds',\n",
       " 'price',\n",
       " 'number_of_reviews',\n",
       " 'reviews_per_month',\n",
       " 'city',\n",
       " 'country']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listings_partitioned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9b9f2b9-1e63-4fea-a43f-ec4f24f91228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------+------------+--------------------+----------------+--------------------+-------------+---------------+------------+---------+---------+---------+-----------+-----------------+-----------------+-----------+-------------+\n",
      "|     id|         listing_url|     scrape_id|last_scraped|                name|         host_id|              street|property_type|      room_type|accommodates|bathrooms| bedrooms|     beds|      price|number_of_reviews|reviews_per_month|       city|      country|\n",
      "+-------+--------------------+--------------+------------+--------------------+----------------+--------------------+-------------+---------------+------------+---------+---------+---------+-----------+-----------------+-----------------+-----------+-------------+\n",
      "|8867364|https://www.airbn...|20170502172350|  2017-05-03|Great LA one bedr...|46414548.0000000|Mid-Wilshire, Los...|    Apartment|Entire home/apt|   2.0000000|1.0000000|1.0000000|2.0000000|160.0000000|        0.0000000|             NULL|Los Angeles|United States|\n",
      "+-------+--------------------+--------------+------------+--------------------+----------------+--------------------+-------------+---------------+------------+---------+---------+---------+-----------+-----------------+-----------------+-----------+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "listings_partitioned.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0572f66b-45c8-40d6-9f5a-b11959b080b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['listing_id',\n",
       " 'review_scores_rating',\n",
       " 'review_scores_accuracy',\n",
       " 'review_scores_cleanliness',\n",
       " 'review_scores_checkin',\n",
       " 'review_scores_communication',\n",
       " 'review_scores_location',\n",
       " 'review_scores_value']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_scores_bucketed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de700ec2-bf9e-4ad9-a925-36d9018f3b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+\n",
      "|listing_id|review_scores_rating|review_scores_accuracy|review_scores_cleanliness|review_scores_checkin|review_scores_communication|review_scores_location|review_scores_value|\n",
      "+----------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+\n",
      "|  17895559|                NULL|                  NULL|                     NULL|                 NULL|                       NULL|                  NULL|               NULL|\n",
      "+----------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_scores_bucketed.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8b697af-8c9b-4063-89bc-e9d81cf8f1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Основное соединение\n",
    "result_df = (\n",
    "    listings_partitioned\n",
    "    .join(\n",
    "        hosts_bucketed,\n",
    "        on=\"host_id\",       # Ключ из listings -> hosts\n",
    "        how=\"left\"          # Все записи listings, даже без host\n",
    "    )\n",
    "    .join(\n",
    "        review_scores_bucketed,\n",
    "        on=col(\"id\") == col(\"listing_id\"),  # Ключ listings.id -> reviews.listing_id\n",
    "        how=\"left\"                          # Все записи listings, даже без reviews\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7e117e5-55ca-4bb8-aa10-b7e900d61f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['host_id',\n",
       " 'id',\n",
       " 'listing_url',\n",
       " 'scrape_id',\n",
       " 'last_scraped',\n",
       " 'name',\n",
       " 'street',\n",
       " 'property_type',\n",
       " 'room_type',\n",
       " 'accommodates',\n",
       " 'bathrooms',\n",
       " 'bedrooms',\n",
       " 'beds',\n",
       " 'price',\n",
       " 'number_of_reviews',\n",
       " 'reviews_per_month',\n",
       " 'city',\n",
       " 'country',\n",
       " 'host_url',\n",
       " 'host_name',\n",
       " 'host_since',\n",
       " 'host_location',\n",
       " 'host_about',\n",
       " 'host_response_time',\n",
       " 'host_response_rate',\n",
       " 'host_acceptance_rate',\n",
       " 'host_listings_count',\n",
       " 'host_total_listings_count',\n",
       " 'host_verifications',\n",
       " 'listing_id',\n",
       " 'review_scores_rating',\n",
       " 'review_scores_accuracy',\n",
       " 'review_scores_cleanliness',\n",
       " 'review_scores_checkin',\n",
       " 'review_scores_communication',\n",
       " 'review_scores_location',\n",
       " 'review_scores_value']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f399c78e-2e61-464b-81d3-d437b34ab88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:======================================================>(126 + 1) / 127]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 247476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "total_rows = result_df.count()\n",
    "print(f\"Total rows: {total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c04ab52b-2139-40b6-9f3a-dd939fdeae73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/03 12:42:50 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 21:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------\n",
      " host_id                     | 0      \n",
      " id                          | 0      \n",
      " listing_url                 | 0      \n",
      " scrape_id                   | 0      \n",
      " last_scraped                | 0      \n",
      " name                        | 223    \n",
      " street                      | 0      \n",
      " property_type               | 4      \n",
      " room_type                   | 0      \n",
      " accommodates                | 39     \n",
      " bathrooms                   | 764    \n",
      " bedrooms                    | 301    \n",
      " beds                        | 476    \n",
      " price                       | 4076   \n",
      " number_of_reviews           | 0      \n",
      " reviews_per_month           | 60968  \n",
      " city                        | 235    \n",
      " country                     | 0      \n",
      " host_url                    | 0      \n",
      " host_name                   | 260    \n",
      " host_since                  | 259    \n",
      " host_location               | 1143   \n",
      " host_about                  | 98959  \n",
      " host_response_time          | 57987  \n",
      " host_response_rate          | 57987  \n",
      " host_acceptance_rate        | 226019 \n",
      " host_listings_count         | 259    \n",
      " host_total_listings_count   | 259    \n",
      " host_verifications          | 427    \n",
      " listing_id                  | 0      \n",
      " review_scores_rating        | 63891  \n",
      " review_scores_accuracy      | 64329  \n",
      " review_scores_cleanliness   | 64208  \n",
      " review_scores_checkin       | 64492  \n",
      " review_scores_communication | 64242  \n",
      " review_scores_location      | 64475  \n",
      " review_scores_value         | 64517  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "null_counts = result_df.agg(\n",
    "    *[count(when(col(c).isNull(), c)).alias(c) for c in result_df.columns]\n",
    ").show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f56edce4-c0dd-4d19-8ef1-47226bbea636",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['last_scraped', 'street', 'property_type', 'room_type', 'accommodates','bathrooms', 'bedrooms', 'beds', 'number_of_reviews', 'reviews_per_month', 'city',\n",
    "           'country', 'host_since', 'host_location', 'host_response_time','host_response_rate','host_listings_count', 'host_total_listings_count', 'host_verifications', 'review_scores_rating',\n",
    "           'review_scores_accuracy','review_scores_cleanliness','review_scores_checkin','review_scores_communication','review_scores_location','review_scores_value']\n",
    "label = 'price'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a50f4c62-f1ff-4215-a98d-b4b593be7fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-------------+---------------+------------+---------+---------+---------+-----------------+-----------------+-------------+--------------+----------+--------------------+------------------+------------------+-------------------+-------------------------+--------------------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+-----------+\n",
      "|last_scraped|              street|property_type|      room_type|accommodates|bathrooms| bedrooms|     beds|number_of_reviews|reviews_per_month|         city|       country|host_since|       host_location|host_response_time|host_response_rate|host_listings_count|host_total_listings_count|  host_verifications|review_scores_rating|review_scores_accuracy|review_scores_cleanliness|review_scores_checkin|review_scores_communication|review_scores_location|review_scores_value|      label|\n",
      "+------------+--------------------+-------------+---------------+------------+---------+---------+---------+-----------------+-----------------+-------------+--------------+----------+--------------------+------------------+------------------+-------------------+-------------------------+--------------------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+-----------+\n",
      "|  2017-04-02|Mission District,...|    Apartment|Entire home/apt|   2.0000000|1.0000000|1.0000000|1.0000000|       87.0000000|        5.6000000|San Francisco| United States|2014-07-29|San Francisco, Ca...|      within a day|       100.0000000|          1.0000000|                1.0000000|email,phone,revie...|          95.0000000|            10.0000000|               10.0000000|           10.0000000|                  9.0000000|            10.0000000|          9.0000000|149.0000000|\n",
      "|  2017-05-04|New York, NY 1002...|    Apartment|Entire home/apt|   4.0000000|1.0000000|1.0000000|2.0000000|       10.0000000|        0.7600000|     New York| United States|2013-06-13|New York, New Yor...|    within an hour|       100.0000000|          1.0000000|                1.0000000|email,phone,faceb...|          95.0000000|            10.0000000|                9.0000000|           10.0000000|                 10.0000000|            10.0000000|         10.0000000|199.0000000|\n",
      "|  2017-05-04|Ficial District, ...|    Apartment|Entire home/apt|   5.0000000|1.0000000|1.0000000|2.0000000|        6.0000000|        0.6100000|     New York| United States|2010-05-14|New York, New Yor...|within a few hours|       100.0000000|          1.0000000|                1.0000000|email,phone,faceb...|         100.0000000|            10.0000000|                9.0000000|           10.0000000|                 10.0000000|            10.0000000|         10.0000000|275.0000000|\n",
      "|  2017-04-05|Panthéon, Paris, ...|    Apartment|Entire home/apt|   2.0000000|1.0000000|0.0000000|1.0000000|       16.0000000|        1.0100000|        Paris|        France|2012-03-12|Paris, Île-de-Fra...|      within a day|       100.0000000|          2.0000000|                2.0000000|email,phone,revie...|          95.0000000|             9.0000000|                9.0000000|           10.0000000|                 10.0000000|            10.0000000|          9.0000000| 52.0000000|\n",
      "|  2017-06-15|Copenhagen, Capit...|    Apartment|Entire home/apt|   2.0000000|1.0000000|1.0000000|1.0000000|        1.0000000|        0.1000000|   Copenhagen|       Denmark|2014-07-31|Copenhagen, Capit...|      within a day|       100.0000000|          2.0000000|                2.0000000|email,phone,faceb...|         100.0000000|            10.0000000|               10.0000000|           10.0000000|                 10.0000000|            10.0000000|         10.0000000|729.0000000|\n",
      "|  2017-05-03|Pasadena, Pasaden...|    Apartment|Entire home/apt|   4.0000000|2.0000000|2.0000000|2.0000000|       13.0000000|        0.9600000|     Pasadena| United States|2015-01-31|Pasadena, Califor...|within a few hours|       100.0000000|         12.0000000|               12.0000000|email,phone,revie...|          98.0000000|            10.0000000|               10.0000000|           10.0000000|                 10.0000000|            10.0000000|         10.0000000|127.0000000|\n",
      "|  2016-04-02|Henry Philip Aven...|    Apartment|Entire home/apt|   6.0000000|2.0000000|3.0000000|4.0000000|       12.0000000|        3.9100000|      Ballina|     Australia|2014-10-02|Ballina, New Sout...|    within an hour|       100.0000000|          1.0000000|                1.0000000|email,phone,faceb...|          97.0000000|            10.0000000|               10.0000000|           10.0000000|                 10.0000000|            10.0000000|          9.0000000|300.0000000|\n",
      "|  2017-04-04|Hurstville, Hurst...|    Apartment|   Private room|   1.0000000|1.5000000|1.0000000|1.0000000|        9.0000000|        2.0600000|   Hurstville|     Australia|2015-12-15|                  AU|within a few hours|       100.0000000|          1.0000000|                1.0000000| email,phone,reviews|          91.0000000|             9.0000000|                8.0000000|           10.0000000|                 10.0000000|            10.0000000|         10.0000000| 35.0000000|\n",
      "|  2017-03-15|Pollença, Illes B...|        House|Entire home/apt|   6.0000000|2.0000000|3.0000000|4.0000000|        6.0000000|        0.5200000|     Pollença|         Spain|2012-08-18|Palma de Mallorca...|within a few hours|        97.0000000|         92.0000000|               92.0000000|email,phone,revie...|          67.0000000|             7.0000000|                6.0000000|            9.0000000|                  9.0000000|             7.0000000|          7.0000000| 72.0000000|\n",
      "|  2016-08-08|Rue du Grand-Pré,...|    Apartment|Entire home/apt|   2.0000000|1.0000000|1.0000000|2.0000000|        3.0000000|        0.5500000|       Genève|   Switzerland|2012-05-01|Geneva, Geneva, S...|within a few hours|        86.0000000|          1.0000000|                1.0000000|email,phone,faceb...|          93.0000000|             9.0000000|                9.0000000|           10.0000000|                 10.0000000|            10.0000000|          9.0000000| 76.0000000|\n",
      "|  2017-05-09|Innere Stadt, Wie...|    Apartment|Entire home/apt|  10.0000000|5.0000000|4.0000000|9.0000000|       34.0000000|        2.0700000|         Wien|       Austria|2015-12-11|Vienna, Vienna, A...|within a few hours|       100.0000000|          2.0000000|                2.0000000|email,phone,revie...|          93.0000000|            10.0000000|               10.0000000|           10.0000000|                 10.0000000|            10.0000000|         10.0000000|200.0000000|\n",
      "|  2017-04-08|El Gòtic, Barcelo...|    Apartment|   Private room|   2.0000000|1.0000000|1.0000000|4.0000000|      141.0000000|        2.9500000|    Barcelona|         Spain|2013-03-17|Barcelona, Catalo...|      within a day|        75.0000000|          5.0000000|                5.0000000|email,phone,faceb...|          92.0000000|             9.0000000|                9.0000000|            9.0000000|                 10.0000000|             9.0000000|          9.0000000| 21.0000000|\n",
      "|  2017-04-02|Stadionbuurt, Ams...|    Apartment|Entire home/apt|   2.0000000|1.0000000|1.0000000|1.0000000|        5.0000000|        0.4500000|    Amsterdam|   Netherlands|2015-09-14|Amsterdam, North ...|      within a day|        50.0000000|          1.0000000|                1.0000000|email,phone,faceb...|         100.0000000|            10.0000000|               10.0000000|           10.0000000|                 10.0000000|            10.0000000|         10.0000000|100.0000000|\n",
      "|  2017-04-05|Batignolles, Pari...|    Apartment|Entire home/apt|   2.0000000|1.0000000|1.0000000|1.0000000|       31.0000000|        2.0200000|        Paris|        France|2012-12-03|Paris, Île-de-Fra...|    within an hour|        90.0000000|          1.0000000|                1.0000000|email,phone,faceb...|          92.0000000|            10.0000000|                9.0000000|           10.0000000|                 10.0000000|            10.0000000|          9.0000000| 85.0000000|\n",
      "|  2017-05-09|Wien, Wien 1130, ...|    Apartment|Entire home/apt|   3.0000000|1.0000000|0.0000000|1.0000000|        5.0000000|        0.3000000|         Wien|       Austria|2015-12-15|Vienna, Vienna, A...|within a few hours|       100.0000000|          1.0000000|                1.0000000| email,phone,reviews|          92.0000000|            10.0000000|               10.0000000|           10.0000000|                 10.0000000|            10.0000000|         10.0000000| 42.0000000|\n",
      "|  2017-05-09|Cannaregio, Venic...|    Apartment|   Private room|   2.0000000|1.0000000|1.0000000|1.0000000|      278.0000000|        5.5200000|       Venice|         Italy|2013-03-17|Venice, Veneto, I...|      within a day|        99.0000000|          3.0000000|                3.0000000|email,phone,revie...|          83.0000000|             9.0000000|                8.0000000|            9.0000000|                  9.0000000|             9.0000000|          8.0000000| 35.0000000|\n",
      "|  2017-03-04|Marlborough Cresc...|    Apartment|Entire home/apt|   2.0000000|1.0000000|1.0000000|1.0000000|        1.0000000|        0.4800000|   Harlington|United Kingdom|2014-04-04|              London|within a few hours|        90.0000000|         32.0000000|               32.0000000|email,phone,revie...|         100.0000000|            10.0000000|               10.0000000|           10.0000000|                 10.0000000|            10.0000000|         10.0000000| 65.0000000|\n",
      "|  2017-04-06|Paris, Île-de-Fra...|    Apartment|Entire home/apt|   2.0000000|1.0000000|1.0000000|1.0000000|       10.0000000|        1.0600000|        Paris|        France|2013-03-30|Paris, Île-de-Fra...|    within an hour|       100.0000000|          1.0000000|                1.0000000|email,phone,revie...|          80.0000000|             9.0000000|                8.0000000|            9.0000000|                  9.0000000|            10.0000000|          7.0000000| 70.0000000|\n",
      "|  2017-04-02|Amsterdam Centrum...|    Apartment|Entire home/apt|   4.0000000|1.0000000|2.0000000|2.0000000|        3.0000000|        0.2600000|    Amsterdam|   Netherlands|2015-08-10|                  IT|within a few hours|        94.0000000|          9.0000000|                9.0000000| email,phone,reviews|          73.0000000|             7.0000000|                6.0000000|            9.0000000|                  7.0000000|            10.0000000|          7.0000000|250.0000000|\n",
      "|  2017-03-04|Marlborough Cresc...|    Apartment|Entire home/apt|   2.0000000|1.0000000|1.0000000|1.0000000|        5.0000000|        0.3400000|   Harlington|United Kingdom|2014-04-04|              London|within a few hours|        90.0000000|         32.0000000|               32.0000000|email,phone,revie...|          92.0000000|            10.0000000|               10.0000000|           10.0000000|                  9.0000000|             9.0000000|          9.0000000| 65.0000000|\n",
      "+------------+--------------------+-------------+---------------+------------+---------+---------+---------+-----------------+-----------------+-------------+--------------+----------+--------------------+------------------+------------------+-------------------+-------------------------+--------------------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result_df = result_df.select(features + [label]).na.drop()\n",
    "# emps = emps.withColumn(\"ename_job\", F.concat(F.col('ename'), F.lit(\"_\"), F.col('job')))\n",
    "result_df = result_df.withColumnRenamed(\"price\",\"label\")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e89290b-4487-472b-a6d8-71432c806c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 151055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "total_rows = result_df.count()\n",
    "print(f\"Total rows: {total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d93af2a-7556-43a4-8861-0ba743db967d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last_scraped',\n",
       " 'street',\n",
       " 'property_type',\n",
       " 'room_type',\n",
       " 'accommodates',\n",
       " 'bathrooms',\n",
       " 'bedrooms',\n",
       " 'beds',\n",
       " 'number_of_reviews',\n",
       " 'reviews_per_month',\n",
       " 'city',\n",
       " 'country',\n",
       " 'host_since',\n",
       " 'host_location',\n",
       " 'host_response_time',\n",
       " 'host_response_rate',\n",
       " 'host_listings_count',\n",
       " 'host_total_listings_count',\n",
       " 'host_verifications',\n",
       " 'review_scores_rating',\n",
       " 'review_scores_accuracy',\n",
       " 'review_scores_cleanliness',\n",
       " 'review_scores_checkin',\n",
       " 'review_scores_communication',\n",
       " 'review_scores_location',\n",
       " 'review_scores_value',\n",
       " 'label']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac5ab39d-2fe8-4fa2-a73f-d07a52116641",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, year, month, dayofmonth\n",
    "\n",
    "df_with_dates = result_df.withColumn(\n",
    "    \"date_parsed\",\n",
    "    to_date(\"last_scraped\", format=\"yyyy-MM-dd\")\n",
    ").withColumn(\n",
    "    \"last_scraped_year\", year(\"date_parsed\")\n",
    ").withColumn(\n",
    "    \"month\", month(\"date_parsed\")\n",
    ").withColumn(\n",
    "    \"day\", dayofmonth(\"date_parsed\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d405016-5fa4-407a-bb78-80bc89f03de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCols\n",
    "from pyspark.sql.functions import sin, cos, pi\n",
    "import math\n",
    "\n",
    "class CyclicDateEncoder(Transformer, HasInputCol, HasOutputCols):\n",
    "    def __init__(self, inputCol=None, outputCols=None):\n",
    "        super(CyclicDateEncoder, self).__init__()\n",
    "        self._set(inputCol=inputCol, outputCols=outputCols)\n",
    "        \n",
    "    def _transform(self, df):\n",
    "        input_col = self.getInputCol()\n",
    "        output_cols = self.getOutputCols()\n",
    "        if input_col == \"month\":\n",
    "            return df.withColumn(\n",
    "                output_cols[0],  # Например, \"month_sin\"\n",
    "                sin(2 * pi() * col(input_col) / 12)  # Нормировка для месяца\n",
    "            ).withColumn(\n",
    "                output_cols[1],  # Например, \"month_cos\"\n",
    "                cos(2 * pi() * col(input_col) / 12)\n",
    "            )\n",
    "        elif input_col == \"day\":\n",
    "            return df.withColumn(\n",
    "                output_cols[0],  # Например, \"month_sin\"\n",
    "                sin(2 * pi() * col(input_col) / 31)  # Нормировка для месяца\n",
    "            ).withColumn(\n",
    "                output_cols[1],  # Например, \"month_cos\"\n",
    "                cos(2 * pi() * col(input_col) / 31)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efb0b907-75f8-4f96-b7d0-c506348ec005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для месяца\n",
    "month_encoder = CyclicDateEncoder(\n",
    "    inputCol=\"month\",\n",
    "    outputCols=[\"last_scrapped_month_sin\", \"last_scrapped_month_cos\"]\n",
    ")\n",
    "\n",
    "# Для дня (аналогично, нормировка на 31)\n",
    "day_encoder = CyclicDateEncoder(\n",
    "    inputCol=\"day\",\n",
    "    outputCols=[\"last_scrapped_day_sin\", \"last_scrapped_day_cos\"]\n",
    ")\n",
    "\n",
    "# Применяем трансформеры\n",
    "df_encoded = month_encoder.transform(df_with_dates)\n",
    "df_encoded = day_encoder.transform(df_encoded)\n",
    "\n",
    "# Результат\n",
    "# df_encoded.select(\"last_scraped_year\", \"month\", \"day\", \"month_sin\", \"month_cos\", \"day_sin\", \"day_cos\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a262ba9e-61a9-4346-97d9-51c4962b147c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last_scraped',\n",
       " 'street',\n",
       " 'property_type',\n",
       " 'room_type',\n",
       " 'accommodates',\n",
       " 'bathrooms',\n",
       " 'bedrooms',\n",
       " 'beds',\n",
       " 'number_of_reviews',\n",
       " 'reviews_per_month',\n",
       " 'city',\n",
       " 'country',\n",
       " 'host_since',\n",
       " 'host_location',\n",
       " 'host_response_time',\n",
       " 'host_response_rate',\n",
       " 'host_listings_count',\n",
       " 'host_total_listings_count',\n",
       " 'host_verifications',\n",
       " 'review_scores_rating',\n",
       " 'review_scores_accuracy',\n",
       " 'review_scores_cleanliness',\n",
       " 'review_scores_checkin',\n",
       " 'review_scores_communication',\n",
       " 'review_scores_location',\n",
       " 'review_scores_value',\n",
       " 'label',\n",
       " 'date_parsed',\n",
       " 'last_scraped_year',\n",
       " 'month',\n",
       " 'day',\n",
       " 'last_scrapped_month_sin',\n",
       " 'last_scrapped_month_cos',\n",
       " 'last_scrapped_day_sin',\n",
       " 'last_scrapped_day_cos']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e05e6d-b953-4dd6-84ae-dbeac729dc76",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00984a8d-8516-47b6-891a-85711cb5f210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['street',\n",
       " 'property_type',\n",
       " 'room_type',\n",
       " 'accommodates',\n",
       " 'bathrooms',\n",
       " 'bedrooms',\n",
       " 'beds',\n",
       " 'number_of_reviews',\n",
       " 'reviews_per_month',\n",
       " 'city',\n",
       " 'country',\n",
       " 'host_since',\n",
       " 'host_location',\n",
       " 'host_response_time',\n",
       " 'host_response_rate',\n",
       " 'host_listings_count',\n",
       " 'host_total_listings_count',\n",
       " 'host_verifications',\n",
       " 'review_scores_rating',\n",
       " 'review_scores_accuracy',\n",
       " 'review_scores_cleanliness',\n",
       " 'review_scores_checkin',\n",
       " 'review_scores_communication',\n",
       " 'review_scores_location',\n",
       " 'review_scores_value',\n",
       " 'label',\n",
       " 'last_scraped_year',\n",
       " 'last_scrapped_month_sin',\n",
       " 'last_scrapped_month_cos',\n",
       " 'last_scrapped_day_sin',\n",
       " 'last_scrapped_day_cos']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded = df_encoded.drop(\"last_scraped\", \"date_parsed\",\"day\",\"month\")\n",
    "df_encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e52010af-75f2-4a0a-af7a-09a738f04487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, year, month, dayofmonth\n",
    "\n",
    "df_with_dates = df_encoded.withColumn(\n",
    "    \"date_parsed\",\n",
    "    to_date(\"host_since\", format=\"yyyy-MM-dd\")  # Уточните формат вашей даты!\n",
    ").withColumn(\n",
    "    \"year_host_since\", year(\"date_parsed\")\n",
    ").withColumn(\n",
    "    \"month\", month(\"date_parsed\")\n",
    ").withColumn(\n",
    "    \"day\", dayofmonth(\"date_parsed\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2dc2595e-10ca-4d5f-9e32-5846b4f163ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['street',\n",
       " 'property_type',\n",
       " 'room_type',\n",
       " 'accommodates',\n",
       " 'bathrooms',\n",
       " 'bedrooms',\n",
       " 'beds',\n",
       " 'number_of_reviews',\n",
       " 'reviews_per_month',\n",
       " 'city',\n",
       " 'country',\n",
       " 'host_since',\n",
       " 'host_location',\n",
       " 'host_response_time',\n",
       " 'host_response_rate',\n",
       " 'host_listings_count',\n",
       " 'host_total_listings_count',\n",
       " 'host_verifications',\n",
       " 'review_scores_rating',\n",
       " 'review_scores_accuracy',\n",
       " 'review_scores_cleanliness',\n",
       " 'review_scores_checkin',\n",
       " 'review_scores_communication',\n",
       " 'review_scores_location',\n",
       " 'review_scores_value',\n",
       " 'label',\n",
       " 'last_scraped_year',\n",
       " 'last_scrapped_month_sin',\n",
       " 'last_scrapped_month_cos',\n",
       " 'last_scrapped_day_sin',\n",
       " 'last_scrapped_day_cos',\n",
       " 'date_parsed',\n",
       " 'year_host_since',\n",
       " 'month',\n",
       " 'day']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_dates.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "569ee715-b91e-4e23-8280-a4ed6dd0428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для месяца\n",
    "month_encoder = CyclicDateEncoder(\n",
    "    inputCol=\"month\",\n",
    "    outputCols=[\"host_since_month_sin\", \"host_since_month_cos\"]\n",
    ")\n",
    "\n",
    "# Для дня (аналогично, нормировка на 31)\n",
    "day_encoder = CyclicDateEncoder(\n",
    "    inputCol=\"day\",\n",
    "    outputCols=[\"host_since_day_sin\", \"host_since_day_cos\"]\n",
    ")\n",
    "\n",
    "# Применяем трансформеры\n",
    "df_encoded = month_encoder.transform(df_with_dates)\n",
    "df_encoded = day_encoder.transform(df_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2afa5bf-3cd1-4840-89b6-2a5c5fbcddc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['street',\n",
       " 'property_type',\n",
       " 'room_type',\n",
       " 'accommodates',\n",
       " 'bathrooms',\n",
       " 'bedrooms',\n",
       " 'beds',\n",
       " 'number_of_reviews',\n",
       " 'reviews_per_month',\n",
       " 'city',\n",
       " 'country',\n",
       " 'host_location',\n",
       " 'host_response_time',\n",
       " 'host_response_rate',\n",
       " 'host_listings_count',\n",
       " 'host_total_listings_count',\n",
       " 'host_verifications',\n",
       " 'review_scores_rating',\n",
       " 'review_scores_accuracy',\n",
       " 'review_scores_cleanliness',\n",
       " 'review_scores_checkin',\n",
       " 'review_scores_communication',\n",
       " 'review_scores_location',\n",
       " 'review_scores_value',\n",
       " 'label',\n",
       " 'last_scraped_year',\n",
       " 'last_scrapped_month_sin',\n",
       " 'last_scrapped_month_cos',\n",
       " 'last_scrapped_day_sin',\n",
       " 'last_scrapped_day_cos',\n",
       " 'year_host_since',\n",
       " 'host_since_month_sin',\n",
       " 'host_since_month_cos',\n",
       " 'host_since_day_sin',\n",
       " 'host_since_day_cos']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_encoded.columns\n",
    "df_encoded = df_encoded.drop(\"month\", \"day\",\"date_parsed\",\"host_since\")\n",
    "df_encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8fddc322-7e55-4ea0-bbc6-3b248ad63837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['street',\n",
       " 'property_type',\n",
       " 'room_type',\n",
       " 'accommodates',\n",
       " 'bathrooms',\n",
       " 'bedrooms',\n",
       " 'beds',\n",
       " 'number_of_reviews',\n",
       " 'reviews_per_month',\n",
       " 'city',\n",
       " 'country',\n",
       " 'host_location',\n",
       " 'host_response_time',\n",
       " 'host_response_rate',\n",
       " 'host_listings_count',\n",
       " 'host_total_listings_count',\n",
       " 'host_verifications',\n",
       " 'review_scores_rating',\n",
       " 'review_scores_accuracy',\n",
       " 'review_scores_cleanliness',\n",
       " 'review_scores_checkin',\n",
       " 'review_scores_communication',\n",
       " 'review_scores_location',\n",
       " 'review_scores_value',\n",
       " 'label',\n",
       " 'last_scraped_year',\n",
       " 'last_scrapped_month_sin',\n",
       " 'last_scrapped_month_cos',\n",
       " 'last_scrapped_day_sin',\n",
       " 'last_scrapped_day_cos',\n",
       " 'year_host_since',\n",
       " 'host_since_month_sin',\n",
       " 'host_since_month_cos',\n",
       " 'host_since_day_sin',\n",
       " 'host_since_day_cos']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49a39df1-8ad5-488a-b961-4977978e7d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = df_encoded.drop(\"last_scraped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d052cf3-b6ba-40d3-b3d6-a2bb4e75e9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, Word2Vec, Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "categoricalCols = ['city','country','host_location','host_verifications', 'property_type','room_type', 'host_response_time']\n",
    "textCols = ['street']\n",
    "others = ['accommodates','bathrooms','bedrooms','beds','number_of_reviews','reviews_per_month',\n",
    "          'host_response_rate','host_listings_count', 'host_total_listings_count','review_scores_rating','review_scores_accuracy',\n",
    "          'review_scores_cleanliness', 'review_scores_checkin','review_scores_communication', 'review_scores_location', 'review_scores_value',\n",
    "         'last_scraped_year','host_since_month_sin','host_since_month_cos','host_since_day_sin','host_since_day_cos','year_host_since']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "73541678-05c4-4b26-97ae-1ebe39221ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 58:=============>    (3 + 1) / 4][Stage 61:=========>        (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              street|\n",
      "+--------------------+\n",
      "|Mission District,...|\n",
      "|New York, NY 1002...|\n",
      "|Ficial District, ...|\n",
      "|Panthéon, Paris, ...|\n",
      "|Copenhagen, Capit...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_encoded.select('street').show(5)\n",
    "# df_encoded.select('city').show(5)\n",
    "# df_encoded.select('country').show(5)\n",
    "# df_encoded.select('host_location').show(5)\n",
    "# df_encoded.select('host_verifications').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3059efa5-56a7-4616-b17a-aeb0a1871920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_count = df_encoded.select(\"city\").distinct().count()\n",
    "# print(unique_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d11b2ab4-fb01-47c9-9a00-8d89e4dd0620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_count = df_encoded.select(\"host_location\").distinct().count()\n",
    "# print(unique_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a633753-04dd-4739-96f4-682139bd8665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_count = df_encoded.select(\"host_verifications\").distinct().count()\n",
    "# print(unique_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d6ef269a-8f2d-41ba-a470-a6784ab264e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_count = df_encoded.select(\"country\").distinct().count()\n",
    "# print(unique_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "52a1d86a-936a-402c-8757-af9bc86e7a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_count = df_encoded.select(\"street\").distinct().count()\n",
    "# print(unique_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a5f4811a-c82f-431c-bd6d-6f696c0ff8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, Word2Vec\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "tokenizer = RegexTokenizer(\n",
    "    inputCol=\"street\", \n",
    "    outputCol=\"city_tokens\",\n",
    "    pattern=\"[,\\s]+\"\n",
    ")\n",
    "\n",
    "# df_encoded_tok = tokenizer.transform(df_encoded)\n",
    "# df_encoded_tok.select('city_tokens').show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "48d704b5-2d49-4565-9ecf-26c777d43e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2Vec = Word2Vec(\n",
    "    vectorSize=10,\n",
    "    minCount=1,\n",
    "    windowSize=5,\n",
    "    inputCol=\"city_tokens\",\n",
    "    outputCol=\"city_vec\"\n",
    ")\n",
    "# word2VecModel = word2Vec.fit(df_encoded_tok)\n",
    "# print(word2VecModel)\n",
    "\n",
    "# df_encoded_tok = word2VecModel.transform(df_encoded_tok)\n",
    "# # df_encoded_tok.show()\n",
    "\n",
    "# # Adding the encoded ename_job to the list of other columns\n",
    "# others += [city_vec]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac3dc27a-2c13-4ccf-b250-0a58e4d0330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = [ StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c)).setHandleInvalid(\"skip\") for c in categoricalCols ]\n",
    "encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) for indexer in indexers ]\n",
    "assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders] + others, outputCol= \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "708d05b1-64c7-4c95-8ffc-dab87e70d3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_encoded_tok.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b463c308-ad37-4d10-8aa0-88754439b6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/03 12:46:09 WARN DAGScheduler: Broadcasting large task binary with size 1584.1 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# You can create a pipeline to use only a single fit and transform on the data.\n",
    "pipeline = Pipeline(stages=[tokenizer, word2Vec] + indexers + encoders + [assembler])\n",
    "\n",
    "\n",
    "# Fit the pipeline ==> This will call the fit functions for all transformers if exist\n",
    "model=pipeline.fit(df_encoded)\n",
    "# Fit the pipeline ==> This will call the transform functions for all transformers\n",
    "data = model.transform(df_encoded)\n",
    "\n",
    "# data.show()\n",
    "\n",
    "# We delete all features and keep only the features and label columns\n",
    "data = data.select([\"features\", \"label\"])\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4\n",
    "# distinct values are treated as continuous.\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "transformed = featureIndexer.transform(data)\n",
    "\n",
    "# Display the output Spark DataFrame\n",
    "# transformed.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a71bafe-c88b-47d2-accc-a9ca27f3a95d",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fd3df2-8f31-42ed-8f0f-7dfcb795a5e8",
   "metadata": {},
   "source": [
    "# Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f29220b1-87c0-44ce-8665-44d7a0b09822",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = transformed.withColumn(\"label\", col(\"label\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "71ca00fb-a991-47d0-8d09-75e5604ed694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/03 12:46:48 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "25/05/03 12:47:08 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  split the data into 60% training and 40% test (it is not stratified)\n",
    "(train_data, test_data) = transformed.randomSplit([0.6, 0.4], seed = 10)\n",
    "\n",
    "def run(command):\n",
    "    import os\n",
    "    return os.popen(command).read()\n",
    "\n",
    "train_data.select(\"features\", \"label\")\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"json\")\\\n",
    "    .save(\"project/data/train\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -cat project/data/train/*.json > ../data/train.json\")\n",
    "\n",
    "test_data.select(\"features\", \"label\")\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"json\")\\\n",
    "    .save(\"project/data/test\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -cat project/data/test/*.json > ../data/test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fecd52-189d-45af-841c-47819b311918",
   "metadata": {},
   "source": [
    "# First model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daffa79-5f41-4e78-aa1a-c914f8b3f1f5",
   "metadata": {},
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9892ef1c-f7c4-4acd-8d7d-5630f0823c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/03 12:47:42 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:47 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/05/03 12:47:47 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:50 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:50 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:51 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:51 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:51 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:51 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:51 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:52 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:52 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:52 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:52 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:52 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:53 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:53 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:53 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:53 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:53 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:53 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:54 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:54 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:54 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:54 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:54 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:55 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:55 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:55 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:55 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:55 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:56 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:56 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:56 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:56 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:56 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:56 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:57 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:57 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:57 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:57 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:57 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:58 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:58 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:58 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:58 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:58 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:58 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:59 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:59 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:59 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:59 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:59 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:47:59 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:00 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:00 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:00 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:00 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:00 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:00 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:01 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:01 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:01 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:01 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:01 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:02 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:02 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:02 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:02 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:02 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:02 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:03 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:03 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:03 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:03 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:03 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:04 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:04 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:04 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:04 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:04 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:04 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:05 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:05 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:05 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:05 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:05 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:06 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:06 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:06 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:06 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:06 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:06 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:07 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:07 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:07 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:07 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:07 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:07 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:08 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:08 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:08 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:08 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:08 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:09 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:48:22 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "# Create Linear Regression Model\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Fit the data to the pipeline stages\n",
    "model_lr = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258b1396-77d4-4dd8-8777-d0e6fcbd629f",
   "metadata": {},
   "source": [
    "## Predict for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b9a0f1ff-a175-4e00-ac1b-a5476dd7dd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/03 12:48:40 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "[Stage 791:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+------------------+\n",
      "|            features|label|     indexedFeatures|        prediction|\n",
      "+--------------------+-----+--------------------+------------------+\n",
      "|(9436,[0,3082,309...| 56.0|(9436,[0,3082,309...| 76.61436773831156|\n",
      "|(9436,[0,3082,309...| 65.0|(9436,[0,3082,309...| 93.57470102039406|\n",
      "|(9436,[0,3082,309...| 99.0|(9436,[0,3082,309...| 98.17849918884895|\n",
      "|(9436,[0,3082,309...| 55.0|(9436,[0,3082,309...| 69.24144230063212|\n",
      "|(9436,[0,3082,309...|175.0|(9436,[0,3082,309...|  99.5360312167013|\n",
      "|(9436,[0,3082,309...| 70.0|(9436,[0,3082,309...| 97.17015544617061|\n",
      "|(9436,[0,3082,309...| 98.0|(9436,[0,3082,309...|107.99071511178227|\n",
      "|(9436,[0,3082,309...| 80.0|(9436,[0,3082,309...|102.53600719051792|\n",
      "|(9436,[0,3082,309...| 85.0|(9436,[0,3082,309...|61.426270517215016|\n",
      "|(9436,[0,3082,309...| 70.0|(9436,[0,3082,309...|102.45182203371223|\n",
      "|(9436,[0,3082,309...| 85.0|(9436,[0,3082,309...| 85.78625609815208|\n",
      "|(9436,[0,3082,309...| 58.0|(9436,[0,3082,309...| 93.37659848643489|\n",
      "|(9436,[0,3082,309...|130.0|(9436,[0,3082,309...| 96.30496023102023|\n",
      "|(9436,[0,3082,309...| 83.0|(9436,[0,3082,309...| 72.54917910218865|\n",
      "|(9436,[0,3082,309...| 50.0|(9436,[0,3082,309...|100.91077749529359|\n",
      "|(9436,[0,3082,309...|108.0|(9436,[0,3082,309...| 89.42429967405587|\n",
      "|(9436,[0,3082,309...| 65.0|(9436,[0,3082,309...| 86.79422018596779|\n",
      "|(9436,[0,3082,309...| 75.0|(9436,[0,3082,309...| 86.86908353699255|\n",
      "|(9436,[0,3082,309...| 54.0|(9436,[0,3082,309...| 72.48550568454084|\n",
      "|(9436,[0,3082,309...| 50.0|(9436,[0,3082,309...| 93.04585546734779|\n",
      "+--------------------+-----+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions = model_lr.transform(test_data)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e97403-59a0-4ec4-b044-678ba6aa2b66",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c6b166ca-8b62-4995-adca-129021dd6d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/03 12:48:55 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:49:13 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "[Stage 813:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 73.41824838925476\n",
      "R^2 on test data = 0.7233085281452736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator \n",
    "\n",
    "# Evaluate the performance of the model\n",
    "evaluator1_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator1_r2 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "rmse = evaluator1_rmse.evaluate(predictions)\n",
    "r2 = evaluator1_r2.evaluate(predictions)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = {}\".format(rmse))\n",
    "print(\"R^2 on test data = {}\".format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b0e075-c37c-492a-b894-2f64d4dea72b",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7e04437d-2bf9-4803-83cc-b7ed17d311e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Param(parent='LinearRegression_2d3a4007ad99', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'),\n",
       " Param(parent='LinearRegression_2d3a4007ad99', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'),\n",
       " Param(parent='LinearRegression_2d3a4007ad99', name='epsilon', doc='The shape parameter to control the amount of robustness. Must be > 1.0. Only valid when loss is huber'),\n",
       " Param(parent='LinearRegression_2d3a4007ad99', name='featuresCol', doc='features column name.'),\n",
       " Param(parent='LinearRegression_2d3a4007ad99', name='fitIntercept', doc='whether to fit an intercept term.'),\n",
       " Param(parent='LinearRegression_2d3a4007ad99', name='labelCol', doc='label column name.'),\n",
       " Param(parent='LinearRegression_2d3a4007ad99', name='loss', doc='The loss function to be optimized. Supported options: squaredError, huber.'),\n",
       " Param(parent='LinearRegression_2d3a4007ad99', name='maxBlockSizeInMB', doc='maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0.'),\n",
       " Param(parent='LinearRegression_2d3a4007ad99', name='maxIter', doc='max number of iterations (>= 0).'),\n",
       " Param(parent='LinearRegression_2d3a4007ad99', name='predictionCol', doc='prediction column name.'),\n",
       " Param(parent='LinearRegression_2d3a4007ad99', name='regParam', doc='regularization parameter (>= 0).'),\n",
       " Param(parent='LinearRegression_2d3a4007ad99', name='solver', doc='The solver algorithm for optimization. Supported options: auto, normal, l-bfgs.'),\n",
       " Param(parent='LinearRegression_2d3a4007ad99', name='standardization', doc='whether to standardize the training features before fitting the model.'),\n",
       " Param(parent='LinearRegression_2d3a4007ad99', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'),\n",
       " Param(parent='LinearRegression_2d3a4007ad99', name='weightCol', doc='weight column name. If this is not set or empty, we treat all instance weights as 1.0.')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "22f3b23c-4d4b-49b0-b0d2-379f7ed0be27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/03 12:49:31 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:49:50 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "import numpy as np\n",
    "\n",
    "# Подготовка данных\n",
    "train_data.cache().count()\n",
    "sample_data = train_data.sample(False, 0.001, seed=42)\n",
    "\n",
    "# Уменьшенный набор параметров\n",
    "grid = (ParamGridBuilder()\n",
    "        .addGrid(model_lr.aggregationDepth, [2, 3])\n",
    "        .addGrid(model_lr.regParam, np.logspace(-2, -1, 3))\n",
    "        .build())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9ecad5b9-dc4d-4d3f-a827-254e32e012fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/03 12:50:01 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "[Stage 840:===================================================> (194 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(sample_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fdd252d1-fcf5-48d5-ba3a-f1e43aef4f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/03 13:06:10 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:06:11 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:06:12 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:06:15 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:06:16 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:06:21 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:06:21 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:06:26 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:06:27 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:06:28 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:06:35 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:06:36 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:06:40 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:06:41 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:06:41 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:06:44 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:06:45 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:06:50 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:06:51 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:06:56 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:06:57 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:06:57 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:07:04 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:07:05 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:07:09 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:07:10 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:07:11 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:07:13 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:07:14 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:07:19 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:07:19 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:07:24 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:07:25 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:07:26 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:07:33 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:07:34 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:07:38 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:07:39 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:07:39 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:07:42 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:07:43 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:07:48 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:07:48 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:07:53 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:07:54 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:07:55 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:08:02 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:08:03 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:08:07 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:08:08 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:08:08 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:08:11 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:08:12 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:08:17 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:08:17 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:08:22 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:08:23 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:08:24 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:08:31 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:08:32 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:08:36 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:08:37 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:08:37 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:08:40 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:08:40 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:08:45 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:08:46 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:08:51 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:08:52 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:08:53 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:09:00 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:09:01 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:09:05 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:09:06 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:09:07 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:09:11 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:09:11 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:09:16 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:09:17 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:09:22 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:09:23 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:09:23 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:09:30 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:09:31 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:09:35 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:09:37 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:09:37 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:09:40 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:09:40 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:09:45 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:09:46 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:09:51 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:09:52 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:09:52 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:10:00 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:10:01 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:10:05 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:10:06 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:10:06 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:10:10 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:10:10 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:10:15 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:10:16 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:10:21 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:10:23 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:10:23 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:10:30 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:10:31 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:10:35 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:10:36 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:10:37 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:10:40 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:10:41 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:10:46 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:10:47 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:10:52 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:10:53 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:10:53 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:11:00 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:11:01 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:11:05 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:11:06 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:11:07 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:11:10 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:11:11 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:11:16 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:11:17 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:11:22 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:11:23 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:11:23 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:11:30 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:11:31 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:11:36 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:11:37 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:11:37 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:11:40 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:11:41 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:11:46 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:11:47 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:11:51 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:11:52 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:11:53 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:12:00 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:12:01 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:12:05 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:12:06 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:12:07 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:12:10 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:12:10 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:12:15 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:12:16 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:12:21 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:12:22 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:12:22 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:12:30 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:12:31 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:12:35 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:12:36 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:12:36 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:12:40 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:12:40 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:12:45 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:12:46 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:12:51 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:12:52 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:12:53 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:13:00 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:13:01 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:13:05 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:13:06 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:13:06 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:13:10 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:13:11 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:13:15 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:13:16 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:13:21 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:13:22 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:13:23 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:13:30 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:13:31 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:13:35 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:13:36 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:13:37 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:13:40 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:13:40 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:13:46 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:13:46 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:13:52 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:13:53 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:13:53 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:14:00 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:14:01 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:14:05 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:14:07 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:14:07 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:14:11 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:14:11 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:14:17 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:14:17 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:14:23 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:14:24 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:14:25 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:14:31 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:14:32 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:14:37 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:14:38 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:14:38 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:15:01 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:15:02 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "[Stage 1278:(108 + 1) / 200][Stage 1281:(64 + 1) / 200][Stage 1283:(0 + 0) / 200]\r"
     ]
    }
   ],
   "source": [
    "train_data.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "27615f8d-038e-475f-9fce-85232aed94ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/03 12:56:55 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:56:55 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:56:55 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:56:55 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:56:55 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:57:06 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:57:09 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:57:14 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:57:19 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:57:24 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:57:35 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:57:36 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:57:39 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:57:40 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:57:40 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:57:45 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:57:46 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:57:51 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:57:51 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:57:57 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:57:58 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:57:58 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:58:09 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:58:09 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:58:16 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:58:17 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:58:17 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:58:19 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:58:20 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:58:25 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:58:26 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:58:31 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:58:32 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:58:32 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:58:40 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:58:41 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:58:45 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:58:46 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:58:46 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:58:50 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:58:51 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:58:56 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:58:56 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:59:01 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:59:02 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:59:03 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:59:10 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:59:11 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:59:15 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:59:16 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:59:16 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:59:19 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:59:20 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:59:25 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:59:25 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:59:30 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:59:31 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:59:32 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:59:40 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:59:40 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:59:44 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:59:45 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:59:45 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:59:49 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:59:50 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:59:55 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 12:59:55 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:00:01 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:00:02 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:00:02 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:00:10 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:00:10 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:00:14 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:00:15 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:00:16 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:00:19 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:00:19 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:00:25 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:00:25 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:00:31 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:00:32 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:00:32 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:00:39 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:00:40 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:00:44 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:00:45 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:00:46 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:00:49 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:00:50 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:00:55 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:00:55 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:01:00 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:01:01 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:01:02 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:01:09 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:01:10 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:01:14 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:01:15 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:01:16 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:01:19 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:01:20 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:01:25 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:01:26 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:01:31 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:01:32 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:01:33 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:01:39 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:01:40 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:01:45 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:01:46 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:01:46 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:01:48 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:01:49 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:01:54 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:01:54 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:01:59 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:02:00 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:02:01 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:02:08 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:02:09 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:02:13 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:02:14 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:02:15 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:02:18 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:02:18 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:02:23 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:02:24 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:02:29 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:02:30 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:02:30 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:02:37 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:02:38 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:02:42 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:02:43 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:02:44 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:02:47 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:02:47 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:02:52 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:02:53 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:02:58 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:02:59 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:02:59 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:03:06 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:03:07 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:03:11 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:03:12 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:03:13 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:03:16 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:03:16 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:03:21 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:03:22 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:03:27 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:03:28 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:03:28 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:03:36 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:03:37 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:03:41 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:03:42 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:03:42 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:03:45 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:03:46 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:03:51 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:03:51 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:03:56 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:03:57 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:03:58 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:04:05 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:04:06 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:04:10 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:04:11 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:04:11 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:04:14 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:04:15 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:04:20 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:04:21 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:04:26 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:04:27 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:04:28 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:04:35 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:04:36 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:04:40 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:04:41 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:04:41 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:04:45 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:04:46 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:04:51 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:04:51 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:04:56 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:04:57 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:04:58 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:05:05 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:05:06 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:05:10 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:05:11 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:05:12 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:05:15 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:05:15 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:05:21 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:05:21 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:05:27 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:05:28 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:05:28 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:05:35 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:05:36 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:05:40 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:05:41 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:05:42 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "[Stage 1053:(187 + 1) / 200][Stage 1055:(12 + 1) / 200][Stage 1057:(0 + 0) / 200]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/multiprocessing/pool.py:856\u001b[39m, in \u001b[36mIMapIterator.next\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    855\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m856\u001b[39m     item = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_items\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpopleft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    857\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[31mIndexError\u001b[39m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      2\u001b[39m tvs = TrainValidationSplit(\n\u001b[32m      3\u001b[39m     estimator=lr,\n\u001b[32m      4\u001b[39m     estimatorParamMaps=grid,\n\u001b[32m      5\u001b[39m     evaluator=RegressionEvaluator(metricName=\u001b[33m\"\u001b[39m\u001b[33mmse\u001b[39m\u001b[33m\"\u001b[39m),  \u001b[38;5;66;03m# Более быстрая метрика\u001b[39;00m\n\u001b[32m      6\u001b[39m     trainRatio=\u001b[32m0.8\u001b[39m,\n\u001b[32m      7\u001b[39m     parallelism=\u001b[32m5\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Запуск на уменьшенных данных\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m tvsModel = \u001b[43mtvs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m bestModel = tvsModel.bestModel\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Финализация на полных данных с лучшими параметрами\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/ml/base.py:205\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    203\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy(params)._fit(dataset)\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    208\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    209\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[32m    210\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/ml/tuning.py:1464\u001b[39m, in \u001b[36mTrainValidationSplit._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m   1462\u001b[39m pool = ThreadPool(processes=\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m.getParallelism(), numModels))\n\u001b[32m   1463\u001b[39m metrics = [\u001b[38;5;28;01mNone\u001b[39;00m] * numModels\n\u001b[32m-> \u001b[39m\u001b[32m1464\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubModel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimap_unordered\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1465\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\n\u001b[32m   1466\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcollectSubModelsParam\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/multiprocessing/pool.py:861\u001b[39m, in \u001b[36mIMapIterator.next\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    859\u001b[39m     \u001b[38;5;28mself\u001b[39m._pool = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    860\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m861\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    863\u001b[39m     item = \u001b[38;5;28mself\u001b[39m._items.popleft()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/threading.py:327\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/03 13:05:45 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:05:46 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:05:51 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/03 13:05:51 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "[Stage 1057:(144 + 2) / 200][Stage 1060:(0 + 0) / 200][Stage 1062:(0 + 0) / 200]\r"
     ]
    }
   ],
   "source": [
    "# Быстрая оценка с TrainValidationSplit\n",
    "tvs = TrainValidationSplit(\n",
    "    estimator=lr,\n",
    "    estimatorParamMaps=grid,\n",
    "    evaluator=RegressionEvaluator(metricName=\"mse\"),  # Более быстрая метрика\n",
    "    trainRatio=0.8,\n",
    "    parallelism=5)\n",
    "\n",
    "# Запуск на уменьшенных данных\n",
    "tvsModel = tvs.fit(sample_data)\n",
    "bestModel = tvsModel.bestModel\n",
    "\n",
    "# Финализация на полных данных с лучшими параметрами\n",
    "final_model = lr.setParams(**{param.name: bestModel.getOrDefault(param) \n",
    "                            for param in bestModel.extractParamMap()})\n",
    "final_model.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9f4388f9-db94-423c-879d-392c90041d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.tuning import ParamGridBuilder, CrossValidator \n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# grid = ParamGridBuilder()\n",
    "# grid = grid.addGrid(\n",
    "#                     model_lr.aggregationDepth, [2, 3, 4])\\\n",
    "#                     .addGrid(model_lr.regParam, np.logspace(1e-3,1e-1)\n",
    "#                     )\\\n",
    "#                     .build()\n",
    "\n",
    "# cv = CrossValidator(estimator = lr, \n",
    "#                     estimatorParamMaps = grid, \n",
    "#                     evaluator = evaluator1_rmse,\n",
    "#                     parallelism = 5,\n",
    "#                     numFolds=3)\n",
    "\n",
    "# cvModel = cv.fit(train_data)\n",
    "# bestModel = cvModel.bestModel\n",
    "# bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5ad629-3665-4ea1-b576-a1f735f761bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a27e31c-959e-438c-bec0-1f109d7e2c15",
   "metadata": {},
   "source": [
    "## Best model 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d86fc18-eb29-4f03-bfef-ed3aa1daa4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "model1 = bestModel\n",
    "pprint(model1.extractParamMap())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632f666a-f792-4b50-b38f-b39f9208ab25",
   "metadata": {},
   "source": [
    "## Save the model to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e562da25-53d1-46dc-ac5d-68a6e9041927",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.write().overwrite().save(\"project/big_data_project/models/model1\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -get project/big_data_project/models/model1 models/model1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e9b220-54e1-49ce-802c-fc005d8f63c3",
   "metadata": {},
   "source": [
    "## Predict for test data using best model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8a05de-dcae-4bb0-8cf1-de760a62e0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model1.transform(test_data)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096936c5-a575-48b7-92d2-be22893e892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select(\"label\", \"prediction\")\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .save(\"project/output/model1_predictions.csv\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -cat project/output/model1_predictions.csv/*.csv > output/model1_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f9a43d-80ce-4ec1-930f-671a240673d9",
   "metadata": {},
   "source": [
    "## Evaluate the best model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0cc0e6-7d0d-4030-a372-375e5e9b84f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator \n",
    "\n",
    "# Evaluate the performance of the model\n",
    "evaluator1_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator1_r2 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "rmse1 = evaluator1_rmse.evaluate(predictions)\n",
    "r21 = evaluator1_r2.evaluate(predictions)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = {}\".format(rmse1))\n",
    "print(\"R^2 on test data = {}\".format(r21))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5308587c-3e52-41c6-ba7d-494d53cc45b1",
   "metadata": {},
   "source": [
    "# Second model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d929b5a0-a159-4c94-ae56-086d3cf52df0",
   "metadata": {},
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d8981a-e274-4ab7-9d03-fefdc068b51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "# Create Linear Regression Model\n",
    "gbt = GBTRegressor()\n",
    "\n",
    "# Fit the data to the pipeline stages\n",
    "model_gbt = gbt.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1c9a67-fe61-49fe-898f-017df0c4006b",
   "metadata": {},
   "source": [
    "## Predict for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb95001-9d15-4da4-98df-2b87c0c39200",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_gbt.transform(test_data)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c94da6f-f913-45ec-a377-d9c42dacb075",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3508997-af5a-4882-8fd6-8d0a5be07ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator \n",
    "\n",
    "# Evaluate the performance of the model\n",
    "evaluator2_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator2_r2 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "rmse2 = evaluator2_rmse.evaluate(predictions)\n",
    "r22 = evaluator2_r2.evaluate(predictions)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = {}\".format(rmse2))\n",
    "print(\"R^2 on test data = {}\".format(r22))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943a2d06-39bd-4c71-b0cc-18a63d995949",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99e50b9-5090-45ed-bfa7-7ddca0706176",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gbt.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dea6a22-9f7a-483b-818e-0b4e62f723e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "grid = ParamGridBuilder()\n",
    "grid = grid.addGrid(model_gbt.maxDepth, [2, 5, 10]).addGrid(model_gbt.lossType, ['squared', 'absolute']).build()\n",
    "\n",
    "cv = CrossValidator(estimator = gbt, \n",
    "                    estimatorParamMaps = grid, \n",
    "                    evaluator = evaluator2_rmse,\n",
    "                    parallelism = 5,\n",
    "                    numFolds=3)\n",
    "\n",
    "cvModel = cv.fit(train_data)\n",
    "bestModel = cvModel.bestModel\n",
    "bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1524320f-8764-4ed4-b8ff-9d0a43b2ee29",
   "metadata": {},
   "source": [
    "## Best model 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccafef7f-5a51-4370-8572-447f06b92248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "model2 = bestModel\n",
    "pprint(model2.extractParamMap())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336a3f34-cd0b-4e7e-a0ff-8670bee1e2ef",
   "metadata": {},
   "source": [
    "## Save the model to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098bb094-67c3-4a8b-a52a-95a1f7c7bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.write().overwrite().save(\"project/models/model2\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -get project/models/model2 models/model2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2220eee3-99d3-44cc-82ab-b941fd5e5e7c",
   "metadata": {},
   "source": [
    "## Predict for test data using best model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53789d15-33a1-43b2-8e8e-df7627df5de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model2.transform(test_data)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52df5365-5d86-407f-b71a-878fff80b152",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select(\"label\", \"prediction\")\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .save(\"project/output/model2_predictions.csv\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -cat project/output/model2_predictions.csv/*.csv > output/model2_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f9e394-a65a-4cb3-b9c8-1036fd3a443a",
   "metadata": {},
   "source": [
    "## Evaluate the best model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e83b832-0088-4e95-95fc-768bc13d70ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator \n",
    "\n",
    "# Evaluate the performance of the model\n",
    "evaluator2_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator2_r2 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "rmse2 = evaluator2_rmse.evaluate(predictions)\n",
    "r22 = evaluator2_r2.evaluate(predictions)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = {}\".format(rmse2))\n",
    "print(\"R^2 on test data = {}\".format(r22))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bd7b48-f536-495a-ac38-90ea1b9014af",
   "metadata": {},
   "source": [
    "# Compare best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b91f3c3-50ca-4ce1-9dfa-dba7449fbc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [[str(model1),rmse1, r21], [str(model2),rmse2, r22]]\n",
    "\n",
    "df = spark.createDataFrame(models, [\"model\", \"RMSE\", \"R2\"])\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e008d30b-9489-44a2-84a0-95bcd9608319",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .save(\"project/output/evaluation.csv\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -cat project/output/evaluation.csv/*.csv > output/evaluation.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc-autonumbering": true,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
