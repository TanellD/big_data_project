{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0635285d-5589-4eeb-9091-9dd2143b3492",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Connect to Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4d77bfb-132e-422c-9933-69d0df5685dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/07 18:37:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/07 18:37:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/05/07 18:37:06 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/05/07 18:37:06 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "25/05/07 18:37:07 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Add here your team number teamx\n",
    "team = 19\n",
    "\n",
    "# location of your Hive database in HDFS\n",
    "# warehouse = \"project/hive/warehouse\"\n",
    "warehouse = \"/user/team19/project/warehouse\"\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"{} - spark ML\".format(team))\\\n",
    "        .master(\"yarn\")\\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hadoop-02.uni.innopolis.ru:9883\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse)\\\n",
    "        .config(\"spark.sql.avro.compression.codec\", \"snappy\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbd810cc-d172-43a6-b017-57203d8a6970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_402\"\n",
      "OpenJDK Runtime Environment (build 1.8.0_402-b06)\n",
      "OpenJDK 64-Bit Server VM (build 25.402-b06, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "# pip freeze\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab0a0875-0d8c-4f48-a65b-230e11c46d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://hadoop-01.uni.innopolis.ru:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>19 - spark ML</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fcef45ac690>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbc89ee-c475-47d1-b000-a800563a8870",
   "metadata": {},
   "source": [
    "# list Hive databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2631136-e462-4815-a807-d1bd4bc3f7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|             default|\n",
      "|             retake1|\n",
      "|             root_db|\n",
      "|                show|\n",
      "|     team0_projectdb|\n",
      "|    team11_projectdb|\n",
      "|           team12_db|\n",
      "|team12_hive_proje...|\n",
      "|    team12_projectdb|\n",
      "|    team13_projectdb|\n",
      "|    team14_projectdb|\n",
      "|    team15_projectdb|\n",
      "|    team16_projectdb|\n",
      "|    team17_projectdb|\n",
      "|    team18_projectdb|\n",
      "|    team19_projectdb|\n",
      "|     team1_projectdb|\n",
      "|    team20_projectdb|\n",
      "| team21_projectdb_v2|\n",
      "| team21_projectdb_v3|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5bf565-af4e-49d4-bf45-ca1e1baf971f",
   "metadata": {},
   "source": [
    "# Read hive tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a159eb78-3d54-4302-8937-9dae58a5dbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+----------------+--------------------+-----------+\n",
      "|       namespace|           tableName|isTemporary|\n",
      "+----------------+--------------------+-----------+\n",
      "|team19_projectdb|  evaluation_results|      false|\n",
      "|team19_projectdb|      gbt_gridsearch|      false|\n",
      "|team19_projectdb|   gbt_metrics_tuned|      false|\n",
      "|team19_projectdb|      hosts_bucketed|      false|\n",
      "|team19_projectdb|listings_partitioned|      false|\n",
      "|team19_projectdb|       lr_gridsearch|      false|\n",
      "|team19_projectdb|    lr_metrics_tuned|      false|\n",
      "|team19_projectdb|  model1_predictions|      false|\n",
      "|team19_projectdb|  model2_predictions|      false|\n",
      "|team19_projectdb|          q1_results|      false|\n",
      "|team19_projectdb|          q2_results|      false|\n",
      "|team19_projectdb|          q3_results|      false|\n",
      "|team19_projectdb|          q4_results|      false|\n",
      "|team19_projectdb|          q5_results|      false|\n",
      "|team19_projectdb|          q6_results|      false|\n",
      "|team19_projectdb|review_scores_buc...|      false|\n",
      "+----------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"USE team19_projectdb\").show()\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3376ccfd-3045-4c2d-9800-19ea2da702b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hosts_bucketed = spark.read.format(\"avro\").table('team19_projectdb.hosts_bucketed')\n",
    "listings_partitioned = spark.read.format(\"avro\").table('team19_projectdb.listings_partitioned')\n",
    "review_scores_bucketed = spark.read.format(\"avro\").table('team19_projectdb.review_scores_bucketed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4550d52e-bf88-4bb8-a79f-37540e6da717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['host_id',\n",
       " 'host_url',\n",
       " 'host_name',\n",
       " 'host_since',\n",
       " 'host_location',\n",
       " 'host_about',\n",
       " 'host_response_time',\n",
       " 'host_response_rate',\n",
       " 'host_acceptance_rate',\n",
       " 'host_listings_count',\n",
       " 'host_total_listings_count',\n",
       " 'host_verifications']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hosts_bucketed.show()\n",
    "hosts_bucketed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "656cc510-59bd-46c1-a2cd-d618ac9e0a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/07 18:37:29 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+---------+----------+-------------+----------+------------------+------------------+--------------------+-------------------+-------------------------+-------------------+\n",
      "|         host_id|            host_url|host_name|host_since|host_location|host_about|host_response_time|host_response_rate|host_acceptance_rate|host_listings_count|host_total_listings_count| host_verifications|\n",
      "+----------------+--------------------+---------+----------+-------------+----------+------------------+------------------+--------------------+-------------------+-------------------------+-------------------+\n",
      "|71592872.0000000|https://www.airbn...|      Ian|2016-05-12|         NULL|      NULL|within a few hours|       100.0000000|                NULL|          1.0000000|                1.0000000|email,phone,reviews|\n",
      "+----------------+--------------------+---------+----------+-------------+----------+------------------+------------------+--------------------+-------------------+-------------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "hosts_bucketed.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f40641c1-606e-4095-8e26-9a83e4eebd0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'listing_url',\n",
       " 'scrape_id',\n",
       " 'last_scraped',\n",
       " 'name',\n",
       " 'host_id',\n",
       " 'street',\n",
       " 'property_type',\n",
       " 'room_type',\n",
       " 'accommodates',\n",
       " 'bathrooms',\n",
       " 'bedrooms',\n",
       " 'beds',\n",
       " 'price',\n",
       " 'number_of_reviews',\n",
       " 'reviews_per_month',\n",
       " 'city',\n",
       " 'country']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listings_partitioned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9b9f2b9-1e63-4fea-a43f-ec4f24f91228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------+------------+-------------+----------------+--------------------+-------------+------------+------------+---------+---------+---------+----------+-----------------+-----------------+-----------------+-------+\n",
      "|      id|         listing_url|     scrape_id|last_scraped|         name|         host_id|              street|property_type|   room_type|accommodates|bathrooms| bedrooms|     beds|     price|number_of_reviews|reviews_per_month|             city|country|\n",
      "+--------+--------------------+--------------+------------+-------------+----------------+--------------------+-------------+------------+------------+---------+---------+---------+----------+-----------------+-----------------+-----------------+-------+\n",
      "|10908984|https://www.airbn...|20160706203047|  2016-07-07|Room \"Shanti\"|52727228.0000000|colina, Playas de...|    Apartment|Private room|   2.0000000|0.0000000|1.0000000|1.0000000|24.0000000|        3.0000000|        0.5600000|Playas de Tijuana| Mexico|\n",
      "+--------+--------------------+--------------+------------+-------------+----------------+--------------------+-------------+------------+------------+---------+---------+---------+----------+-----------------+-----------------+-----------------+-------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "listings_partitioned.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0572f66b-45c8-40d6-9f5a-b11959b080b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['listing_id',\n",
       " 'review_scores_rating',\n",
       " 'review_scores_accuracy',\n",
       " 'review_scores_cleanliness',\n",
       " 'review_scores_checkin',\n",
       " 'review_scores_communication',\n",
       " 'review_scores_location',\n",
       " 'review_scores_value']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_scores_bucketed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de700ec2-bf9e-4ad9-a925-36d9018f3b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+\n",
      "|listing_id|review_scores_rating|review_scores_accuracy|review_scores_cleanliness|review_scores_checkin|review_scores_communication|review_scores_location|review_scores_value|\n",
      "+----------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+\n",
      "|  17895559|                NULL|                  NULL|                     NULL|                 NULL|                       NULL|                  NULL|               NULL|\n",
      "+----------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_scores_bucketed.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8b697af-8c9b-4063-89bc-e9d81cf8f1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "result_df = (\n",
    "    listings_partitioned\n",
    "    .join(\n",
    "        hosts_bucketed,\n",
    "        on=\"host_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .join(\n",
    "        review_scores_bucketed,\n",
    "        on=col(\"id\") == col(\"listing_id\"),\n",
    "        how=\"left\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7e117e5-55ca-4bb8-aa10-b7e900d61f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['host_id',\n",
       " 'id',\n",
       " 'listing_url',\n",
       " 'scrape_id',\n",
       " 'last_scraped',\n",
       " 'name',\n",
       " 'street',\n",
       " 'property_type',\n",
       " 'room_type',\n",
       " 'accommodates',\n",
       " 'bathrooms',\n",
       " 'bedrooms',\n",
       " 'beds',\n",
       " 'price',\n",
       " 'number_of_reviews',\n",
       " 'reviews_per_month',\n",
       " 'city',\n",
       " 'country',\n",
       " 'host_url',\n",
       " 'host_name',\n",
       " 'host_since',\n",
       " 'host_location',\n",
       " 'host_about',\n",
       " 'host_response_time',\n",
       " 'host_response_rate',\n",
       " 'host_acceptance_rate',\n",
       " 'host_listings_count',\n",
       " 'host_total_listings_count',\n",
       " 'host_verifications',\n",
       " 'listing_id',\n",
       " 'review_scores_rating',\n",
       " 'review_scores_accuracy',\n",
       " 'review_scores_cleanliness',\n",
       " 'review_scores_checkin',\n",
       " 'review_scores_communication',\n",
       " 'review_scores_location',\n",
       " 'review_scores_value']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f399c78e-2e61-464b-81d3-d437b34ab88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:======================================================>(126 + 1) / 127]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 247476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "total_rows = result_df.count()\n",
    "print(f\"Total rows: {total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c04ab52b-2139-40b6-9f3a-dd939fdeae73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/07 18:38:01 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 19:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------\n",
      " host_id                     | 0      \n",
      " id                          | 0      \n",
      " listing_url                 | 0      \n",
      " scrape_id                   | 0      \n",
      " last_scraped                | 0      \n",
      " name                        | 223    \n",
      " street                      | 0      \n",
      " property_type               | 4      \n",
      " room_type                   | 0      \n",
      " accommodates                | 39     \n",
      " bathrooms                   | 764    \n",
      " bedrooms                    | 301    \n",
      " beds                        | 476    \n",
      " price                       | 4076   \n",
      " number_of_reviews           | 0      \n",
      " reviews_per_month           | 60968  \n",
      " city                        | 235    \n",
      " country                     | 0      \n",
      " host_url                    | 0      \n",
      " host_name                   | 260    \n",
      " host_since                  | 259    \n",
      " host_location               | 1143   \n",
      " host_about                  | 98959  \n",
      " host_response_time          | 57987  \n",
      " host_response_rate          | 57987  \n",
      " host_acceptance_rate        | 226019 \n",
      " host_listings_count         | 259    \n",
      " host_total_listings_count   | 259    \n",
      " host_verifications          | 427    \n",
      " listing_id                  | 0      \n",
      " review_scores_rating        | 63891  \n",
      " review_scores_accuracy      | 64329  \n",
      " review_scores_cleanliness   | 64208  \n",
      " review_scores_checkin       | 64492  \n",
      " review_scores_communication | 64242  \n",
      " review_scores_location      | 64475  \n",
      " review_scores_value         | 64517  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "null_counts = result_df.agg(\n",
    "    *[count(when(col(c).isNull(), c)).alias(c) for c in result_df.columns]\n",
    ").show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f56edce4-c0dd-4d19-8ef1-47226bbea636",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['last_scraped', 'street', 'property_type', 'room_type', 'accommodates','bathrooms', 'bedrooms', 'beds', 'number_of_reviews', 'reviews_per_month', 'city',\n",
    "           'country', 'host_since', 'host_location', 'host_response_time','host_response_rate','host_listings_count', 'host_total_listings_count', 'host_verifications', 'review_scores_rating',\n",
    "           'review_scores_accuracy','review_scores_cleanliness','review_scores_checkin','review_scores_communication','review_scores_location','review_scores_value']\n",
    "label = 'price'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a50f4c62-f1ff-4215-a98d-b4b593be7fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-------------+---------------+------------+---------+---------+---------+-----------------+-----------------+-------------+--------------+----------+--------------------+------------------+------------------+-------------------+-------------------------+--------------------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+-----------+\n",
      "|last_scraped|              street|property_type|      room_type|accommodates|bathrooms| bedrooms|     beds|number_of_reviews|reviews_per_month|         city|       country|host_since|       host_location|host_response_time|host_response_rate|host_listings_count|host_total_listings_count|  host_verifications|review_scores_rating|review_scores_accuracy|review_scores_cleanliness|review_scores_checkin|review_scores_communication|review_scores_location|review_scores_value|      label|\n",
      "+------------+--------------------+-------------+---------------+------------+---------+---------+---------+-----------------+-----------------+-------------+--------------+----------+--------------------+------------------+------------------+-------------------+-------------------------+--------------------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+-----------+\n",
      "|  2017-04-02|Mission District,...|    Apartment|Entire home/apt|   2.0000000|1.0000000|1.0000000|1.0000000|       87.0000000|        5.6000000|San Francisco| United States|2014-07-29|San Francisco, Ca...|      within a day|       100.0000000|          1.0000000|                1.0000000|email,phone,revie...|          95.0000000|            10.0000000|               10.0000000|           10.0000000|                  9.0000000|            10.0000000|          9.0000000|149.0000000|\n",
      "|  2017-05-04|New York, NY 1002...|    Apartment|Entire home/apt|   4.0000000|1.0000000|1.0000000|2.0000000|       10.0000000|        0.7600000|     New York| United States|2013-06-13|New York, New Yor...|    within an hour|       100.0000000|          1.0000000|                1.0000000|email,phone,faceb...|          95.0000000|            10.0000000|                9.0000000|           10.0000000|                 10.0000000|            10.0000000|         10.0000000|199.0000000|\n",
      "|  2017-05-04|Ficial District, ...|    Apartment|Entire home/apt|   5.0000000|1.0000000|1.0000000|2.0000000|        6.0000000|        0.6100000|     New York| United States|2010-05-14|New York, New Yor...|within a few hours|       100.0000000|          1.0000000|                1.0000000|email,phone,faceb...|         100.0000000|            10.0000000|                9.0000000|           10.0000000|                 10.0000000|            10.0000000|         10.0000000|275.0000000|\n",
      "|  2017-04-05|Panthéon, Paris, ...|    Apartment|Entire home/apt|   2.0000000|1.0000000|0.0000000|1.0000000|       16.0000000|        1.0100000|        Paris|        France|2012-03-12|Paris, Île-de-Fra...|      within a day|       100.0000000|          2.0000000|                2.0000000|email,phone,revie...|          95.0000000|             9.0000000|                9.0000000|           10.0000000|                 10.0000000|            10.0000000|          9.0000000| 52.0000000|\n",
      "|  2017-06-15|Copenhagen, Capit...|    Apartment|Entire home/apt|   2.0000000|1.0000000|1.0000000|1.0000000|        1.0000000|        0.1000000|   Copenhagen|       Denmark|2014-07-31|Copenhagen, Capit...|      within a day|       100.0000000|          2.0000000|                2.0000000|email,phone,faceb...|         100.0000000|            10.0000000|               10.0000000|           10.0000000|                 10.0000000|            10.0000000|         10.0000000|729.0000000|\n",
      "|  2017-05-03|Pasadena, Pasaden...|    Apartment|Entire home/apt|   4.0000000|2.0000000|2.0000000|2.0000000|       13.0000000|        0.9600000|     Pasadena| United States|2015-01-31|Pasadena, Califor...|within a few hours|       100.0000000|         12.0000000|               12.0000000|email,phone,revie...|          98.0000000|            10.0000000|               10.0000000|           10.0000000|                 10.0000000|            10.0000000|         10.0000000|127.0000000|\n",
      "|  2016-04-02|Henry Philip Aven...|    Apartment|Entire home/apt|   6.0000000|2.0000000|3.0000000|4.0000000|       12.0000000|        3.9100000|      Ballina|     Australia|2014-10-02|Ballina, New Sout...|    within an hour|       100.0000000|          1.0000000|                1.0000000|email,phone,faceb...|          97.0000000|            10.0000000|               10.0000000|           10.0000000|                 10.0000000|            10.0000000|          9.0000000|300.0000000|\n",
      "|  2017-04-04|Hurstville, Hurst...|    Apartment|   Private room|   1.0000000|1.5000000|1.0000000|1.0000000|        9.0000000|        2.0600000|   Hurstville|     Australia|2015-12-15|                  AU|within a few hours|       100.0000000|          1.0000000|                1.0000000| email,phone,reviews|          91.0000000|             9.0000000|                8.0000000|           10.0000000|                 10.0000000|            10.0000000|         10.0000000| 35.0000000|\n",
      "|  2017-03-15|Pollença, Illes B...|        House|Entire home/apt|   6.0000000|2.0000000|3.0000000|4.0000000|        6.0000000|        0.5200000|     Pollença|         Spain|2012-08-18|Palma de Mallorca...|within a few hours|        97.0000000|         92.0000000|               92.0000000|email,phone,revie...|          67.0000000|             7.0000000|                6.0000000|            9.0000000|                  9.0000000|             7.0000000|          7.0000000| 72.0000000|\n",
      "|  2016-08-08|Rue du Grand-Pré,...|    Apartment|Entire home/apt|   2.0000000|1.0000000|1.0000000|2.0000000|        3.0000000|        0.5500000|       Genève|   Switzerland|2012-05-01|Geneva, Geneva, S...|within a few hours|        86.0000000|          1.0000000|                1.0000000|email,phone,faceb...|          93.0000000|             9.0000000|                9.0000000|           10.0000000|                 10.0000000|            10.0000000|          9.0000000| 76.0000000|\n",
      "|  2017-05-09|Innere Stadt, Wie...|    Apartment|Entire home/apt|  10.0000000|5.0000000|4.0000000|9.0000000|       34.0000000|        2.0700000|         Wien|       Austria|2015-12-11|Vienna, Vienna, A...|within a few hours|       100.0000000|          2.0000000|                2.0000000|email,phone,revie...|          93.0000000|            10.0000000|               10.0000000|           10.0000000|                 10.0000000|            10.0000000|         10.0000000|200.0000000|\n",
      "|  2017-04-08|El Gòtic, Barcelo...|    Apartment|   Private room|   2.0000000|1.0000000|1.0000000|4.0000000|      141.0000000|        2.9500000|    Barcelona|         Spain|2013-03-17|Barcelona, Catalo...|      within a day|        75.0000000|          5.0000000|                5.0000000|email,phone,faceb...|          92.0000000|             9.0000000|                9.0000000|            9.0000000|                 10.0000000|             9.0000000|          9.0000000| 21.0000000|\n",
      "|  2017-04-02|Stadionbuurt, Ams...|    Apartment|Entire home/apt|   2.0000000|1.0000000|1.0000000|1.0000000|        5.0000000|        0.4500000|    Amsterdam|   Netherlands|2015-09-14|Amsterdam, North ...|      within a day|        50.0000000|          1.0000000|                1.0000000|email,phone,faceb...|         100.0000000|            10.0000000|               10.0000000|           10.0000000|                 10.0000000|            10.0000000|         10.0000000|100.0000000|\n",
      "|  2017-04-05|Batignolles, Pari...|    Apartment|Entire home/apt|   2.0000000|1.0000000|1.0000000|1.0000000|       31.0000000|        2.0200000|        Paris|        France|2012-12-03|Paris, Île-de-Fra...|    within an hour|        90.0000000|          1.0000000|                1.0000000|email,phone,faceb...|          92.0000000|            10.0000000|                9.0000000|           10.0000000|                 10.0000000|            10.0000000|          9.0000000| 85.0000000|\n",
      "|  2017-05-09|Wien, Wien 1130, ...|    Apartment|Entire home/apt|   3.0000000|1.0000000|0.0000000|1.0000000|        5.0000000|        0.3000000|         Wien|       Austria|2015-12-15|Vienna, Vienna, A...|within a few hours|       100.0000000|          1.0000000|                1.0000000| email,phone,reviews|          92.0000000|            10.0000000|               10.0000000|           10.0000000|                 10.0000000|            10.0000000|         10.0000000| 42.0000000|\n",
      "|  2017-05-09|Cannaregio, Venic...|    Apartment|   Private room|   2.0000000|1.0000000|1.0000000|1.0000000|      278.0000000|        5.5200000|       Venice|         Italy|2013-03-17|Venice, Veneto, I...|      within a day|        99.0000000|          3.0000000|                3.0000000|email,phone,revie...|          83.0000000|             9.0000000|                8.0000000|            9.0000000|                  9.0000000|             9.0000000|          8.0000000| 35.0000000|\n",
      "|  2017-03-04|Marlborough Cresc...|    Apartment|Entire home/apt|   2.0000000|1.0000000|1.0000000|1.0000000|        1.0000000|        0.4800000|   Harlington|United Kingdom|2014-04-04|              London|within a few hours|        90.0000000|         32.0000000|               32.0000000|email,phone,revie...|         100.0000000|            10.0000000|               10.0000000|           10.0000000|                 10.0000000|            10.0000000|         10.0000000| 65.0000000|\n",
      "|  2017-04-06|Paris, Île-de-Fra...|    Apartment|Entire home/apt|   2.0000000|1.0000000|1.0000000|1.0000000|       10.0000000|        1.0600000|        Paris|        France|2013-03-30|Paris, Île-de-Fra...|    within an hour|       100.0000000|          1.0000000|                1.0000000|email,phone,revie...|          80.0000000|             9.0000000|                8.0000000|            9.0000000|                  9.0000000|            10.0000000|          7.0000000| 70.0000000|\n",
      "|  2017-04-02|Amsterdam Centrum...|    Apartment|Entire home/apt|   4.0000000|1.0000000|2.0000000|2.0000000|        3.0000000|        0.2600000|    Amsterdam|   Netherlands|2015-08-10|                  IT|within a few hours|        94.0000000|          9.0000000|                9.0000000| email,phone,reviews|          73.0000000|             7.0000000|                6.0000000|            9.0000000|                  7.0000000|            10.0000000|          7.0000000|250.0000000|\n",
      "|  2017-03-04|Marlborough Cresc...|    Apartment|Entire home/apt|   2.0000000|1.0000000|1.0000000|1.0000000|        5.0000000|        0.3400000|   Harlington|United Kingdom|2014-04-04|              London|within a few hours|        90.0000000|         32.0000000|               32.0000000|email,phone,revie...|          92.0000000|            10.0000000|               10.0000000|           10.0000000|                  9.0000000|             9.0000000|          9.0000000| 65.0000000|\n",
      "+------------+--------------------+-------------+---------------+------------+---------+---------+---------+-----------------+-----------------+-------------+--------------+----------+--------------------+------------------+------------------+-------------------+-------------------------+--------------------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result_df = result_df.select(features + [label]).na.drop()\n",
    "result_df = result_df.withColumnRenamed(\"price\",\"label\")\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "224a1d3b-4c93-4898-99bc-058b80ec136f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 70:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------------------------\n",
      " summary                     | count                                \n",
      " host_id                     | 247476                               \n",
      " id                          | 247476                               \n",
      " listing_url                 | 247476                               \n",
      " scrape_id                   | 247476                               \n",
      " name                        | 247253                               \n",
      " street                      | 247476                               \n",
      " property_type               | 247472                               \n",
      " room_type                   | 247476                               \n",
      " accommodates                | 247437                               \n",
      " bathrooms                   | 246712                               \n",
      " bedrooms                    | 247175                               \n",
      " beds                        | 247000                               \n",
      " price                       | 243400                               \n",
      " number_of_reviews           | 247476                               \n",
      " reviews_per_month           | 186508                               \n",
      " city                        | 247241                               \n",
      " country                     | 247476                               \n",
      " host_url                    | 247476                               \n",
      " host_name                   | 247216                               \n",
      " host_location               | 246333                               \n",
      " host_about                  | 148517                               \n",
      " host_response_time          | 189489                               \n",
      " host_response_rate          | 189489                               \n",
      " host_acceptance_rate        | 21457                                \n",
      " host_listings_count         | 247217                               \n",
      " host_total_listings_count   | 247217                               \n",
      " host_verifications          | 247049                               \n",
      " listing_id                  | 247476                               \n",
      " review_scores_rating        | 183585                               \n",
      " review_scores_accuracy      | 183147                               \n",
      " review_scores_cleanliness   | 183268                               \n",
      " review_scores_checkin       | 182984                               \n",
      " review_scores_communication | 183234                               \n",
      " review_scores_location      | 183001                               \n",
      " review_scores_value         | 182959                               \n",
      "-RECORD 1-----------------------------------------------------------\n",
      " summary                     | mean                                 \n",
      " host_id                     | 32282739.13325332558                 \n",
      " id                          | 9847590.603484781                    \n",
      " listing_url                 | NULL                                 \n",
      " scrape_id                   | 2.016932519631094E13                 \n",
      " name                        | 336.77777777777777                   \n",
      " street                      | NULL                                 \n",
      " property_type               | NULL                                 \n",
      " room_type                   | NULL                                 \n",
      " accommodates                | 3.31007084632                        \n",
      " bathrooms                   | 1.24820235740                        \n",
      " bedrooms                    | 1.37685850106                        \n",
      " beds                        | 1.93438461538                        \n",
      " price                       | 138.29770747740346                   \n",
      " number_of_reviews           | 16.75041216118                       \n",
      " reviews_per_month           | 1.47515082463                        \n",
      " city                        | 9637.25                              \n",
      " country                     | NULL                                 \n",
      " host_url                    | NULL                                 \n",
      " host_name                   | NaN                                  \n",
      " host_location               | 9.8465362835E10                      \n",
      " host_about                  | 2691.1428571428573                   \n",
      " host_response_time          | NULL                                 \n",
      " host_response_rate          | 93.41786066737                       \n",
      " host_acceptance_rate        | NULL                                 \n",
      " host_listings_count         | 9.51282881032                        \n",
      " host_total_listings_count   | 9.51282881032                        \n",
      " host_verifications          | NULL                                 \n",
      " listing_id                  | 9847590.603484781                    \n",
      " review_scores_rating        | 92.90701310020                       \n",
      " review_scores_accuracy      | 9.52799663658                        \n",
      " review_scores_cleanliness   | 9.33008490298                        \n",
      " review_scores_checkin       | 9.69214794736                        \n",
      " review_scores_communication | 9.70900051301                        \n",
      " review_scores_location      | 9.46829252299                        \n",
      " review_scores_value         | 9.32331833908                        \n",
      "-RECORD 2-----------------------------------------------------------\n",
      " summary                     | stddev                               \n",
      " host_id                     | 3.1696530534197055E7                 \n",
      " id                          | 5401747.8027132675                   \n",
      " listing_url                 | NULL                                 \n",
      " scrape_id                   | 3.272186516472855E9                  \n",
      " name                        | 524.6974789766427                    \n",
      " street                      | NULL                                 \n",
      " property_type               | NULL                                 \n",
      " room_type                   | NULL                                 \n",
      " accommodates                | 2.0859471212519725                   \n",
      " bathrooms                   | 0.6049440848737103                   \n",
      " bedrooms                    | 0.9309279632028441                   \n",
      " beds                        | 1.4843471755090223                   \n",
      " price                       | 150.28914848592652                   \n",
      " number_of_reviews           | 32.404951032694065                   \n",
      " reviews_per_month           | 1.7633070439777259                   \n",
      " city                        | 20944.453920375374                   \n",
      " country                     | NULL                                 \n",
      " host_url                    | NULL                                 \n",
      " host_name                   | NaN                                  \n",
      " host_location               | 1.966181347650552...                 \n",
      " host_about                  | 14173.384343783728                   \n",
      " host_response_time          | NULL                                 \n",
      " host_response_rate          | 17.53345552281389                    \n",
      " host_acceptance_rate        | NULL                                 \n",
      " host_listings_count         | 57.03194433897067                    \n",
      " host_total_listings_count   | 57.03194433897067                    \n",
      " host_verifications          | NULL                                 \n",
      " listing_id                  | 5401747.8027132675                   \n",
      " review_scores_rating        | 8.516945250273995                    \n",
      " review_scores_accuracy      | 0.8528224133038332                   \n",
      " review_scores_cleanliness   | 1.0297347778791262                   \n",
      " review_scores_checkin       | 0.7260230732620283                   \n",
      " review_scores_communication | 0.7228920999974688                   \n",
      " review_scores_location      | 0.805630009604399                    \n",
      " review_scores_value         | 0.8997462538148977                   \n",
      "-RECORD 3-----------------------------------------------------------\n",
      " summary                     | min                                  \n",
      " host_id                     | 19.0000000                           \n",
      " id                          | 10000073                             \n",
      " listing_url                 | https://www.airbn...                 \n",
      " scrape_id                   | 20151011232639                       \n",
      " name                        | ! ! ! Coliving: l...                 \n",
      " street                      | (4th floor), 6 Ki...                 \n",
      " property_type               | Apartment                            \n",
      " room_type                   | Entire home/apt                      \n",
      " accommodates                | 1.0000000                            \n",
      " bathrooms                   | 0E-7                                 \n",
      " bedrooms                    | 0E-7                                 \n",
      " beds                        | 0E-7                                 \n",
      " price                       | 0.0000000                            \n",
      " number_of_reviews           | 0E-7                                 \n",
      " reviews_per_month           | 0.0100000                            \n",
      " city                        | -                                    \n",
      " country                     | Australia                            \n",
      " host_url                    | https://www.airbn...                 \n",
      " host_name                   | 'Cil                                 \n",
      " host_location               | +393392378650                        \n",
      " host_about                  | !                                    \n",
      " host_response_time          | a few days or more                   \n",
      " host_response_rate          | 0E-7                                 \n",
      " host_acceptance_rate        | 0%                                   \n",
      " host_listings_count         | 0E-7                                 \n",
      " host_total_listings_count   | 0E-7                                 \n",
      " host_verifications          | email                                \n",
      " listing_id                  | 10000073                             \n",
      " review_scores_rating        | 20.0000000                           \n",
      " review_scores_accuracy      | 2.0000000                            \n",
      " review_scores_cleanliness   | 2.0000000                            \n",
      " review_scores_checkin       | 2.0000000                            \n",
      " review_scores_communication | 2.0000000                            \n",
      " review_scores_location      | 2.0000000                            \n",
      " review_scores_value         | 2.0000000                            \n",
      "-RECORD 4-----------------------------------------------------------\n",
      " summary                     | max                                  \n",
      " host_id                     | 135020586.0000000                    \n",
      " id                          | 9999980                              \n",
      " listing_url                 | https://www.airbn...                 \n",
      " scrape_id                   | 20170615002708                       \n",
      " name                        | ，可愛3人房，轉飛機首先，適合出遊... \n",
      " street                      | 킹스턴 어폰 세임스, 잉글랜드 ...     \n",
      " property_type               | Yurt                                 \n",
      " room_type                   | Shared room                          \n",
      " accommodates                | 16.0000000                           \n",
      " bathrooms                   | 8.0000000                            \n",
      " bedrooms                    | 10.0000000                           \n",
      " beds                        | 19.0000000                           \n",
      " price                       | 999.0000000                          \n",
      " number_of_reviews           | 605.0000000                          \n",
      " reviews_per_month           | 223.0000000                          \n",
      " city                        | 홍콩                                 \n",
      " country                     | Vatican City                         \n",
      " host_url                    | https://www.airbn...                 \n",
      " host_name                   | 지아                                 \n",
      " host_location               | 홍콩,홍콩섬, 완차이,코즈웨이 베이    \n",
      " host_about                  | ️                                    \n",
      " host_response_time          | within an hour                       \n",
      " host_response_rate          | 100.0000000                          \n",
      " host_acceptance_rate        | 99%                                  \n",
      " host_listings_count         | 959.0000000                          \n",
      " host_total_listings_count   | 959.0000000                          \n",
      " host_verifications          | work_email                           \n",
      " listing_id                  | 9999980                              \n",
      " review_scores_rating        | 100.0000000                          \n",
      " review_scores_accuracy      | 10.0000000                           \n",
      " review_scores_cleanliness   | 10.0000000                           \n",
      " review_scores_checkin       | 10.0000000                           \n",
      " review_scores_communication | 10.0000000                           \n",
      " review_scores_location      | 10.0000000                           \n",
      " review_scores_value         | 10.0000000                           \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result_df.describe().show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e89290b-4487-472b-a6d8-71432c806c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 151055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "total_rows = result_df.count()\n",
    "print(f\"Total rows: {total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d93af2a-7556-43a4-8861-0ba743db967d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last_scraped',\n",
       " 'street',\n",
       " 'property_type',\n",
       " 'room_type',\n",
       " 'accommodates',\n",
       " 'bathrooms',\n",
       " 'bedrooms',\n",
       " 'beds',\n",
       " 'number_of_reviews',\n",
       " 'reviews_per_month',\n",
       " 'city',\n",
       " 'country',\n",
       " 'host_since',\n",
       " 'host_location',\n",
       " 'host_response_time',\n",
       " 'host_response_rate',\n",
       " 'host_listings_count',\n",
       " 'host_total_listings_count',\n",
       " 'host_verifications',\n",
       " 'review_scores_rating',\n",
       " 'review_scores_accuracy',\n",
       " 'review_scores_cleanliness',\n",
       " 'review_scores_checkin',\n",
       " 'review_scores_communication',\n",
       " 'review_scores_location',\n",
       " 'review_scores_value',\n",
       " 'label']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac5ab39d-2fe8-4fa2-a73f-d07a52116641",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, year, month, dayofmonth\n",
    "\n",
    "df_with_dates = result_df.withColumn(\n",
    "    \"date_parsed\",\n",
    "    to_date(\"last_scraped\", format=\"yyyy-MM-dd\")\n",
    ").withColumn(\n",
    "    \"last_scraped_year\", year(\"date_parsed\")\n",
    ").withColumn(\n",
    "    \"month\", month(\"date_parsed\")\n",
    ").withColumn(\n",
    "    \"day\", dayofmonth(\"date_parsed\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d405016-5fa4-407a-bb78-80bc89f03de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCols\n",
    "from pyspark.sql.functions import sin, cos, pi\n",
    "import math\n",
    "\n",
    "class CyclicDateEncoder(Transformer, HasInputCol, HasOutputCols):\n",
    "    def __init__(self, inputCol=None, outputCols=None):\n",
    "        super(CyclicDateEncoder, self).__init__()\n",
    "        self._set(inputCol=inputCol, outputCols=outputCols)\n",
    "        \n",
    "    def _transform(self, df):\n",
    "        input_col = self.getInputCol()\n",
    "        output_cols = self.getOutputCols()\n",
    "        if input_col == \"month\":\n",
    "            return df.withColumn(\n",
    "                output_cols[0],\n",
    "                sin(2 * pi() * col(input_col) / 12)\n",
    "            ).withColumn(\n",
    "                output_cols[1],\n",
    "                cos(2 * pi() * col(input_col) / 12)\n",
    "            )\n",
    "        elif input_col == \"day\":\n",
    "            return df.withColumn(\n",
    "                output_cols[0],\n",
    "                sin(2 * pi() * col(input_col) / 31)\n",
    "            ).withColumn(\n",
    "                output_cols[1],\n",
    "                cos(2 * pi() * col(input_col) / 31)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "efb0b907-75f8-4f96-b7d0-c506348ec005",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_encoder = CyclicDateEncoder(\n",
    "    inputCol=\"month\",\n",
    "    outputCols=[\"last_scrapped_month_sin\", \"last_scrapped_month_cos\"]\n",
    ")\n",
    "day_encoder = CyclicDateEncoder(\n",
    "    inputCol=\"day\",\n",
    "    outputCols=[\"last_scrapped_day_sin\", \"last_scrapped_day_cos\"]\n",
    ")\n",
    "df_encoded = month_encoder.transform(df_with_dates)\n",
    "df_encoded = day_encoder.transform(df_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a262ba9e-61a9-4346-97d9-51c4962b147c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last_scraped',\n",
       " 'street',\n",
       " 'property_type',\n",
       " 'room_type',\n",
       " 'accommodates',\n",
       " 'bathrooms',\n",
       " 'bedrooms',\n",
       " 'beds',\n",
       " 'number_of_reviews',\n",
       " 'reviews_per_month',\n",
       " 'city',\n",
       " 'country',\n",
       " 'host_since',\n",
       " 'host_location',\n",
       " 'host_response_time',\n",
       " 'host_response_rate',\n",
       " 'host_listings_count',\n",
       " 'host_total_listings_count',\n",
       " 'host_verifications',\n",
       " 'review_scores_rating',\n",
       " 'review_scores_accuracy',\n",
       " 'review_scores_cleanliness',\n",
       " 'review_scores_checkin',\n",
       " 'review_scores_communication',\n",
       " 'review_scores_location',\n",
       " 'review_scores_value',\n",
       " 'label',\n",
       " 'date_parsed',\n",
       " 'last_scraped_year',\n",
       " 'month',\n",
       " 'day',\n",
       " 'last_scrapped_month_sin',\n",
       " 'last_scrapped_month_cos',\n",
       " 'last_scrapped_day_sin',\n",
       " 'last_scrapped_day_cos']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e05e6d-b953-4dd6-84ae-dbeac729dc76",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00984a8d-8516-47b6-891a-85711cb5f210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['street',\n",
       " 'property_type',\n",
       " 'room_type',\n",
       " 'accommodates',\n",
       " 'bathrooms',\n",
       " 'bedrooms',\n",
       " 'beds',\n",
       " 'number_of_reviews',\n",
       " 'reviews_per_month',\n",
       " 'city',\n",
       " 'country',\n",
       " 'host_since',\n",
       " 'host_location',\n",
       " 'host_response_time',\n",
       " 'host_response_rate',\n",
       " 'host_listings_count',\n",
       " 'host_total_listings_count',\n",
       " 'host_verifications',\n",
       " 'review_scores_rating',\n",
       " 'review_scores_accuracy',\n",
       " 'review_scores_cleanliness',\n",
       " 'review_scores_checkin',\n",
       " 'review_scores_communication',\n",
       " 'review_scores_location',\n",
       " 'review_scores_value',\n",
       " 'label',\n",
       " 'last_scraped_year',\n",
       " 'last_scrapped_month_sin',\n",
       " 'last_scrapped_month_cos',\n",
       " 'last_scrapped_day_sin',\n",
       " 'last_scrapped_day_cos']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded = df_encoded.drop(\"last_scraped\", \"date_parsed\",\"day\",\"month\")\n",
    "df_encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e52010af-75f2-4a0a-af7a-09a738f04487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, year, month, dayofmonth\n",
    "\n",
    "df_with_dates = df_encoded.withColumn(\n",
    "    \"date_parsed\",\n",
    "    to_date(\"host_since\", format=\"yyyy-MM-dd\")\n",
    ").withColumn(\n",
    "    \"year_host_since\", year(\"date_parsed\")\n",
    ").withColumn(\n",
    "    \"month\", month(\"date_parsed\")\n",
    ").withColumn(\n",
    "    \"day\", dayofmonth(\"date_parsed\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2dc2595e-10ca-4d5f-9e32-5846b4f163ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['street',\n",
       " 'property_type',\n",
       " 'room_type',\n",
       " 'accommodates',\n",
       " 'bathrooms',\n",
       " 'bedrooms',\n",
       " 'beds',\n",
       " 'number_of_reviews',\n",
       " 'reviews_per_month',\n",
       " 'city',\n",
       " 'country',\n",
       " 'host_since',\n",
       " 'host_location',\n",
       " 'host_response_time',\n",
       " 'host_response_rate',\n",
       " 'host_listings_count',\n",
       " 'host_total_listings_count',\n",
       " 'host_verifications',\n",
       " 'review_scores_rating',\n",
       " 'review_scores_accuracy',\n",
       " 'review_scores_cleanliness',\n",
       " 'review_scores_checkin',\n",
       " 'review_scores_communication',\n",
       " 'review_scores_location',\n",
       " 'review_scores_value',\n",
       " 'label',\n",
       " 'last_scraped_year',\n",
       " 'last_scrapped_month_sin',\n",
       " 'last_scrapped_month_cos',\n",
       " 'last_scrapped_day_sin',\n",
       " 'last_scrapped_day_cos',\n",
       " 'date_parsed',\n",
       " 'year_host_since',\n",
       " 'month',\n",
       " 'day']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_dates.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "569ee715-b91e-4e23-8280-a4ed6dd0428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_encoder = CyclicDateEncoder(\n",
    "    inputCol=\"month\",\n",
    "    outputCols=[\"host_since_month_sin\", \"host_since_month_cos\"]\n",
    ")\n",
    "day_encoder = CyclicDateEncoder(\n",
    "    inputCol=\"day\",\n",
    "    outputCols=[\"host_since_day_sin\", \"host_since_day_cos\"]\n",
    ")\n",
    "df_encoded = month_encoder.transform(df_with_dates)\n",
    "df_encoded = day_encoder.transform(df_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2afa5bf-3cd1-4840-89b6-2a5c5fbcddc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['street',\n",
       " 'property_type',\n",
       " 'room_type',\n",
       " 'accommodates',\n",
       " 'bathrooms',\n",
       " 'bedrooms',\n",
       " 'beds',\n",
       " 'number_of_reviews',\n",
       " 'reviews_per_month',\n",
       " 'city',\n",
       " 'country',\n",
       " 'host_location',\n",
       " 'host_response_time',\n",
       " 'host_response_rate',\n",
       " 'host_listings_count',\n",
       " 'host_total_listings_count',\n",
       " 'host_verifications',\n",
       " 'review_scores_rating',\n",
       " 'review_scores_accuracy',\n",
       " 'review_scores_cleanliness',\n",
       " 'review_scores_checkin',\n",
       " 'review_scores_communication',\n",
       " 'review_scores_location',\n",
       " 'review_scores_value',\n",
       " 'label',\n",
       " 'last_scraped_year',\n",
       " 'last_scrapped_month_sin',\n",
       " 'last_scrapped_month_cos',\n",
       " 'last_scrapped_day_sin',\n",
       " 'last_scrapped_day_cos',\n",
       " 'year_host_since',\n",
       " 'host_since_month_sin',\n",
       " 'host_since_month_cos',\n",
       " 'host_since_day_sin',\n",
       " 'host_since_day_cos']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded = df_encoded.drop(\"month\", \"day\",\"date_parsed\",\"host_since\")\n",
    "df_encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8fddc322-7e55-4ea0-bbc6-3b248ad63837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['street',\n",
       " 'property_type',\n",
       " 'room_type',\n",
       " 'accommodates',\n",
       " 'bathrooms',\n",
       " 'bedrooms',\n",
       " 'beds',\n",
       " 'number_of_reviews',\n",
       " 'reviews_per_month',\n",
       " 'city',\n",
       " 'country',\n",
       " 'host_location',\n",
       " 'host_response_time',\n",
       " 'host_response_rate',\n",
       " 'host_listings_count',\n",
       " 'host_total_listings_count',\n",
       " 'host_verifications',\n",
       " 'review_scores_rating',\n",
       " 'review_scores_accuracy',\n",
       " 'review_scores_cleanliness',\n",
       " 'review_scores_checkin',\n",
       " 'review_scores_communication',\n",
       " 'review_scores_location',\n",
       " 'review_scores_value',\n",
       " 'label',\n",
       " 'last_scraped_year',\n",
       " 'last_scrapped_month_sin',\n",
       " 'last_scrapped_month_cos',\n",
       " 'last_scrapped_day_sin',\n",
       " 'last_scrapped_day_cos',\n",
       " 'year_host_since',\n",
       " 'host_since_month_sin',\n",
       " 'host_since_month_cos',\n",
       " 'host_since_day_sin',\n",
       " 'host_since_day_cos']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49a39df1-8ad5-488a-b961-4977978e7d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = df_encoded.drop(\"last_scraped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d052cf3-b6ba-40d3-b3d6-a2bb4e75e9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, Word2Vec, Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "categoricalCols = ['city','country','host_location','host_verifications', 'property_type','room_type', 'host_response_time']\n",
    "textCols = ['street']\n",
    "others = ['accommodates','bathrooms','bedrooms','beds','number_of_reviews','reviews_per_month',\n",
    "          'host_response_rate','host_listings_count', 'host_total_listings_count','review_scores_rating','review_scores_accuracy',\n",
    "          'review_scores_cleanliness', 'review_scores_checkin','review_scores_communication', 'review_scores_location', 'review_scores_value',\n",
    "         'last_scraped_year','host_since_month_sin','host_since_month_cos','host_since_day_sin','host_since_day_cos','year_host_since']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "73541678-05c4-4b26-97ae-1ebe39221ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 58:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              street|\n",
      "+--------------------+\n",
      "|Mission District,...|\n",
      "|New York, NY 1002...|\n",
      "|Ficial District, ...|\n",
      "|Panthéon, Paris, ...|\n",
      "|Copenhagen, Capit...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_encoded.select('street').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3059efa5-56a7-4616-b17a-aeb0a1871920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_count = df_encoded.select(\"city\").distinct().count()\n",
    "# print(unique_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d11b2ab4-fb01-47c9-9a00-8d89e4dd0620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_count = df_encoded.select(\"host_location\").distinct().count()\n",
    "# print(unique_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a633753-04dd-4739-96f4-682139bd8665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_count = df_encoded.select(\"host_verifications\").distinct().count()\n",
    "# print(unique_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d6ef269a-8f2d-41ba-a470-a6784ab264e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_count = df_encoded.select(\"country\").distinct().count()\n",
    "# print(unique_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52a1d86a-936a-402c-8757-af9bc86e7a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_count = df_encoded.select(\"street\").distinct().count()\n",
    "# print(unique_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5f4811a-c82f-431c-bd6d-6f696c0ff8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, Word2Vec\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "tokenizer = RegexTokenizer(\n",
    "    inputCol=\"street\", \n",
    "    outputCol=\"city_tokens\",\n",
    "    pattern=\"[,\\s]+\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "48d704b5-2d49-4565-9ecf-26c777d43e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2Vec = Word2Vec(\n",
    "    vectorSize=10,\n",
    "    minCount=1,\n",
    "    windowSize=5,\n",
    "    inputCol=\"city_tokens\",\n",
    "    outputCol=\"city_vec\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac3dc27a-2c13-4ccf-b250-0a58e4d0330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = [ StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c)).setHandleInvalid(\"skip\") for c in categoricalCols ]\n",
    "encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) for indexer in indexers ]\n",
    "assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders] + others, outputCol= \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "519c996c-a898-4430-8971-b2c0f7653d42",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf_encoded\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdescribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.show(truncate=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/sql/dataframe.py:2856\u001b[39m, in \u001b[36mDataFrame.describe\u001b[39m\u001b[34m(self, *cols)\u001b[39m\n\u001b[32m   2854\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cols) == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cols[\u001b[32m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m   2855\u001b[39m     cols = cols[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2856\u001b[39m jdf = \u001b[38;5;28mself\u001b[39m._jdf.describe(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jseq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   2857\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m.sparkSession)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/sql/dataframe.py:2753\u001b[39m, in \u001b[36mDataFrame._jseq\u001b[39m\u001b[34m(self, cols, converter)\u001b[39m\n\u001b[32m   2747\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_jseq\u001b[39m(\n\u001b[32m   2748\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2749\u001b[39m     cols: Sequence,\n\u001b[32m   2750\u001b[39m     converter: Optional[Callable[..., Union[\u001b[33m\"\u001b[39m\u001b[33mPrimitiveType\u001b[39m\u001b[33m\"\u001b[39m, JavaObject]]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   2751\u001b[39m ) -> JavaObject:\n\u001b[32m   2752\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a JVM Seq of Columns from a list of Column or names\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2753\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconverter\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/sql/column.py:90\u001b[39m, in \u001b[36m_to_seq\u001b[39m\u001b[34m(sc, cols, converter)\u001b[39m\n\u001b[32m     88\u001b[39m     cols = [converter(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols]\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPythonUtils\u001b[49m.toSeq(cols)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1712\u001b[39m, in \u001b[36mJVMView.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1709\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name == UserHelpAutoCompletion.KEY:\n\u001b[32m   1710\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[32m-> \u001b[39m\u001b[32m1712\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_gateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1713\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\n\u001b[32m   1714\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\n\u001b[32m   1715\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1716\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer == proto.SUCCESS_PACKAGE:\n\u001b[32m   1717\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m._gateway_client, jvm_id=\u001b[38;5;28mself\u001b[39m._id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1036\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry=\u001b[38;5;28;01mTrue\u001b[39;00m, binary=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1016\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[33;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[33;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1034\u001b[39m \u001b[33;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1038\u001b[39m         response = connection.send_command(command)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/clientserver.py:284\u001b[39m, in \u001b[36mJavaClient._get_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection.socket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/clientserver.py:291\u001b[39m, in \u001b[36mJavaClient._create_new_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    288\u001b[39m     connection = ClientServerConnection(\n\u001b[32m    289\u001b[39m         \u001b[38;5;28mself\u001b[39m.java_parameters, \u001b[38;5;28mself\u001b[39m.python_parameters,\n\u001b[32m    290\u001b[39m         \u001b[38;5;28mself\u001b[39m.gateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_thread_connection(connection)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/clientserver.py:438\u001b[39m, in \u001b[36mClientServerConnection.connect_to_java_server\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ssl_context:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m.socket = \u001b[38;5;28mself\u001b[39m.ssl_context.wrap_socket(\n\u001b[32m    437\u001b[39m         \u001b[38;5;28mself\u001b[39m.socket, server_hostname=\u001b[38;5;28mself\u001b[39m.java_address)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.stream = \u001b[38;5;28mself\u001b[39m.socket.makefile(\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    440\u001b[39m \u001b[38;5;28mself\u001b[39m.is_connected = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "df_encoded.describe().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b463c308-ad37-4d10-8aa0-88754439b6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/07 15:13:18 WARN DAGScheduler: Broadcasting large task binary with size 1584.1 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# You can create a pipeline to use only a single fit and transform on the data.\n",
    "pipeline = Pipeline(stages=[tokenizer, word2Vec] + indexers + encoders + [assembler])\n",
    "\n",
    "\n",
    "# Fit the pipeline ==> This will call the fit functions for all transformers if exist\n",
    "model=pipeline.fit(df_encoded)\n",
    "# Fit the pipeline ==> This will call the transform functions for all transformers\n",
    "data = model.transform(df_encoded)\n",
    "\n",
    "# data.show()\n",
    "\n",
    "# We delete all features and keep only the features and label columns\n",
    "data = data.select([\"features\", \"label\"])\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4\n",
    "# distinct values are treated as continuous.\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "transformed = featureIndexer.transform(data)\n",
    "\n",
    "# Display the output Spark DataFrame\n",
    "# transformed.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a71bafe-c88b-47d2-accc-a9ca27f3a95d",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fd3df2-8f31-42ed-8f0f-7dfcb795a5e8",
   "metadata": {},
   "source": [
    "# Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f29220b1-87c0-44ce-8665-44d7a0b09822",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = transformed.withColumn(\"label\", col(\"label\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a4aeb021-6320-4794-8524-1706700e4635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/07 15:14:04 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|            features|label|     indexedFeatures|\n",
      "+--------------------+-----+--------------------+\n",
      "|(9436,[12,3080,31...|149.0|(9436,[12,3080,31...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transformed.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a1c6e9e8-ecd1-4005-8df4-1e19729ca5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1965:============================>                           (1 + 1) / 2]4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|             label|\n",
      "+-------+------------------+\n",
      "|  count|            151055|\n",
      "|   mean|132.59360497831915|\n",
      "| stddev|141.10245841580925|\n",
      "|    min|               0.0|\n",
      "|    max|             999.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transformed.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "71ca00fb-a991-47d0-8d09-75e5604ed694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/07 15:14:20 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "25/05/07 15:14:47 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  split the data into 60% training and 40% test (it is not stratified)\n",
    "(train_data, test_data) = transformed.randomSplit([0.6, 0.4], seed = 10)\n",
    "\n",
    "def run(command):\n",
    "    import os\n",
    "    return os.popen(command).read()\n",
    "\n",
    "train_data.select(\"features\", \"label\")\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"json\")\\\n",
    "    .save(\"project/data/train\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -cat project/data/train/*.json > ../data/train.json\")\n",
    "\n",
    "test_data.select(\"features\", \"label\")\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"json\")\\\n",
    "    .save(\"project/data/test\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -cat project/data/test/*.json > ../data/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ca7b805f-a2f0-4578-af2d-a840addce535",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_partitions = 12\n",
    "train_data = train_data.repartition(optimal_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c48797e1-5b95-4c79-ab04-301db3b0e33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/07 15:15:12 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "[Stage 256:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fecd52-189d-45af-841c-47819b311918",
   "metadata": {},
   "source": [
    "# First model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daffa79-5f41-4e78-aa1a-c914f8b3f1f5",
   "metadata": {},
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9892ef1c-f7c4-4acd-8d7d-5630f0823c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/07 15:15:38 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:01 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:08 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:09 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:12 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/05/07 15:16:12 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:13 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:13 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:14 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:14 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:14 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:15 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:16 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:16 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:16 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:17 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:17 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:18 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:18 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:18 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:19 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:19 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:20 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:20 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:21 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:21 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:21 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:22 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:22 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:23 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:23 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:23 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:24 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:24 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:25 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:25 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:25 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:26 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:26 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:27 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:27 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:27 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:28 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:28 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:29 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:29 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:30 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:30 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:30 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:31 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:31 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:32 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:32 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:32 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:33 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:33 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:34 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:34 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:35 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:35 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:35 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:36 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:36 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:37 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:37 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:38 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:38 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:38 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:39 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:39 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:40 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:16:56 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/07 15:17:02 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/07 15:17:03 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "# Create Linear Regression Model\n",
    "lr = LinearRegression(maxIter=30)\n",
    "\n",
    "# Fit the data to the pipeline stages\n",
    "model_lr = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258b1396-77d4-4dd8-8777-d0e6fcbd629f",
   "metadata": {},
   "source": [
    "## Predict for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b9a0f1ff-a175-4e00-ac1b-a5476dd7dd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/07 15:17:20 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "[Stage 545:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+------------------+\n",
      "|            features|label|     indexedFeatures|        prediction|\n",
      "+--------------------+-----+--------------------+------------------+\n",
      "|(9436,[0,3082,309...| 56.0|(9436,[0,3082,309...| 76.44810687011432|\n",
      "|(9436,[0,3082,309...| 65.0|(9436,[0,3082,309...| 93.52075228571721|\n",
      "|(9436,[0,3082,309...| 99.0|(9436,[0,3082,309...|  97.8595575441559|\n",
      "|(9436,[0,3082,309...| 55.0|(9436,[0,3082,309...| 69.85328822933207|\n",
      "|(9436,[0,3082,309...|175.0|(9436,[0,3082,309...| 99.18242281728362|\n",
      "|(9436,[0,3082,309...| 70.0|(9436,[0,3082,309...| 96.90623292131295|\n",
      "|(9436,[0,3082,309...| 98.0|(9436,[0,3082,309...|107.63512722587984|\n",
      "|(9436,[0,3082,309...| 80.0|(9436,[0,3082,309...|102.37314584747764|\n",
      "|(9436,[0,3082,309...| 85.0|(9436,[0,3082,309...| 62.16993914863906|\n",
      "|(9436,[0,3082,309...| 70.0|(9436,[0,3082,309...|101.64227380466673|\n",
      "|(9436,[0,3082,309...| 85.0|(9436,[0,3082,309...| 85.63457153808577|\n",
      "|(9436,[0,3082,309...| 58.0|(9436,[0,3082,309...|  92.9919850579854|\n",
      "|(9436,[0,3082,309...|130.0|(9436,[0,3082,309...| 96.01495336567496|\n",
      "|(9436,[0,3082,309...| 83.0|(9436,[0,3082,309...| 72.26359589944786|\n",
      "|(9436,[0,3082,309...| 50.0|(9436,[0,3082,309...|100.72370730228431|\n",
      "|(9436,[0,3082,309...|108.0|(9436,[0,3082,309...|  89.0169739028288|\n",
      "|(9436,[0,3082,309...| 65.0|(9436,[0,3082,309...| 86.58470533945638|\n",
      "|(9436,[0,3082,309...| 75.0|(9436,[0,3082,309...| 86.39901836183253|\n",
      "|(9436,[0,3082,309...| 54.0|(9436,[0,3082,309...| 71.85632917004659|\n",
      "|(9436,[0,3082,309...| 50.0|(9436,[0,3082,309...| 92.58283927728917|\n",
      "+--------------------+-----+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions = model_lr.transform(test_data)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e97403-59a0-4ec4-b044-678ba6aa2b66",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c6b166ca-8b62-4995-adca-129021dd6d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/07 15:17:40 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/07 15:18:01 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "[Stage 567:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 73.45921960024111\n",
      "R^2 on test data = 0.7229996254387352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator \n",
    "\n",
    "# Evaluate the performance of the model\n",
    "evaluator1_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator1_r2 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "rmse = evaluator1_rmse.evaluate(predictions)\n",
    "r2 = evaluator1_r2.evaluate(predictions)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = {}\".format(rmse))\n",
    "print(\"R^2 on test data = {}\".format(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9e5de93a-a831-4597-a57a-7121ed5e78b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non fine-tuned first model\n",
    "metrics_filename = \"Non_fine_tuned_linear_regression_metrics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0cedb438-41c4-48e8-bb1b-a1ca5b8c7a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "metrics_data = [{\"RMSE\": rmse, \"R2\": r2, \"model\": \"linear_reg_1\"}]\n",
    "metrics_df = spark.createDataFrame(metrics_data)\n",
    "metrics_df.write.format(\"json\").mode(\"overwrite\").save(f\"project/big_data_project/output/{metrics_filename}\")\n",
    "# model2.write().overwrite().save(\"project/big_data_project/models/model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "704e1cbf-2773-4f0e-aab8-0f9bb028d977",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get: `../output/model1_no_tune/Non_fine_tuned_linear_regression_metrics/_SUCCESS': File exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(\"hdfs dfs -get project/big_data_project/output/Non_fine_tuned_linear_regression_metrics ../output/model1_no_tune\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b0e075-c37c-492a-b894-2f64d4dea72b",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7e04437d-2bf9-4803-83cc-b7ed17d311e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Param(parent='LinearRegression_c6603fbad9a8', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'),\n",
       " Param(parent='LinearRegression_c6603fbad9a8', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'),\n",
       " Param(parent='LinearRegression_c6603fbad9a8', name='epsilon', doc='The shape parameter to control the amount of robustness. Must be > 1.0. Only valid when loss is huber'),\n",
       " Param(parent='LinearRegression_c6603fbad9a8', name='featuresCol', doc='features column name.'),\n",
       " Param(parent='LinearRegression_c6603fbad9a8', name='fitIntercept', doc='whether to fit an intercept term.'),\n",
       " Param(parent='LinearRegression_c6603fbad9a8', name='labelCol', doc='label column name.'),\n",
       " Param(parent='LinearRegression_c6603fbad9a8', name='loss', doc='The loss function to be optimized. Supported options: squaredError, huber.'),\n",
       " Param(parent='LinearRegression_c6603fbad9a8', name='maxBlockSizeInMB', doc='maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0.'),\n",
       " Param(parent='LinearRegression_c6603fbad9a8', name='maxIter', doc='max number of iterations (>= 0).'),\n",
       " Param(parent='LinearRegression_c6603fbad9a8', name='predictionCol', doc='prediction column name.'),\n",
       " Param(parent='LinearRegression_c6603fbad9a8', name='regParam', doc='regularization parameter (>= 0).'),\n",
       " Param(parent='LinearRegression_c6603fbad9a8', name='solver', doc='The solver algorithm for optimization. Supported options: auto, normal, l-bfgs.'),\n",
       " Param(parent='LinearRegression_c6603fbad9a8', name='standardization', doc='whether to standardize the training features before fitting the model.'),\n",
       " Param(parent='LinearRegression_c6603fbad9a8', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'),\n",
       " Param(parent='LinearRegression_c6603fbad9a8', name='weightCol', doc='weight column name. If this is not set or empty, we treat all instance weights as 1.0.')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "22f3b23c-4d4b-49b0-b0d2-379f7ed0be27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, label: double, indexedFeatures: vector]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f614ca-d122-40d3-a06f-2b32bd182188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator \n",
    "\n",
    "import numpy as np\n",
    "grid = ParamGridBuilder()\\\n",
    "    .addGrid(model_lr.regParam, np.logspace(-2, -1, 2))\\\n",
    "    .addGrid(lr.elasticNetParam, [0.2, 0.6])\\\n",
    "    .build()\n",
    "\n",
    "cv = CrossValidator(estimator = lr, \n",
    "                    estimatorParamMaps = grid, \n",
    "                    evaluator = evaluator1_rmse,\n",
    "                    parallelism = 5,\n",
    "                    numFolds=3)\n",
    "\n",
    "cvModel = cv.fit(train_data)\n",
    "bestModel = cvModel.bestModel\n",
    "bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e5ef5756-2bb5-4340-9cfb-edcfd50514fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Results:\n",
      "+---------+-----------------+-----------------+\n",
      "|reg_param|elastic_net_param|rmse             |\n",
      "+---------+-----------------+-----------------+\n",
      "|0.01     |0.2              |75.12670885936343|\n",
      "|0.01     |0.6              |75.06411943921381|\n",
      "|0.1      |0.2              |74.88251784248949|\n",
      "|0.1      |0.6              |74.43698490144723|\n",
      "+---------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "# Create a list of Rows with parameter combinations and metrics\n",
    "cv_results = []\n",
    "for i, param_combo in enumerate(grid):\n",
    "    cv_results.append(Row(\n",
    "        reg_param=float(param_combo[lr.regParam]),\n",
    "        elastic_net_param=float(param_combo[lr.elasticNetParam]),\n",
    "        rmse=float(cvModel.avgMetrics[i])\n",
    "    ))\n",
    "\n",
    "# Create a Spark DataFrame from the results\n",
    "cv_results_df = spark.createDataFrame(cv_results)\n",
    "\n",
    "# Show the results\n",
    "print(\"Cross-Validation Results:\")\n",
    "cv_results_df.show(truncate=False)\n",
    "# Optionally save the results\n",
    "cv_results_df.coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"project/big_data_project/output/cv_results_lr\")\n",
    "run(\"hdfs dfs -get project/big_data_project/output/cv_results_lr ./output/model1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a27e31c-959e-438c-bec0-1f109d7e2c15",
   "metadata": {},
   "source": [
    "## Best model 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8d86fc18-eb29-4f03-bfef-ed3aa1daa4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Param(parent='LinearRegression_a9df76969288', name='predictionCol', doc='prediction column name.'): 'prediction',\n",
      " Param(parent='LinearRegression_a9df76969288', name='regParam', doc='regularization parameter (>= 0).'): 0.1,\n",
      " Param(parent='LinearRegression_a9df76969288', name='maxIter', doc='max number of iterations (>= 0).'): 30,\n",
      " Param(parent='LinearRegression_a9df76969288', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06,\n",
      " Param(parent='LinearRegression_a9df76969288', name='solver', doc='The solver algorithm for optimization. Supported options: auto, normal, l-bfgs.'): 'auto',\n",
      " Param(parent='LinearRegression_a9df76969288', name='standardization', doc='whether to standardize the training features before fitting the model.'): True,\n",
      " Param(parent='LinearRegression_a9df76969288', name='fitIntercept', doc='whether to fit an intercept term.'): True,\n",
      " Param(parent='LinearRegression_a9df76969288', name='loss', doc='The loss function to be optimized. Supported options: squaredError, huber.'): 'squaredError',\n",
      " Param(parent='LinearRegression_a9df76969288', name='labelCol', doc='label column name.'): 'label',\n",
      " Param(parent='LinearRegression_a9df76969288', name='maxBlockSizeInMB', doc='maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0.'): 0.0,\n",
      " Param(parent='LinearRegression_a9df76969288', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.6,\n",
      " Param(parent='LinearRegression_a9df76969288', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2,\n",
      " Param(parent='LinearRegression_a9df76969288', name='epsilon', doc='The shape parameter to control the amount of robustness. Must be > 1.0. Only valid when loss is huber'): 1.35,\n",
      " Param(parent='LinearRegression_a9df76969288', name='featuresCol', doc='features column name.'): 'features'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "model1 = bestModel\n",
    "pprint(model1.extractParamMap())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632f666a-f792-4b50-b38f-b39f9208ab25",
   "metadata": {},
   "source": [
    "## Save the model to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e562da25-53d1-46dc-ac5d-68a6e9041927",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get: `model1/model1/data/_SUCCESS': File exists                                 \n",
      "get: `model1/model1/metadata/_SUCCESS': File exists\n",
      "get: `model1/model1/metadata/part-00000': File exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.write().overwrite().save(\"project/big_data_project/models/model1\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -get project/big_data_project/models/model1 model1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2d223183-464e-44eb-ad3a-efa40af253a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run(\"hdfs dfs -ls project/big_data_project/models/model1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e9b220-54e1-49ce-802c-fc005d8f63c3",
   "metadata": {},
   "source": [
    "## Predict for test data using best model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cf8a05de-dcae-4bb0-8cf1-de760a62e0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/06 09:49:21 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "[Stage 2121:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+------------------+\n",
      "|            features|label|     indexedFeatures|        prediction|\n",
      "+--------------------+-----+--------------------+------------------+\n",
      "|(9436,[0,3082,309...| 56.0|(9436,[0,3082,309...| 74.49045917259855|\n",
      "|(9436,[0,3082,309...| 65.0|(9436,[0,3082,309...| 91.17054378757439|\n",
      "|(9436,[0,3082,309...| 99.0|(9436,[0,3082,309...| 97.21836977507473|\n",
      "|(9436,[0,3082,309...| 55.0|(9436,[0,3082,309...| 67.71750060563363|\n",
      "|(9436,[0,3082,309...|175.0|(9436,[0,3082,309...| 98.25505187191948|\n",
      "|(9436,[0,3082,309...| 70.0|(9436,[0,3082,309...| 96.12909031431809|\n",
      "|(9436,[0,3082,309...| 98.0|(9436,[0,3082,309...|105.55048234974811|\n",
      "|(9436,[0,3082,309...| 80.0|(9436,[0,3082,309...| 101.4196470645652|\n",
      "|(9436,[0,3082,309...| 85.0|(9436,[0,3082,309...| 61.58927615376342|\n",
      "|(9436,[0,3082,309...| 70.0|(9436,[0,3082,309...|100.66625029541183|\n",
      "|(9436,[0,3082,309...| 85.0|(9436,[0,3082,309...| 85.12105835718285|\n",
      "|(9436,[0,3082,309...| 58.0|(9436,[0,3082,309...| 91.63572862718729|\n",
      "|(9436,[0,3082,309...|130.0|(9436,[0,3082,309...| 95.15478203622979|\n",
      "|(9436,[0,3082,309...| 83.0|(9436,[0,3082,309...| 72.25439275387225|\n",
      "|(9436,[0,3082,309...| 50.0|(9436,[0,3082,309...|100.02315892759361|\n",
      "|(9436,[0,3082,309...|108.0|(9436,[0,3082,309...| 88.42846988417341|\n",
      "|(9436,[0,3082,309...| 65.0|(9436,[0,3082,309...| 85.71640857323973|\n",
      "|(9436,[0,3082,309...| 75.0|(9436,[0,3082,309...|  85.5512621328985|\n",
      "|(9436,[0,3082,309...| 54.0|(9436,[0,3082,309...|  72.1005713787631|\n",
      "|(9436,[0,3082,309...| 50.0|(9436,[0,3082,309...| 92.15524905049551|\n",
      "+--------------------+-----+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions = model1.transform(test_data)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "096936c5-a575-48b7-92d2-be22893e892c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/06 09:49:36 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.select(\"label\", \"prediction\")\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .save(\"project/big_data_project/output/model1_predictions.csv\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -cat project/big_data_project/output/model1_predictions.csv/*.csv > ../output/model1_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f9a43d-80ce-4ec1-930f-671a240673d9",
   "metadata": {},
   "source": [
    "## Evaluate the best model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1d0cc0e6-7d0d-4030-a372-375e5e9b84f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/06 09:49:56 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/06 09:50:11 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "[Stage 2154:>                                                       (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 72.84485892524282\n",
      "R^2 on test data = 0.7276135190670916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator \n",
    "\n",
    "# Evaluate the performance of the model\n",
    "evaluator1_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator1_r2 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "rmse1 = evaluator1_rmse.evaluate(predictions)\n",
    "r21 = evaluator1_r2.evaluate(predictions)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = {}\".format(rmse1))\n",
    "print(\"R^2 on test data = {}\".format(r21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "55fbd052-40dd-43e7-95d6-59c91ea4c22f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_filename = \"fine_tuned_linear_regression_metrics\"\n",
    "metrics_data = [{\"RMSE\": rmse1, \"R2\": r21, \"model\": \"linear_reg_tuned\"}]\n",
    "metrics_df = spark.createDataFrame(metrics_data)\n",
    "metrics_df.write.format(\"json\").mode(\"overwrite\").save(f\"project/big_data_project/output/{metrics_filename}\")\n",
    "run(\"hdfs dfs -get project/big_data_project/output/fine_tuned_linear_regression_metrics ../output/model1_tune\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5308587c-3e52-41c6-ba7d-494d53cc45b1",
   "metadata": {},
   "source": [
    "# Second model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d929b5a0-a159-4c94-ae56-086d3cf52df0",
   "metadata": {},
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "55d8981a-e274-4ab7-9d03-fefdc068b51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/06 09:50:14 WARN DAGScheduler: Broadcasting large task binary with size 1630.7 KiB\n",
      "25/05/06 09:50:14 WARN DAGScheduler: Broadcasting large task binary with size 1630.8 KiB\n",
      "25/05/06 09:50:14 WARN DAGScheduler: Broadcasting large task binary with size 1634.9 KiB\n",
      "25/05/06 09:50:15 WARN DAGScheduler: Broadcasting large task binary with size 1895.2 KiB\n",
      "25/05/06 09:50:28 WARN DAGScheduler: Broadcasting large task binary with size 1896.1 KiB\n",
      "25/05/06 09:50:32 WARN DAGScheduler: Broadcasting large task binary with size 1896.8 KiB\n",
      "25/05/06 09:50:35 WARN DAGScheduler: Broadcasting large task binary with size 1898.0 KiB\n",
      "25/05/06 09:50:39 WARN DAGScheduler: Broadcasting large task binary with size 1900.4 KiB\n",
      "25/05/06 09:50:44 WARN DAGScheduler: Broadcasting large task binary with size 1907.8 KiB\n",
      "25/05/06 09:50:50 WARN DAGScheduler: Broadcasting large task binary with size 1908.3 KiB\n",
      "25/05/06 09:50:54 WARN DAGScheduler: Broadcasting large task binary with size 1908.9 KiB\n",
      "25/05/06 09:50:58 WARN DAGScheduler: Broadcasting large task binary with size 1910.1 KiB\n",
      "25/05/06 09:51:02 WARN DAGScheduler: Broadcasting large task binary with size 1912.4 KiB\n",
      "25/05/06 09:51:07 WARN DAGScheduler: Broadcasting large task binary with size 1915.4 KiB\n",
      "25/05/06 09:51:13 WARN DAGScheduler: Broadcasting large task binary with size 1915.9 KiB\n",
      "25/05/06 09:51:17 WARN DAGScheduler: Broadcasting large task binary with size 1916.5 KiB\n",
      "25/05/06 09:51:21 WARN DAGScheduler: Broadcasting large task binary with size 1917.6 KiB\n",
      "25/05/06 09:51:25 WARN DAGScheduler: Broadcasting large task binary with size 1920.0 KiB\n",
      "25/05/06 09:51:30 WARN DAGScheduler: Broadcasting large task binary with size 1922.9 KiB\n",
      "25/05/06 09:51:36 WARN DAGScheduler: Broadcasting large task binary with size 1923.4 KiB\n",
      "25/05/06 09:51:40 WARN DAGScheduler: Broadcasting large task binary with size 1924.0 KiB\n",
      "25/05/06 09:51:44 WARN DAGScheduler: Broadcasting large task binary with size 1925.2 KiB\n",
      "25/05/06 09:51:48 WARN DAGScheduler: Broadcasting large task binary with size 1927.2 KiB\n",
      "25/05/06 09:51:52 WARN DAGScheduler: Broadcasting large task binary with size 1929.7 KiB\n",
      "25/05/06 09:51:58 WARN DAGScheduler: Broadcasting large task binary with size 1930.2 KiB\n",
      "25/05/06 09:52:03 WARN DAGScheduler: Broadcasting large task binary with size 1930.8 KiB\n",
      "25/05/06 09:52:06 WARN DAGScheduler: Broadcasting large task binary with size 1932.0 KiB\n",
      "25/05/06 09:52:11 WARN DAGScheduler: Broadcasting large task binary with size 1934.3 KiB\n",
      "25/05/06 09:52:16 WARN DAGScheduler: Broadcasting large task binary with size 1937.0 KiB\n",
      "25/05/06 09:52:22 WARN DAGScheduler: Broadcasting large task binary with size 1937.5 KiB\n",
      "25/05/06 09:52:26 WARN DAGScheduler: Broadcasting large task binary with size 1938.1 KiB\n",
      "25/05/06 09:52:30 WARN DAGScheduler: Broadcasting large task binary with size 1939.3 KiB\n",
      "25/05/06 09:52:34 WARN DAGScheduler: Broadcasting large task binary with size 1941.8 KiB\n",
      "25/05/06 09:52:39 WARN DAGScheduler: Broadcasting large task binary with size 1944.6 KiB\n",
      "25/05/06 09:52:45 WARN DAGScheduler: Broadcasting large task binary with size 1945.2 KiB\n",
      "25/05/06 09:52:49 WARN DAGScheduler: Broadcasting large task binary with size 1945.8 KiB\n",
      "25/05/06 09:52:53 WARN DAGScheduler: Broadcasting large task binary with size 1947.0 KiB\n",
      "25/05/06 09:52:57 WARN DAGScheduler: Broadcasting large task binary with size 1949.3 KiB\n",
      "25/05/06 09:53:02 WARN DAGScheduler: Broadcasting large task binary with size 1952.0 KiB\n",
      "25/05/06 09:53:08 WARN DAGScheduler: Broadcasting large task binary with size 1952.5 KiB\n",
      "25/05/06 09:53:12 WARN DAGScheduler: Broadcasting large task binary with size 1953.1 KiB\n",
      "25/05/06 09:53:16 WARN DAGScheduler: Broadcasting large task binary with size 1954.3 KiB\n",
      "25/05/06 09:53:20 WARN DAGScheduler: Broadcasting large task binary with size 1956.6 KiB\n",
      "25/05/06 09:53:25 WARN DAGScheduler: Broadcasting large task binary with size 1959.0 KiB\n",
      "25/05/06 09:53:31 WARN DAGScheduler: Broadcasting large task binary with size 1959.5 KiB\n",
      "25/05/06 09:53:35 WARN DAGScheduler: Broadcasting large task binary with size 1960.1 KiB\n",
      "25/05/06 09:53:39 WARN DAGScheduler: Broadcasting large task binary with size 1961.3 KiB\n",
      "25/05/06 09:53:44 WARN DAGScheduler: Broadcasting large task binary with size 1963.6 KiB\n",
      "25/05/06 09:53:49 WARN DAGScheduler: Broadcasting large task binary with size 1966.4 KiB\n",
      "25/05/06 09:53:55 WARN DAGScheduler: Broadcasting large task binary with size 1966.9 KiB\n",
      "25/05/06 09:53:58 WARN DAGScheduler: Broadcasting large task binary with size 1967.5 KiB\n",
      "25/05/06 09:54:02 WARN DAGScheduler: Broadcasting large task binary with size 1968.8 KiB\n",
      "25/05/06 09:54:06 WARN DAGScheduler: Broadcasting large task binary with size 1971.1 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "gbt = GBTRegressor(maxIter=10)\n",
    "model_gbt = gbt.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1c9a67-fe61-49fe-898f-017df0c4006b",
   "metadata": {},
   "source": [
    "## Predict for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6cb95001-9d15-4da4-98df-2b87c0c39200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/06 09:54:24 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "[Stage 2534:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+-----------------+\n",
      "|            features|label|     indexedFeatures|       prediction|\n",
      "+--------------------+-----+--------------------+-----------------+\n",
      "|(9436,[0,3082,309...| 56.0|(9436,[0,3082,309...|87.58780532556878|\n",
      "|(9436,[0,3082,309...| 65.0|(9436,[0,3082,309...|89.69886703721815|\n",
      "|(9436,[0,3082,309...| 99.0|(9436,[0,3082,309...|86.72297685617565|\n",
      "|(9436,[0,3082,309...| 55.0|(9436,[0,3082,309...|78.27495048004113|\n",
      "|(9436,[0,3082,309...|175.0|(9436,[0,3082,309...|86.72297685617565|\n",
      "|(9436,[0,3082,309...| 70.0|(9436,[0,3082,309...|87.58780532556878|\n",
      "|(9436,[0,3082,309...| 98.0|(9436,[0,3082,309...|88.83403856782502|\n",
      "|(9436,[0,3082,309...| 80.0|(9436,[0,3082,309...|88.83403856782502|\n",
      "|(9436,[0,3082,309...| 85.0|(9436,[0,3082,309...|78.07007380111293|\n",
      "|(9436,[0,3082,309...| 70.0|(9436,[0,3082,309...|88.83403856782502|\n",
      "|(9436,[0,3082,309...| 85.0|(9436,[0,3082,309...|78.27495048004113|\n",
      "|(9436,[0,3082,309...| 58.0|(9436,[0,3082,309...|87.58780532556878|\n",
      "|(9436,[0,3082,309...|130.0|(9436,[0,3082,309...|87.58780532556878|\n",
      "|(9436,[0,3082,309...| 83.0|(9436,[0,3082,309...|89.25915719501516|\n",
      "|(9436,[0,3082,309...| 50.0|(9436,[0,3082,309...|88.83403856782502|\n",
      "|(9436,[0,3082,309...|108.0|(9436,[0,3082,309...|87.58780532556878|\n",
      "|(9436,[0,3082,309...| 65.0|(9436,[0,3082,309...|78.27495048004113|\n",
      "|(9436,[0,3082,309...| 75.0|(9436,[0,3082,309...|78.27495048004113|\n",
      "|(9436,[0,3082,309...| 54.0|(9436,[0,3082,309...|89.25915719501516|\n",
      "|(9436,[0,3082,309...| 50.0|(9436,[0,3082,309...|87.58780532556878|\n",
      "+--------------------+-----+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions = model_gbt.transform(test_data)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c94da6f-f913-45ec-a377-d9c42dacb075",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f3508997-af5a-4882-8fd6-8d0a5be07ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/06 09:54:40 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/06 09:54:55 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "[Stage 2556:============================>                           (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 69.91688790908475\n",
      "R^2 on test data = 0.7490703886968049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator \n",
    "\n",
    "# Evaluate the performance of the model\n",
    "evaluator2_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator2_r2 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "rmse2 = evaluator2_rmse.evaluate(predictions)\n",
    "r22 = evaluator2_r2.evaluate(predictions)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = {}\".format(rmse2))\n",
    "print(\"R^2 on test data = {}\".format(r22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0e39080c-bf8e-438c-b010-2c5ad9ba3cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save metrics \n",
    "metrics_filename = \"GBT_no_tune\"\n",
    "metrics_data = [{\"RMSE\": rmse2, \"R2\": r22, \"model\": \"GBT_no_tune\"}]\n",
    "metrics_df = spark.createDataFrame(metrics_data)\n",
    "metrics_df.write.format(\"json\").mode(\"overwrite\").save(f\"project/big_data_project/output/{metrics_filename}\")\n",
    "run(\"hdfs dfs -get project/big_data_project/output/GBT_no_tune ../output/model2_no_tune\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943a2d06-39bd-4c71-b0cc-18a63d995949",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b99e50b9-5090-45ed-bfa7-7ddca0706176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Param(parent='GBTRegressor_2054749e2086', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval.'),\n",
       " Param(parent='GBTRegressor_2054749e2086', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext.'),\n",
       " Param(parent='GBTRegressor_2054749e2086', name='featureSubsetStrategy', doc=\"The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto'\"),\n",
       " Param(parent='GBTRegressor_2054749e2086', name='featuresCol', doc='features column name.'),\n",
       " Param(parent='GBTRegressor_2054749e2086', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: variance'),\n",
       " Param(parent='GBTRegressor_2054749e2086', name='labelCol', doc='label column name.'),\n",
       " Param(parent='GBTRegressor_2054749e2086', name='leafCol', doc='Leaf indices column name. Predicted leaf index of each instance in each tree by preorder.'),\n",
       " Param(parent='GBTRegressor_2054749e2086', name='lossType', doc='Loss function which GBT tries to minimize (case-insensitive). Supported options: squared, absolute'),\n",
       " Param(parent='GBTRegressor_2054749e2086', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'),\n",
       " Param(parent='GBTRegressor_2054749e2086', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'),\n",
       " Param(parent='GBTRegressor_2054749e2086', name='maxIter', doc='max number of iterations (>= 0).'),\n",
       " Param(parent='GBTRegressor_2054749e2086', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size.'),\n",
       " Param(parent='GBTRegressor_2054749e2086', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'),\n",
       " Param(parent='GBTRegressor_2054749e2086', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'),\n",
       " Param(parent='GBTRegressor_2054749e2086', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'),\n",
       " Param(parent='GBTRegressor_2054749e2086', name='predictionCol', doc='prediction column name.'),\n",
       " Param(parent='GBTRegressor_2054749e2086', name='seed', doc='random seed.'),\n",
       " Param(parent='GBTRegressor_2054749e2086', name='stepSize', doc='Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator.'),\n",
       " Param(parent='GBTRegressor_2054749e2086', name='subsamplingRate', doc='Fraction of the training data used for learning each decision tree, in range (0, 1].'),\n",
       " Param(parent='GBTRegressor_2054749e2086', name='validationIndicatorCol', doc='name of the column that indicates whether each row is for training or for validation. False indicates training; true indicates validation.'),\n",
       " Param(parent='GBTRegressor_2054749e2086', name='validationTol', doc='Threshold for stopping early when fit with validation is used. If the error rate on the validation input changes by less than the validationTol, then learning will stop early (before `maxIter`). This parameter is ignored when fit without validation is used.'),\n",
       " Param(parent='GBTRegressor_2054749e2086', name='weightCol', doc='weight column name. If this is not set or empty, we treat all instance weights as 1.0.')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gbt.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5dea6a22-9f7a-483b-818e-0b4e62f723e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/06 09:54:59 WARN DAGScheduler: Broadcasting large task binary with size 1608.5 KiB\n",
      "25/05/06 09:54:59 WARN DAGScheduler: Broadcasting large task binary with size 1608.5 KiB\n",
      "25/05/06 09:54:59 WARN DAGScheduler: Broadcasting large task binary with size 1608.5 KiB\n",
      "25/05/06 09:54:59 WARN DAGScheduler: Broadcasting large task binary with size 1608.5 KiB\n",
      "25/05/06 09:54:59 WARN DAGScheduler: Broadcasting large task binary with size 1648.4 KiB\n",
      "25/05/06 09:54:59 WARN DAGScheduler: Broadcasting large task binary with size 1648.4 KiB\n",
      "25/05/06 09:54:59 WARN DAGScheduler: Broadcasting large task binary with size 1648.4 KiB\n",
      "25/05/06 09:55:00 WARN DAGScheduler: Broadcasting large task binary with size 1648.4 KiB\n",
      "25/05/06 09:55:00 WARN DAGScheduler: Broadcasting large task binary with size 1648.4 KiB\n",
      "25/05/06 09:55:00 WARN DAGScheduler: Broadcasting large task binary with size 1648.4 KiB\n",
      "25/05/06 09:55:00 WARN DAGScheduler: Broadcasting large task binary with size 1648.4 KiB\n",
      "25/05/06 09:55:00 WARN DAGScheduler: Broadcasting large task binary with size 1652.6 KiB\n",
      "25/05/06 09:55:00 WARN DAGScheduler: Broadcasting large task binary with size 1652.6 KiB\n",
      "25/05/06 09:55:00 WARN DAGScheduler: Broadcasting large task binary with size 1652.6 KiB\n",
      "25/05/06 09:55:00 WARN DAGScheduler: Broadcasting large task binary with size 1648.4 KiB\n",
      "25/05/06 09:55:01 WARN DAGScheduler: Broadcasting large task binary with size 1912.9 KiB\n",
      "25/05/06 09:55:01 WARN DAGScheduler: Broadcasting large task binary with size 1912.9 KiB\n",
      "25/05/06 09:55:01 WARN DAGScheduler: Broadcasting large task binary with size 1912.9 KiB\n",
      "25/05/06 09:55:01 WARN DAGScheduler: Broadcasting large task binary with size 1652.6 KiB\n",
      "25/05/06 09:55:10 WARN DAGScheduler: Broadcasting large task binary with size 1913.8 KiB\n",
      "25/05/06 09:55:20 WARN DAGScheduler: Broadcasting large task binary with size 1913.8 KiB\n",
      "25/05/06 09:55:28 WARN DAGScheduler: Broadcasting large task binary with size 1913.8 KiB\n",
      "25/05/06 09:55:28 WARN DAGScheduler: Broadcasting large task binary with size 1912.9 KiB\n",
      "25/05/06 09:55:31 WARN DAGScheduler: Broadcasting large task binary with size 1914.5 KiB\n",
      "25/05/06 09:55:33 WARN DAGScheduler: Broadcasting large task binary with size 1914.5 KiB\n",
      "25/05/06 09:55:36 WARN DAGScheduler: Broadcasting large task binary with size 1914.5 KiB\n",
      "25/05/06 09:55:45 WARN DAGScheduler: Broadcasting large task binary with size 1913.8 KiB\n",
      "25/05/06 09:55:48 WARN DAGScheduler: Broadcasting large task binary with size 1915.7 KiB\n",
      "25/05/06 09:55:51 WARN DAGScheduler: Broadcasting large task binary with size 1921.2 KiB\n",
      "25/05/06 09:55:54 WARN DAGScheduler: Broadcasting large task binary with size 1915.7 KiB\n",
      "25/05/06 09:55:57 WARN DAGScheduler: Broadcasting large task binary with size 1914.5 KiB\n",
      "25/05/06 09:56:01 WARN DAGScheduler: Broadcasting large task binary with size 1918.1 KiB\n",
      "25/05/06 09:56:04 WARN DAGScheduler: Broadcasting large task binary with size 1921.8 KiB\n",
      "25/05/06 09:56:08 WARN DAGScheduler: Broadcasting large task binary with size 1918.1 KiB\n",
      "25/05/06 09:56:11 WARN DAGScheduler: Broadcasting large task binary with size 1921.2 KiB\n",
      "25/05/06 09:56:15 WARN DAGScheduler: Broadcasting large task binary with size 1925.5 KiB\n",
      "25/05/06 09:56:18 WARN DAGScheduler: Broadcasting large task binary with size 1922.4 KiB\n",
      "25/05/06 09:56:23 WARN DAGScheduler: Broadcasting large task binary with size 1925.5 KiB\n",
      "25/05/06 09:56:26 WARN DAGScheduler: Broadcasting large task binary with size 1921.8 KiB\n",
      "25/05/06 09:56:30 WARN DAGScheduler: Broadcasting large task binary with size 1926.0 KiB\n",
      "25/05/06 09:56:33 WARN DAGScheduler: Broadcasting large task binary with size 1924.8 KiB\n",
      "25/05/06 09:56:37 WARN DAGScheduler: Broadcasting large task binary with size 1926.0 KiB\n",
      "25/05/06 09:56:41 WARN DAGScheduler: Broadcasting large task binary with size 1922.4 KiB\n",
      "25/05/06 09:56:44 WARN DAGScheduler: Broadcasting large task binary with size 1926.6 KiB\n",
      "25/05/06 09:56:47 WARN DAGScheduler: Broadcasting large task binary with size 1925.4 KiB\n",
      "25/05/06 09:56:50 WARN DAGScheduler: Broadcasting large task binary with size 1926.6 KiB\n",
      "25/05/06 09:56:53 WARN DAGScheduler: Broadcasting large task binary with size 1924.8 KiB\n",
      "25/05/06 09:56:57 WARN DAGScheduler: Broadcasting large task binary with size 1927.8 KiB\n",
      "25/05/06 09:57:00 WARN DAGScheduler: Broadcasting large task binary with size 1926.0 KiB\n",
      "25/05/06 09:57:03 WARN DAGScheduler: Broadcasting large task binary with size 1927.8 KiB\n",
      "25/05/06 09:57:07 WARN DAGScheduler: Broadcasting large task binary with size 1925.4 KiB\n",
      "25/05/06 09:57:11 WARN DAGScheduler: Broadcasting large task binary with size 1930.3 KiB\n",
      "25/05/06 09:57:14 WARN DAGScheduler: Broadcasting large task binary with size 1928.3 KiB\n",
      "25/05/06 09:57:17 WARN DAGScheduler: Broadcasting large task binary with size 1930.3 KiB\n",
      "25/05/06 09:57:20 WARN DAGScheduler: Broadcasting large task binary with size 1926.0 KiB\n",
      "25/05/06 09:57:25 WARN DAGScheduler: Broadcasting large task binary with size 1933.1 KiB\n",
      "25/05/06 09:57:29 WARN DAGScheduler: Broadcasting large task binary with size 1928.7 KiB\n",
      "25/05/06 09:57:33 WARN DAGScheduler: Broadcasting large task binary with size 1933.0 KiB\n",
      "25/05/06 09:57:36 WARN DAGScheduler: Broadcasting large task binary with size 1928.3 KiB\n",
      "25/05/06 09:57:40 WARN DAGScheduler: Broadcasting large task binary with size 1933.6 KiB\n",
      "25/05/06 09:57:43 WARN DAGScheduler: Broadcasting large task binary with size 1929.4 KiB\n",
      "25/05/06 09:57:46 WARN DAGScheduler: Broadcasting large task binary with size 1933.5 KiB\n",
      "25/05/06 09:57:50 WARN DAGScheduler: Broadcasting large task binary with size 1928.7 KiB\n",
      "25/05/06 09:57:53 WARN DAGScheduler: Broadcasting large task binary with size 1934.2 KiB\n",
      "25/05/06 09:57:57 WARN DAGScheduler: Broadcasting large task binary with size 1931.6 KiB\n",
      "25/05/06 09:57:59 WARN DAGScheduler: Broadcasting large task binary with size 1934.1 KiB\n",
      "25/05/06 09:58:02 WARN DAGScheduler: Broadcasting large task binary with size 1929.4 KiB\n",
      "25/05/06 09:58:06 WARN DAGScheduler: Broadcasting large task binary with size 1935.4 KiB\n",
      "25/05/06 09:58:09 WARN DAGScheduler: Broadcasting large task binary with size 1932.1 KiB\n",
      "25/05/06 09:58:13 WARN DAGScheduler: Broadcasting large task binary with size 1935.3 KiB\n",
      "25/05/06 09:58:16 WARN DAGScheduler: Broadcasting large task binary with size 1931.6 KiB\n",
      "25/05/06 09:58:19 WARN DAGScheduler: Broadcasting large task binary with size 1937.8 KiB\n",
      "25/05/06 09:58:22 WARN DAGScheduler: Broadcasting large task binary with size 1932.7 KiB\n",
      "25/05/06 09:58:26 WARN DAGScheduler: Broadcasting large task binary with size 1937.7 KiB\n",
      "25/05/06 09:58:29 WARN DAGScheduler: Broadcasting large task binary with size 1932.1 KiB\n",
      "25/05/06 09:58:34 WARN DAGScheduler: Broadcasting large task binary with size 1940.3 KiB\n",
      "25/05/06 09:58:37 WARN DAGScheduler: Broadcasting large task binary with size 1935.0 KiB\n",
      "25/05/06 09:58:41 WARN DAGScheduler: Broadcasting large task binary with size 1940.4 KiB\n",
      "25/05/06 09:58:44 WARN DAGScheduler: Broadcasting large task binary with size 1932.7 KiB\n",
      "25/05/06 09:58:48 WARN DAGScheduler: Broadcasting large task binary with size 1940.9 KiB\n",
      "25/05/06 09:58:52 WARN DAGScheduler: Broadcasting large task binary with size 1935.5 KiB\n",
      "25/05/06 09:58:56 WARN DAGScheduler: Broadcasting large task binary with size 1940.9 KiB\n",
      "25/05/06 09:58:58 WARN DAGScheduler: Broadcasting large task binary with size 1935.0 KiB\n",
      "25/05/06 09:59:01 WARN DAGScheduler: Broadcasting large task binary with size 1941.5 KiB\n",
      "25/05/06 09:59:04 WARN DAGScheduler: Broadcasting large task binary with size 1936.0 KiB\n",
      "25/05/06 09:59:07 WARN DAGScheduler: Broadcasting large task binary with size 1941.5 KiB\n",
      "25/05/06 09:59:11 WARN DAGScheduler: Broadcasting large task binary with size 1935.5 KiB\n",
      "25/05/06 09:59:15 WARN DAGScheduler: Broadcasting large task binary with size 1942.6 KiB\n",
      "25/05/06 09:59:18 WARN DAGScheduler: Broadcasting large task binary with size 1938.3 KiB\n",
      "25/05/06 09:59:20 WARN DAGScheduler: Broadcasting large task binary with size 1942.7 KiB\n",
      "25/05/06 09:59:24 WARN DAGScheduler: Broadcasting large task binary with size 1936.0 KiB\n",
      "25/05/06 09:59:27 WARN DAGScheduler: Broadcasting large task binary with size 1945.1 KiB\n",
      "25/05/06 09:59:31 WARN DAGScheduler: Broadcasting large task binary with size 1938.8 KiB\n",
      "25/05/06 09:59:34 WARN DAGScheduler: Broadcasting large task binary with size 1944.9 KiB\n",
      "25/05/06 09:59:37 WARN DAGScheduler: Broadcasting large task binary with size 1938.3 KiB\n",
      "25/05/06 09:59:41 WARN DAGScheduler: Broadcasting large task binary with size 1948.1 KiB\n",
      "25/05/06 09:59:44 WARN DAGScheduler: Broadcasting large task binary with size 1939.4 KiB\n",
      "25/05/06 09:59:48 WARN DAGScheduler: Broadcasting large task binary with size 1947.5 KiB\n",
      "25/05/06 09:59:52 WARN DAGScheduler: Broadcasting large task binary with size 1938.8 KiB\n",
      "25/05/06 09:59:56 WARN DAGScheduler: Broadcasting large task binary with size 1948.5 KiB\n",
      "25/05/06 09:59:59 WARN DAGScheduler: Broadcasting large task binary with size 1941.7 KiB\n",
      "25/05/06 10:00:03 WARN DAGScheduler: Broadcasting large task binary with size 1948.0 KiB\n",
      "25/05/06 10:00:06 WARN DAGScheduler: Broadcasting large task binary with size 1939.4 KiB\n",
      "25/05/06 10:00:09 WARN DAGScheduler: Broadcasting large task binary with size 1949.1 KiB\n",
      "25/05/06 10:00:12 WARN DAGScheduler: Broadcasting large task binary with size 1942.2 KiB\n",
      "25/05/06 10:00:16 WARN DAGScheduler: Broadcasting large task binary with size 1948.6 KiB\n",
      "25/05/06 10:00:19 WARN DAGScheduler: Broadcasting large task binary with size 1941.7 KiB\n",
      "25/05/06 10:00:22 WARN DAGScheduler: Broadcasting large task binary with size 1950.3 KiB\n",
      "25/05/06 10:00:25 WARN DAGScheduler: Broadcasting large task binary with size 1942.8 KiB\n",
      "25/05/06 10:00:28 WARN DAGScheduler: Broadcasting large task binary with size 1949.8 KiB\n",
      "25/05/06 10:00:32 WARN DAGScheduler: Broadcasting large task binary with size 1942.2 KiB\n",
      "25/05/06 10:00:35 WARN DAGScheduler: Broadcasting large task binary with size 1952.1 KiB\n",
      "25/05/06 10:00:38 WARN DAGScheduler: Broadcasting large task binary with size 1945.1 KiB\n",
      "25/05/06 10:00:41 WARN DAGScheduler: Broadcasting large task binary with size 1951.7 KiB\n",
      "25/05/06 10:00:44 WARN DAGScheduler: Broadcasting large task binary with size 1942.8 KiB\n",
      "25/05/06 10:00:48 WARN DAGScheduler: Broadcasting large task binary with size 1954.4 KiB\n",
      "25/05/06 10:00:52 WARN DAGScheduler: Broadcasting large task binary with size 1945.6 KiB\n",
      "25/05/06 10:00:56 WARN DAGScheduler: Broadcasting large task binary with size 1954.1 KiB\n",
      "25/05/06 10:00:59 WARN DAGScheduler: Broadcasting large task binary with size 1945.1 KiB\n",
      "25/05/06 10:01:03 WARN DAGScheduler: Broadcasting large task binary with size 1954.9 KiB\n",
      "25/05/06 10:01:06 WARN DAGScheduler: Broadcasting large task binary with size 1946.2 KiB\n",
      "25/05/06 10:01:10 WARN DAGScheduler: Broadcasting large task binary with size 1954.6 KiB\n",
      "25/05/06 10:01:13 WARN DAGScheduler: Broadcasting large task binary with size 1945.6 KiB\n",
      "25/05/06 10:01:16 WARN DAGScheduler: Broadcasting large task binary with size 1955.5 KiB\n",
      "25/05/06 10:01:19 WARN DAGScheduler: Broadcasting large task binary with size 1948.5 KiB\n",
      "25/05/06 10:01:22 WARN DAGScheduler: Broadcasting large task binary with size 1955.2 KiB\n",
      "25/05/06 10:01:25 WARN DAGScheduler: Broadcasting large task binary with size 1946.2 KiB\n",
      "25/05/06 10:01:28 WARN DAGScheduler: Broadcasting large task binary with size 1956.7 KiB\n",
      "25/05/06 10:01:32 WARN DAGScheduler: Broadcasting large task binary with size 1949.0 KiB\n",
      "25/05/06 10:01:35 WARN DAGScheduler: Broadcasting large task binary with size 1956.4 KiB\n",
      "25/05/06 10:01:38 WARN DAGScheduler: Broadcasting large task binary with size 1948.5 KiB\n",
      "25/05/06 10:01:42 WARN DAGScheduler: Broadcasting large task binary with size 1959.1 KiB\n",
      "25/05/06 10:01:45 WARN DAGScheduler: Broadcasting large task binary with size 1949.6 KiB\n",
      "25/05/06 10:01:48 WARN DAGScheduler: Broadcasting large task binary with size 1958.9 KiB\n",
      "25/05/06 10:01:52 WARN DAGScheduler: Broadcasting large task binary with size 1949.0 KiB\n",
      "25/05/06 10:01:56 WARN DAGScheduler: Broadcasting large task binary with size 1962.0 KiB\n",
      "25/05/06 10:01:59 WARN DAGScheduler: Broadcasting large task binary with size 1608.5 KiB\n",
      "25/05/06 10:02:03 WARN DAGScheduler: Broadcasting large task binary with size 1961.6 KiB\n",
      "25/05/06 10:02:06 WARN DAGScheduler: Broadcasting large task binary with size 1949.6 KiB\n",
      "25/05/06 10:02:10 WARN DAGScheduler: Broadcasting large task binary with size 1962.5 KiB\n",
      "25/05/06 10:02:10 WARN DAGScheduler: Broadcasting large task binary with size 1640.3 KiB\n",
      "25/05/06 10:02:14 WARN DAGScheduler: Broadcasting large task binary with size 1962.1 KiB\n",
      "25/05/06 10:02:18 WARN DAGScheduler: Broadcasting large task binary with size 1640.3 KiB\n",
      "25/05/06 10:02:20 WARN DAGScheduler: Broadcasting large task binary with size 1963.1 KiB\n",
      "25/05/06 10:02:20 WARN DAGScheduler: Broadcasting large task binary with size 1641.4 KiB\n",
      "25/05/06 10:02:23 WARN DAGScheduler: Broadcasting large task binary with size 1962.7 KiB\n",
      "25/05/06 10:02:23 WARN DAGScheduler: Broadcasting large task binary with size 1641.4 KiB\n",
      "25/05/06 10:02:27 WARN DAGScheduler: Broadcasting large task binary with size 1964.3 KiB\n",
      "25/05/06 10:02:30 WARN DAGScheduler: Broadcasting large task binary with size 1963.9 KiB\n",
      "25/05/06 10:02:33 WARN DAGScheduler: Broadcasting large task binary with size 1966.7 KiB\n",
      "25/05/06 10:02:37 WARN DAGScheduler: Broadcasting large task binary with size 1966.0 KiB\n",
      "25/05/06 10:02:41 WARN DAGScheduler: Broadcasting large task binary with size 1969.2 KiB\n",
      "25/05/06 10:02:45 WARN DAGScheduler: Broadcasting large task binary with size 1968.4 KiB\n",
      "25/05/06 10:02:48 WARN DAGScheduler: Broadcasting large task binary with size 1969.7 KiB\n",
      "25/05/06 10:02:52 WARN DAGScheduler: Broadcasting large task binary with size 1968.9 KiB\n",
      "25/05/06 10:02:55 WARN DAGScheduler: Broadcasting large task binary with size 1970.3 KiB\n",
      "25/05/06 10:02:58 WARN DAGScheduler: Broadcasting large task binary with size 1969.5 KiB\n",
      "25/05/06 10:03:01 WARN DAGScheduler: Broadcasting large task binary with size 1971.5 KiB\n",
      "25/05/06 10:03:04 WARN DAGScheduler: Broadcasting large task binary with size 1970.7 KiB\n",
      "25/05/06 10:03:08 WARN DAGScheduler: Broadcasting large task binary with size 1973.4 KiB\n",
      "25/05/06 10:03:11 WARN DAGScheduler: Broadcasting large task binary with size 1972.5 KiB\n",
      "25/05/06 10:03:15 WARN DAGScheduler: Broadcasting large task binary with size 1975.8 KiB\n",
      "25/05/06 10:03:19 WARN DAGScheduler: Broadcasting large task binary with size 1974.8 KiB\n",
      "25/05/06 10:03:22 WARN DAGScheduler: Broadcasting large task binary with size 1976.3 KiB\n",
      "25/05/06 10:03:26 WARN DAGScheduler: Broadcasting large task binary with size 1975.3 KiB\n",
      "25/05/06 10:03:29 WARN DAGScheduler: Broadcasting large task binary with size 1976.9 KiB\n",
      "25/05/06 10:03:32 WARN DAGScheduler: Broadcasting large task binary with size 1975.9 KiB\n",
      "25/05/06 10:03:35 WARN DAGScheduler: Broadcasting large task binary with size 1978.1 KiB\n",
      "25/05/06 10:03:38 WARN DAGScheduler: Broadcasting large task binary with size 1977.1 KiB\n",
      "25/05/06 10:03:41 WARN DAGScheduler: Broadcasting large task binary with size 1980.5 KiB\n",
      "25/05/06 10:03:45 WARN DAGScheduler: Broadcasting large task binary with size 1979.3 KiB\n",
      "25/05/06 10:03:49 WARN DAGScheduler: Broadcasting large task binary with size 1983.4 KiB\n",
      "25/05/06 10:03:53 WARN DAGScheduler: Broadcasting large task binary with size 1981.9 KiB\n",
      "25/05/06 10:03:57 WARN DAGScheduler: Broadcasting large task binary with size 1983.8 KiB\n",
      "25/05/06 10:04:01 WARN DAGScheduler: Broadcasting large task binary with size 1982.3 KiB\n",
      "25/05/06 10:04:04 WARN DAGScheduler: Broadcasting large task binary with size 1984.5 KiB\n",
      "25/05/06 10:04:07 WARN DAGScheduler: Broadcasting large task binary with size 1982.9 KiB\n",
      "25/05/06 10:04:10 WARN DAGScheduler: Broadcasting large task binary with size 1985.6 KiB\n",
      "25/05/06 10:04:13 WARN DAGScheduler: Broadcasting large task binary with size 1984.1 KiB\n",
      "25/05/06 10:04:17 WARN DAGScheduler: Broadcasting large task binary with size 1988.0 KiB\n",
      "25/05/06 10:04:20 WARN DAGScheduler: Broadcasting large task binary with size 1986.2 KiB\n",
      "25/05/06 10:04:24 WARN DAGScheduler: Broadcasting large task binary with size 1640.3 KiB\n",
      "25/05/06 10:04:28 WARN DAGScheduler: Broadcasting large task binary with size 1641.4 KiB\n",
      "25/05/06 10:04:28 WARN DAGScheduler: Broadcasting large task binary with size 1640.3 KiB\n",
      "25/05/06 10:04:28 WARN DAGScheduler: Broadcasting large task binary with size 1641.4 KiB\n",
      "25/05/06 10:04:29 WARN DAGScheduler: Broadcasting large task binary with size 1608.6 KiB\n",
      "25/05/06 10:04:29 WARN DAGScheduler: Broadcasting large task binary with size 1608.6 KiB\n",
      "25/05/06 10:04:29 WARN DAGScheduler: Broadcasting large task binary with size 1608.6 KiB\n",
      "25/05/06 10:04:29 WARN DAGScheduler: Broadcasting large task binary with size 1608.6 KiB\n",
      "25/05/06 10:04:29 WARN DAGScheduler: Broadcasting large task binary with size 1648.4 KiB\n",
      "25/05/06 10:04:29 WARN DAGScheduler: Broadcasting large task binary with size 1648.4 KiB\n",
      "25/05/06 10:04:29 WARN DAGScheduler: Broadcasting large task binary with size 1648.4 KiB\n",
      "25/05/06 10:04:29 WARN DAGScheduler: Broadcasting large task binary with size 1648.5 KiB\n",
      "25/05/06 10:04:30 WARN DAGScheduler: Broadcasting large task binary with size 1648.5 KiB\n",
      "25/05/06 10:04:30 WARN DAGScheduler: Broadcasting large task binary with size 1648.5 KiB\n",
      "25/05/06 10:04:30 WARN DAGScheduler: Broadcasting large task binary with size 1648.4 KiB\n",
      "25/05/06 10:04:30 WARN DAGScheduler: Broadcasting large task binary with size 1652.6 KiB\n",
      "25/05/06 10:04:30 WARN DAGScheduler: Broadcasting large task binary with size 1652.6 KiB\n",
      "25/05/06 10:04:30 WARN DAGScheduler: Broadcasting large task binary with size 1648.5 KiB\n",
      "25/05/06 10:04:30 WARN DAGScheduler: Broadcasting large task binary with size 1652.6 KiB\n",
      "25/05/06 10:04:30 WARN DAGScheduler: Broadcasting large task binary with size 1912.9 KiB\n",
      "25/05/06 10:04:31 WARN DAGScheduler: Broadcasting large task binary with size 1912.9 KiB\n",
      "25/05/06 10:04:31 WARN DAGScheduler: Broadcasting large task binary with size 1652.6 KiB\n",
      "25/05/06 10:04:32 WARN DAGScheduler: Broadcasting large task binary with size 1912.9 KiB\n",
      "25/05/06 10:04:40 WARN DAGScheduler: Broadcasting large task binary with size 1913.8 KiB\n",
      "25/05/06 10:04:48 WARN DAGScheduler: Broadcasting large task binary with size 1913.8 KiB\n",
      "25/05/06 10:04:50 WARN DAGScheduler: Broadcasting large task binary with size 1912.9 KiB\n",
      "25/05/06 10:04:58 WARN DAGScheduler: Broadcasting large task binary with size 1913.8 KiB\n",
      "25/05/06 10:05:00 WARN DAGScheduler: Broadcasting large task binary with size 1914.5 KiB\n",
      "25/05/06 10:05:04 WARN DAGScheduler: Broadcasting large task binary with size 1914.5 KiB\n",
      "25/05/06 10:05:12 WARN DAGScheduler: Broadcasting large task binary with size 1913.8 KiB\n",
      "25/05/06 10:05:14 WARN DAGScheduler: Broadcasting large task binary with size 1914.5 KiB\n",
      "25/05/06 10:05:17 WARN DAGScheduler: Broadcasting large task binary with size 1921.3 KiB\n",
      "25/05/06 10:05:20 WARN DAGScheduler: Broadcasting large task binary with size 1921.3 KiB\n",
      "25/05/06 10:05:23 WARN DAGScheduler: Broadcasting large task binary with size 1914.5 KiB\n",
      "25/05/06 10:05:26 WARN DAGScheduler: Broadcasting large task binary with size 1915.7 KiB\n",
      "25/05/06 10:05:29 WARN DAGScheduler: Broadcasting large task binary with size 1921.8 KiB\n",
      "25/05/06 10:05:33 WARN DAGScheduler: Broadcasting large task binary with size 1921.8 KiB\n",
      "25/05/06 10:05:36 WARN DAGScheduler: Broadcasting large task binary with size 1915.7 KiB\n",
      "25/05/06 10:05:40 WARN DAGScheduler: Broadcasting large task binary with size 1918.1 KiB\n",
      "25/05/06 10:05:42 WARN DAGScheduler: Broadcasting large task binary with size 1922.4 KiB\n",
      "25/05/06 10:05:45 WARN DAGScheduler: Broadcasting large task binary with size 1922.4 KiB\n",
      "25/05/06 10:05:49 WARN DAGScheduler: Broadcasting large task binary with size 1918.1 KiB\n",
      "25/05/06 10:05:53 WARN DAGScheduler: Broadcasting large task binary with size 1925.6 KiB\n",
      "25/05/06 10:05:56 WARN DAGScheduler: Broadcasting large task binary with size 1924.9 KiB\n",
      "25/05/06 10:05:59 WARN DAGScheduler: Broadcasting large task binary with size 1924.9 KiB\n",
      "25/05/06 10:06:03 WARN DAGScheduler: Broadcasting large task binary with size 1925.5 KiB\n",
      "25/05/06 10:06:07 WARN DAGScheduler: Broadcasting large task binary with size 1926.1 KiB\n",
      "25/05/06 10:06:11 WARN DAGScheduler: Broadcasting large task binary with size 1925.4 KiB\n",
      "25/05/06 10:06:14 WARN DAGScheduler: Broadcasting large task binary with size 1925.4 KiB\n",
      "25/05/06 10:06:19 WARN DAGScheduler: Broadcasting large task binary with size 1926.0 KiB\n",
      "25/05/06 10:06:21 WARN DAGScheduler: Broadcasting large task binary with size 1926.7 KiB\n",
      "25/05/06 10:06:24 WARN DAGScheduler: Broadcasting large task binary with size 1926.0 KiB\n",
      "25/05/06 10:06:27 WARN DAGScheduler: Broadcasting large task binary with size 1926.0 KiB\n",
      "25/05/06 10:06:30 WARN DAGScheduler: Broadcasting large task binary with size 1926.6 KiB\n",
      "25/05/06 10:06:33 WARN DAGScheduler: Broadcasting large task binary with size 1927.9 KiB\n",
      "25/05/06 10:06:36 WARN DAGScheduler: Broadcasting large task binary with size 1928.3 KiB\n",
      "25/05/06 10:06:39 WARN DAGScheduler: Broadcasting large task binary with size 1928.3 KiB\n",
      "25/05/06 10:06:42 WARN DAGScheduler: Broadcasting large task binary with size 1927.8 KiB\n",
      "25/05/06 10:06:46 WARN DAGScheduler: Broadcasting large task binary with size 1930.3 KiB\n",
      "25/05/06 10:06:50 WARN DAGScheduler: Broadcasting large task binary with size 1928.8 KiB\n",
      "25/05/06 10:06:53 WARN DAGScheduler: Broadcasting large task binary with size 1928.8 KiB\n",
      "25/05/06 10:06:57 WARN DAGScheduler: Broadcasting large task binary with size 1930.2 KiB\n",
      "25/05/06 10:07:01 WARN DAGScheduler: Broadcasting large task binary with size 1933.0 KiB\n",
      "25/05/06 10:07:04 WARN DAGScheduler: Broadcasting large task binary with size 1929.4 KiB\n",
      "25/05/06 10:07:07 WARN DAGScheduler: Broadcasting large task binary with size 1929.4 KiB\n",
      "25/05/06 10:07:11 WARN DAGScheduler: Broadcasting large task binary with size 1932.8 KiB\n",
      "25/05/06 10:07:15 WARN DAGScheduler: Broadcasting large task binary with size 1933.5 KiB\n",
      "25/05/06 10:07:18 WARN DAGScheduler: Broadcasting large task binary with size 1931.7 KiB\n",
      "25/05/06 10:07:21 WARN DAGScheduler: Broadcasting large task binary with size 1931.7 KiB\n",
      "25/05/06 10:07:24 WARN DAGScheduler: Broadcasting large task binary with size 1933.4 KiB\n",
      "25/05/06 10:07:28 WARN DAGScheduler: Broadcasting large task binary with size 1934.1 KiB\n",
      "25/05/06 10:07:31 WARN DAGScheduler: Broadcasting large task binary with size 1932.2 KiB\n",
      "25/05/06 10:07:35 WARN DAGScheduler: Broadcasting large task binary with size 1932.2 KiB\n",
      "25/05/06 10:07:38 WARN DAGScheduler: Broadcasting large task binary with size 1933.9 KiB\n",
      "25/05/06 10:07:41 WARN DAGScheduler: Broadcasting large task binary with size 1935.3 KiB\n",
      "25/05/06 10:07:44 WARN DAGScheduler: Broadcasting large task binary with size 1932.8 KiB\n",
      "25/05/06 10:07:47 WARN DAGScheduler: Broadcasting large task binary with size 1932.8 KiB\n",
      "25/05/06 10:07:50 WARN DAGScheduler: Broadcasting large task binary with size 1935.1 KiB\n",
      "25/05/06 10:07:53 WARN DAGScheduler: Broadcasting large task binary with size 1937.6 KiB\n",
      "25/05/06 10:07:56 WARN DAGScheduler: Broadcasting large task binary with size 1935.1 KiB\n",
      "25/05/06 10:07:59 WARN DAGScheduler: Broadcasting large task binary with size 1935.1 KiB\n",
      "25/05/06 10:08:03 WARN DAGScheduler: Broadcasting large task binary with size 1937.5 KiB\n",
      "25/05/06 10:08:07 WARN DAGScheduler: Broadcasting large task binary with size 1940.5 KiB\n",
      "25/05/06 10:08:11 WARN DAGScheduler: Broadcasting large task binary with size 1935.6 KiB\n",
      "25/05/06 10:08:14 WARN DAGScheduler: Broadcasting large task binary with size 1935.6 KiB\n",
      "25/05/06 10:08:19 WARN DAGScheduler: Broadcasting large task binary with size 1940.1 KiB\n",
      "25/05/06 10:08:22 WARN DAGScheduler: Broadcasting large task binary with size 1941.0 KiB\n",
      "25/05/06 10:08:25 WARN DAGScheduler: Broadcasting large task binary with size 1936.2 KiB\n",
      "25/05/06 10:08:28 WARN DAGScheduler: Broadcasting large task binary with size 1936.2 KiB\n",
      "25/05/06 10:08:32 WARN DAGScheduler: Broadcasting large task binary with size 1940.6 KiB\n",
      "25/05/06 10:08:35 WARN DAGScheduler: Broadcasting large task binary with size 1941.6 KiB\n",
      "25/05/06 10:08:38 WARN DAGScheduler: Broadcasting large task binary with size 1938.4 KiB\n",
      "25/05/06 10:08:41 WARN DAGScheduler: Broadcasting large task binary with size 1938.4 KiB\n",
      "25/05/06 10:08:44 WARN DAGScheduler: Broadcasting large task binary with size 1941.2 KiB\n",
      "25/05/06 10:08:47 WARN DAGScheduler: Broadcasting large task binary with size 1942.8 KiB\n",
      "25/05/06 10:08:50 WARN DAGScheduler: Broadcasting large task binary with size 1938.9 KiB\n",
      "25/05/06 10:08:54 WARN DAGScheduler: Broadcasting large task binary with size 1938.9 KiB\n",
      "25/05/06 10:08:58 WARN DAGScheduler: Broadcasting large task binary with size 1942.4 KiB\n",
      "25/05/06 10:09:01 WARN DAGScheduler: Broadcasting large task binary with size 1945.2 KiB\n",
      "25/05/06 10:09:04 WARN DAGScheduler: Broadcasting large task binary with size 1939.5 KiB\n",
      "25/05/06 10:09:07 WARN DAGScheduler: Broadcasting large task binary with size 1939.5 KiB\n",
      "25/05/06 10:09:11 WARN DAGScheduler: Broadcasting large task binary with size 1944.8 KiB\n",
      "25/05/06 10:09:15 WARN DAGScheduler: Broadcasting large task binary with size 1947.8 KiB\n",
      "25/05/06 10:09:18 WARN DAGScheduler: Broadcasting large task binary with size 1941.7 KiB\n",
      "25/05/06 10:09:21 WARN DAGScheduler: Broadcasting large task binary with size 1941.7 KiB\n",
      "25/05/06 10:09:26 WARN DAGScheduler: Broadcasting large task binary with size 1947.7 KiB\n",
      "25/05/06 10:09:29 WARN DAGScheduler: Broadcasting large task binary with size 1948.3 KiB\n",
      "25/05/06 10:09:33 WARN DAGScheduler: Broadcasting large task binary with size 1942.2 KiB\n",
      "25/05/06 10:09:37 WARN DAGScheduler: Broadcasting large task binary with size 1942.2 KiB\n",
      "25/05/06 10:09:41 WARN DAGScheduler: Broadcasting large task binary with size 1948.1 KiB\n",
      "25/05/06 10:09:44 WARN DAGScheduler: Broadcasting large task binary with size 1948.9 KiB\n",
      "25/05/06 10:09:46 WARN DAGScheduler: Broadcasting large task binary with size 1942.8 KiB\n",
      "25/05/06 10:09:49 WARN DAGScheduler: Broadcasting large task binary with size 1942.8 KiB\n",
      "25/05/06 10:09:52 WARN DAGScheduler: Broadcasting large task binary with size 1948.8 KiB\n",
      "25/05/06 10:09:55 WARN DAGScheduler: Broadcasting large task binary with size 1950.1 KiB\n",
      "25/05/06 10:09:58 WARN DAGScheduler: Broadcasting large task binary with size 1945.1 KiB\n",
      "25/05/06 10:10:02 WARN DAGScheduler: Broadcasting large task binary with size 1945.1 KiB\n",
      "25/05/06 10:10:05 WARN DAGScheduler: Broadcasting large task binary with size 1949.9 KiB\n",
      "25/05/06 10:10:08 WARN DAGScheduler: Broadcasting large task binary with size 1952.4 KiB\n",
      "25/05/06 10:10:12 WARN DAGScheduler: Broadcasting large task binary with size 1945.6 KiB\n",
      "25/05/06 10:10:16 WARN DAGScheduler: Broadcasting large task binary with size 1945.6 KiB\n",
      "25/05/06 10:10:20 WARN DAGScheduler: Broadcasting large task binary with size 1952.3 KiB\n",
      "25/05/06 10:10:24 WARN DAGScheduler: Broadcasting large task binary with size 1955.3 KiB\n",
      "25/05/06 10:10:26 WARN DAGScheduler: Broadcasting large task binary with size 1946.2 KiB\n",
      "25/05/06 10:10:29 WARN DAGScheduler: Broadcasting large task binary with size 1946.2 KiB\n",
      "25/05/06 10:10:33 WARN DAGScheduler: Broadcasting large task binary with size 1955.0 KiB\n",
      "25/05/06 10:10:36 WARN DAGScheduler: Broadcasting large task binary with size 1955.8 KiB\n",
      "25/05/06 10:10:40 WARN DAGScheduler: Broadcasting large task binary with size 1948.5 KiB\n",
      "25/05/06 10:10:43 WARN DAGScheduler: Broadcasting large task binary with size 1948.5 KiB\n",
      "25/05/06 10:10:47 WARN DAGScheduler: Broadcasting large task binary with size 1955.5 KiB\n",
      "25/05/06 10:10:49 WARN DAGScheduler: Broadcasting large task binary with size 1956.4 KiB\n",
      "25/05/06 10:10:53 WARN DAGScheduler: Broadcasting large task binary with size 1949.0 KiB\n",
      "25/05/06 10:10:57 WARN DAGScheduler: Broadcasting large task binary with size 1949.0 KiB\n",
      "25/05/06 10:11:00 WARN DAGScheduler: Broadcasting large task binary with size 1956.1 KiB\n",
      "25/05/06 10:11:03 WARN DAGScheduler: Broadcasting large task binary with size 1957.6 KiB\n",
      "25/05/06 10:11:06 WARN DAGScheduler: Broadcasting large task binary with size 1949.6 KiB\n",
      "25/05/06 10:11:09 WARN DAGScheduler: Broadcasting large task binary with size 1949.6 KiB\n",
      "25/05/06 10:11:12 WARN DAGScheduler: Broadcasting large task binary with size 1957.2 KiB\n",
      "25/05/06 10:11:16 WARN DAGScheduler: Broadcasting large task binary with size 1959.7 KiB\n",
      "25/05/06 10:11:19 WARN DAGScheduler: Broadcasting large task binary with size 1608.5 KiB\n",
      "25/05/06 10:11:22 WARN DAGScheduler: Broadcasting large task binary with size 1608.5 KiB\n",
      "25/05/06 10:11:26 WARN DAGScheduler: Broadcasting large task binary with size 1959.6 KiB\n",
      "25/05/06 10:11:29 WARN DAGScheduler: Broadcasting large task binary with size 1962.3 KiB\n",
      "25/05/06 10:11:30 WARN DAGScheduler: Broadcasting large task binary with size 1640.3 KiB\n",
      "25/05/06 10:11:30 WARN DAGScheduler: Broadcasting large task binary with size 1640.3 KiB\n",
      "25/05/06 10:11:35 WARN DAGScheduler: Broadcasting large task binary with size 1962.6 KiB\n",
      "25/05/06 10:11:38 WARN DAGScheduler: Broadcasting large task binary with size 1962.9 KiB\n",
      "25/05/06 10:11:38 WARN DAGScheduler: Broadcasting large task binary with size 1641.4 KiB\n",
      "25/05/06 10:11:38 WARN DAGScheduler: Broadcasting large task binary with size 1641.4 KiB\n",
      "25/05/06 10:11:42 WARN DAGScheduler: Broadcasting large task binary with size 1963.1 KiB\n",
      "25/05/06 10:11:45 WARN DAGScheduler: Broadcasting large task binary with size 1963.5 KiB\n",
      "25/05/06 10:11:48 WARN DAGScheduler: Broadcasting large task binary with size 1963.6 KiB\n",
      "25/05/06 10:11:52 WARN DAGScheduler: Broadcasting large task binary with size 1964.7 KiB\n",
      "25/05/06 10:11:55 WARN DAGScheduler: Broadcasting large task binary with size 1964.8 KiB\n",
      "25/05/06 10:11:58 WARN DAGScheduler: Broadcasting large task binary with size 1967.0 KiB\n",
      "25/05/06 10:12:02 WARN DAGScheduler: Broadcasting large task binary with size 1967.2 KiB\n",
      "25/05/06 10:12:06 WARN DAGScheduler: Broadcasting large task binary with size 1969.9 KiB\n",
      "25/05/06 10:12:10 WARN DAGScheduler: Broadcasting large task binary with size 1970.2 KiB\n",
      "25/05/06 10:12:14 WARN DAGScheduler: Broadcasting large task binary with size 1970.4 KiB\n",
      "25/05/06 10:12:18 WARN DAGScheduler: Broadcasting large task binary with size 1970.7 KiB\n",
      "25/05/06 10:12:21 WARN DAGScheduler: Broadcasting large task binary with size 1971.0 KiB\n",
      "25/05/06 10:12:23 WARN DAGScheduler: Broadcasting large task binary with size 1971.3 KiB\n",
      "25/05/06 10:12:26 WARN DAGScheduler: Broadcasting large task binary with size 1972.2 KiB\n",
      "25/05/06 10:12:29 WARN DAGScheduler: Broadcasting large task binary with size 1972.5 KiB\n",
      "25/05/06 10:12:33 WARN DAGScheduler: Broadcasting large task binary with size 1974.6 KiB\n",
      "25/05/06 10:12:36 WARN DAGScheduler: Broadcasting large task binary with size 1974.9 KiB\n",
      "25/05/06 10:12:40 WARN DAGScheduler: Broadcasting large task binary with size 1977.5 KiB\n",
      "25/05/06 10:12:44 WARN DAGScheduler: Broadcasting large task binary with size 1977.6 KiB\n",
      "25/05/06 10:12:48 WARN DAGScheduler: Broadcasting large task binary with size 1978.0 KiB\n",
      "25/05/06 10:12:52 WARN DAGScheduler: Broadcasting large task binary with size 1978.0 KiB\n",
      "25/05/06 10:12:55 WARN DAGScheduler: Broadcasting large task binary with size 1978.5 KiB\n",
      "25/05/06 10:12:58 WARN DAGScheduler: Broadcasting large task binary with size 1978.6 KiB\n",
      "25/05/06 10:13:00 WARN DAGScheduler: Broadcasting large task binary with size 1979.7 KiB\n",
      "25/05/06 10:13:04 WARN DAGScheduler: Broadcasting large task binary with size 1979.8 KiB\n",
      "25/05/06 10:13:07 WARN DAGScheduler: Broadcasting large task binary with size 1982.0 KiB\n",
      "25/05/06 10:13:11 WARN DAGScheduler: Broadcasting large task binary with size 1982.3 KiB\n",
      "25/05/06 10:13:15 WARN DAGScheduler: Broadcasting large task binary with size 1984.3 KiB\n",
      "25/05/06 10:13:19 WARN DAGScheduler: Broadcasting large task binary with size 1985.1 KiB\n",
      "25/05/06 10:13:23 WARN DAGScheduler: Broadcasting large task binary with size 1984.8 KiB\n",
      "25/05/06 10:13:27 WARN DAGScheduler: Broadcasting large task binary with size 1985.5 KiB\n",
      "25/05/06 10:13:29 WARN DAGScheduler: Broadcasting large task binary with size 1985.4 KiB\n",
      "25/05/06 10:13:32 WARN DAGScheduler: Broadcasting large task binary with size 1986.1 KiB\n",
      "25/05/06 10:13:35 WARN DAGScheduler: Broadcasting large task binary with size 1986.6 KiB\n",
      "25/05/06 10:13:38 WARN DAGScheduler: Broadcasting large task binary with size 1987.3 KiB\n",
      "25/05/06 10:13:42 WARN DAGScheduler: Broadcasting large task binary with size 1989.1 KiB\n",
      "25/05/06 10:13:46 WARN DAGScheduler: Broadcasting large task binary with size 1989.7 KiB\n",
      "25/05/06 10:13:50 WARN DAGScheduler: Broadcasting large task binary with size 1640.3 KiB\n",
      "25/05/06 10:13:54 WARN DAGScheduler: Broadcasting large task binary with size 1640.3 KiB\n",
      "25/05/06 10:13:54 WARN DAGScheduler: Broadcasting large task binary with size 1641.4 KiB\n",
      "25/05/06 10:13:54 WARN DAGScheduler: Broadcasting large task binary with size 1641.4 KiB\n",
      "25/05/06 10:13:54 WARN DAGScheduler: Broadcasting large task binary with size 1608.5 KiB\n",
      "25/05/06 10:13:54 WARN DAGScheduler: Broadcasting large task binary with size 1608.5 KiB\n",
      "25/05/06 10:13:54 WARN DAGScheduler: Broadcasting large task binary with size 1608.5 KiB\n",
      "25/05/06 10:13:54 WARN DAGScheduler: Broadcasting large task binary with size 1608.5 KiB\n",
      "25/05/06 10:13:55 WARN DAGScheduler: Broadcasting large task binary with size 1648.4 KiB\n",
      "25/05/06 10:13:55 WARN DAGScheduler: Broadcasting large task binary with size 1648.4 KiB\n",
      "25/05/06 10:13:55 WARN DAGScheduler: Broadcasting large task binary with size 1648.4 KiB\n",
      "25/05/06 10:13:55 WARN DAGScheduler: Broadcasting large task binary with size 1648.4 KiB\n",
      "25/05/06 10:13:55 WARN DAGScheduler: Broadcasting large task binary with size 1648.4 KiB\n",
      "25/05/06 10:13:55 WARN DAGScheduler: Broadcasting large task binary with size 1648.4 KiB\n",
      "25/05/06 10:13:55 WARN DAGScheduler: Broadcasting large task binary with size 1648.4 KiB\n",
      "25/05/06 10:13:55 WARN DAGScheduler: Broadcasting large task binary with size 1652.6 KiB\n",
      "25/05/06 10:13:56 WARN DAGScheduler: Broadcasting large task binary with size 1652.6 KiB\n",
      "25/05/06 10:13:56 WARN DAGScheduler: Broadcasting large task binary with size 1648.4 KiB\n",
      "25/05/06 10:13:56 WARN DAGScheduler: Broadcasting large task binary with size 1652.6 KiB\n",
      "25/05/06 10:13:56 WARN DAGScheduler: Broadcasting large task binary with size 1912.9 KiB\n",
      "25/05/06 10:13:56 WARN DAGScheduler: Broadcasting large task binary with size 1912.9 KiB\n",
      "25/05/06 10:13:57 WARN DAGScheduler: Broadcasting large task binary with size 1652.6 KiB\n",
      "25/05/06 10:13:58 WARN DAGScheduler: Broadcasting large task binary with size 1912.9 KiB\n",
      "25/05/06 10:14:06 WARN DAGScheduler: Broadcasting large task binary with size 1913.8 KiB\n",
      "25/05/06 10:14:14 WARN DAGScheduler: Broadcasting large task binary with size 1913.8 KiB\n",
      "25/05/06 10:14:15 WARN DAGScheduler: Broadcasting large task binary with size 1912.9 KiB\n",
      "25/05/06 10:14:23 WARN DAGScheduler: Broadcasting large task binary with size 1913.8 KiB\n",
      "25/05/06 10:14:26 WARN DAGScheduler: Broadcasting large task binary with size 1914.5 KiB\n",
      "25/05/06 10:14:29 WARN DAGScheduler: Broadcasting large task binary with size 1914.5 KiB\n",
      "25/05/06 10:14:37 WARN DAGScheduler: Broadcasting large task binary with size 1913.8 KiB\n",
      "25/05/06 10:14:39 WARN DAGScheduler: Broadcasting large task binary with size 1914.5 KiB\n",
      "25/05/06 10:14:42 WARN DAGScheduler: Broadcasting large task binary with size 1915.7 KiB\n",
      "25/05/06 10:14:45 WARN DAGScheduler: Broadcasting large task binary with size 1921.3 KiB\n",
      "25/05/06 10:14:48 WARN DAGScheduler: Broadcasting large task binary with size 1914.5 KiB\n",
      "25/05/06 10:14:51 WARN DAGScheduler: Broadcasting large task binary with size 1921.3 KiB\n",
      "25/05/06 10:14:55 WARN DAGScheduler: Broadcasting large task binary with size 1918.1 KiB\n",
      "25/05/06 10:14:59 WARN DAGScheduler: Broadcasting large task binary with size 1921.8 KiB\n",
      "25/05/06 10:15:02 WARN DAGScheduler: Broadcasting large task binary with size 1915.7 KiB\n",
      "25/05/06 10:15:05 WARN DAGScheduler: Broadcasting large task binary with size 1921.8 KiB\n",
      "25/05/06 10:15:10 WARN DAGScheduler: Broadcasting large task binary with size 1925.5 KiB\n",
      "25/05/06 10:15:12 WARN DAGScheduler: Broadcasting large task binary with size 1922.4 KiB\n",
      "25/05/06 10:15:16 WARN DAGScheduler: Broadcasting large task binary with size 1918.1 KiB\n",
      "25/05/06 10:15:19 WARN DAGScheduler: Broadcasting large task binary with size 1922.4 KiB\n",
      "25/05/06 10:15:22 WARN DAGScheduler: Broadcasting large task binary with size 1926.0 KiB\n",
      "25/05/06 10:15:26 WARN DAGScheduler: Broadcasting large task binary with size 1924.8 KiB\n",
      "25/05/06 10:15:30 WARN DAGScheduler: Broadcasting large task binary with size 1925.5 KiB\n",
      "25/05/06 10:15:33 WARN DAGScheduler: Broadcasting large task binary with size 1924.8 KiB\n",
      "25/05/06 10:15:36 WARN DAGScheduler: Broadcasting large task binary with size 1926.6 KiB\n",
      "25/05/06 10:15:39 WARN DAGScheduler: Broadcasting large task binary with size 1925.3 KiB\n",
      "25/05/06 10:15:43 WARN DAGScheduler: Broadcasting large task binary with size 1926.0 KiB\n",
      "25/05/06 10:15:47 WARN DAGScheduler: Broadcasting large task binary with size 1925.3 KiB\n",
      "25/05/06 10:15:51 WARN DAGScheduler: Broadcasting large task binary with size 1927.8 KiB\n",
      "25/05/06 10:15:54 WARN DAGScheduler: Broadcasting large task binary with size 1926.0 KiB\n",
      "25/05/06 10:15:56 WARN DAGScheduler: Broadcasting large task binary with size 1926.6 KiB\n",
      "25/05/06 10:15:59 WARN DAGScheduler: Broadcasting large task binary with size 1926.0 KiB\n",
      "25/05/06 10:16:03 WARN DAGScheduler: Broadcasting large task binary with size 1930.1 KiB\n",
      "25/05/06 10:16:06 WARN DAGScheduler: Broadcasting large task binary with size 1928.2 KiB\n",
      "25/05/06 10:16:09 WARN DAGScheduler: Broadcasting large task binary with size 1927.8 KiB\n",
      "25/05/06 10:16:12 WARN DAGScheduler: Broadcasting large task binary with size 1928.2 KiB\n",
      "25/05/06 10:16:16 WARN DAGScheduler: Broadcasting large task binary with size 1933.2 KiB\n",
      "25/05/06 10:16:20 WARN DAGScheduler: Broadcasting large task binary with size 1928.7 KiB\n",
      "25/05/06 10:16:24 WARN DAGScheduler: Broadcasting large task binary with size 1929.8 KiB\n",
      "25/05/06 10:16:27 WARN DAGScheduler: Broadcasting large task binary with size 1928.7 KiB\n",
      "25/05/06 10:16:31 WARN DAGScheduler: Broadcasting large task binary with size 1933.7 KiB\n",
      "25/05/06 10:16:34 WARN DAGScheduler: Broadcasting large task binary with size 1929.3 KiB\n",
      "25/05/06 10:16:38 WARN DAGScheduler: Broadcasting large task binary with size 1932.7 KiB\n",
      "25/05/06 10:16:41 WARN DAGScheduler: Broadcasting large task binary with size 1929.3 KiB\n",
      "25/05/06 10:16:44 WARN DAGScheduler: Broadcasting large task binary with size 1934.3 KiB\n",
      "25/05/06 10:16:47 WARN DAGScheduler: Broadcasting large task binary with size 1931.6 KiB\n",
      "25/05/06 10:16:51 WARN DAGScheduler: Broadcasting large task binary with size 1933.2 KiB\n",
      "25/05/06 10:16:54 WARN DAGScheduler: Broadcasting large task binary with size 1931.6 KiB\n",
      "25/05/06 10:16:57 WARN DAGScheduler: Broadcasting large task binary with size 1935.5 KiB\n",
      "25/05/06 10:17:00 WARN DAGScheduler: Broadcasting large task binary with size 1932.1 KiB\n",
      "25/05/06 10:17:03 WARN DAGScheduler: Broadcasting large task binary with size 1933.8 KiB\n",
      "25/05/06 10:17:07 WARN DAGScheduler: Broadcasting large task binary with size 1932.1 KiB\n",
      "25/05/06 10:17:11 WARN DAGScheduler: Broadcasting large task binary with size 1937.8 KiB\n",
      "25/05/06 10:17:14 WARN DAGScheduler: Broadcasting large task binary with size 1932.7 KiB\n",
      "25/05/06 10:17:17 WARN DAGScheduler: Broadcasting large task binary with size 1935.0 KiB\n",
      "25/05/06 10:17:19 WARN DAGScheduler: Broadcasting large task binary with size 1932.7 KiB\n",
      "25/05/06 10:17:24 WARN DAGScheduler: Broadcasting large task binary with size 1940.5 KiB\n",
      "25/05/06 10:17:27 WARN DAGScheduler: Broadcasting large task binary with size 1935.0 KiB\n",
      "25/05/06 10:17:31 WARN DAGScheduler: Broadcasting large task binary with size 1937.1 KiB\n",
      "25/05/06 10:17:34 WARN DAGScheduler: Broadcasting large task binary with size 1935.0 KiB\n",
      "25/05/06 10:17:38 WARN DAGScheduler: Broadcasting large task binary with size 1941.0 KiB\n",
      "25/05/06 10:17:42 WARN DAGScheduler: Broadcasting large task binary with size 1935.5 KiB\n",
      "25/05/06 10:17:46 WARN DAGScheduler: Broadcasting large task binary with size 1939.7 KiB\n",
      "25/05/06 10:17:50 WARN DAGScheduler: Broadcasting large task binary with size 1935.5 KiB\n",
      "25/05/06 10:17:53 WARN DAGScheduler: Broadcasting large task binary with size 1941.6 KiB\n",
      "25/05/06 10:17:56 WARN DAGScheduler: Broadcasting large task binary with size 1936.1 KiB\n",
      "25/05/06 10:17:59 WARN DAGScheduler: Broadcasting large task binary with size 1940.1 KiB\n",
      "25/05/06 10:18:02 WARN DAGScheduler: Broadcasting large task binary with size 1936.1 KiB\n",
      "25/05/06 10:18:06 WARN DAGScheduler: Broadcasting large task binary with size 1942.8 KiB\n",
      "25/05/06 10:18:09 WARN DAGScheduler: Broadcasting large task binary with size 1938.4 KiB\n",
      "25/05/06 10:18:12 WARN DAGScheduler: Broadcasting large task binary with size 1940.7 KiB\n",
      "25/05/06 10:18:15 WARN DAGScheduler: Broadcasting large task binary with size 1938.4 KiB\n",
      "25/05/06 10:18:18 WARN DAGScheduler: Broadcasting large task binary with size 1945.0 KiB\n",
      "25/05/06 10:18:22 WARN DAGScheduler: Broadcasting large task binary with size 1938.8 KiB\n",
      "25/05/06 10:18:25 WARN DAGScheduler: Broadcasting large task binary with size 1941.9 KiB\n",
      "25/05/06 10:18:29 WARN DAGScheduler: Broadcasting large task binary with size 1938.8 KiB\n",
      "25/05/06 10:18:34 WARN DAGScheduler: Broadcasting large task binary with size 1947.5 KiB\n",
      "25/05/06 10:18:37 WARN DAGScheduler: Broadcasting large task binary with size 1939.4 KiB\n",
      "25/05/06 10:18:40 WARN DAGScheduler: Broadcasting large task binary with size 1944.3 KiB\n",
      "25/05/06 10:18:43 WARN DAGScheduler: Broadcasting large task binary with size 1939.4 KiB\n",
      "25/05/06 10:18:47 WARN DAGScheduler: Broadcasting large task binary with size 1948.0 KiB\n",
      "25/05/06 10:18:50 WARN DAGScheduler: Broadcasting large task binary with size 1941.7 KiB\n",
      "25/05/06 10:18:54 WARN DAGScheduler: Broadcasting large task binary with size 1946.6 KiB\n",
      "25/05/06 10:18:57 WARN DAGScheduler: Broadcasting large task binary with size 1941.7 KiB\n",
      "25/05/06 10:19:00 WARN DAGScheduler: Broadcasting large task binary with size 1948.6 KiB\n",
      "25/05/06 10:19:04 WARN DAGScheduler: Broadcasting large task binary with size 1942.1 KiB\n",
      "25/05/06 10:19:08 WARN DAGScheduler: Broadcasting large task binary with size 1947.1 KiB\n",
      "25/05/06 10:19:11 WARN DAGScheduler: Broadcasting large task binary with size 1942.1 KiB\n",
      "25/05/06 10:19:15 WARN DAGScheduler: Broadcasting large task binary with size 1949.8 KiB\n",
      "25/05/06 10:19:18 WARN DAGScheduler: Broadcasting large task binary with size 1942.7 KiB\n",
      "25/05/06 10:19:20 WARN DAGScheduler: Broadcasting large task binary with size 1947.7 KiB\n",
      "25/05/06 10:19:23 WARN DAGScheduler: Broadcasting large task binary with size 1942.7 KiB\n",
      "25/05/06 10:19:27 WARN DAGScheduler: Broadcasting large task binary with size 1951.9 KiB\n",
      "25/05/06 10:19:30 WARN DAGScheduler: Broadcasting large task binary with size 1945.0 KiB\n",
      "25/05/06 10:19:33 WARN DAGScheduler: Broadcasting large task binary with size 1948.9 KiB\n",
      "25/05/06 10:19:36 WARN DAGScheduler: Broadcasting large task binary with size 1945.0 KiB\n",
      "25/05/06 10:19:41 WARN DAGScheduler: Broadcasting large task binary with size 1954.2 KiB\n",
      "25/05/06 10:19:45 WARN DAGScheduler: Broadcasting large task binary with size 1945.6 KiB\n",
      "25/05/06 10:19:48 WARN DAGScheduler: Broadcasting large task binary with size 1951.1 KiB\n",
      "25/05/06 10:19:52 WARN DAGScheduler: Broadcasting large task binary with size 1945.6 KiB\n",
      "25/05/06 10:19:56 WARN DAGScheduler: Broadcasting large task binary with size 1954.7 KiB\n",
      "25/05/06 10:19:59 WARN DAGScheduler: Broadcasting large task binary with size 1946.1 KiB\n",
      "25/05/06 10:20:03 WARN DAGScheduler: Broadcasting large task binary with size 1953.4 KiB\n",
      "25/05/06 10:20:06 WARN DAGScheduler: Broadcasting large task binary with size 1946.1 KiB\n",
      "25/05/06 10:20:09 WARN DAGScheduler: Broadcasting large task binary with size 1955.3 KiB\n",
      "25/05/06 10:20:12 WARN DAGScheduler: Broadcasting large task binary with size 1948.4 KiB\n",
      "25/05/06 10:20:15 WARN DAGScheduler: Broadcasting large task binary with size 1953.9 KiB\n",
      "25/05/06 10:20:19 WARN DAGScheduler: Broadcasting large task binary with size 1948.4 KiB\n",
      "25/05/06 10:20:22 WARN DAGScheduler: Broadcasting large task binary with size 1956.5 KiB\n",
      "25/05/06 10:20:26 WARN DAGScheduler: Broadcasting large task binary with size 1948.9 KiB\n",
      "25/05/06 10:20:29 WARN DAGScheduler: Broadcasting large task binary with size 1954.5 KiB\n",
      "25/05/06 10:20:32 WARN DAGScheduler: Broadcasting large task binary with size 1948.9 KiB\n",
      "25/05/06 10:20:36 WARN DAGScheduler: Broadcasting large task binary with size 1958.9 KiB\n",
      "25/05/06 10:20:39 WARN DAGScheduler: Broadcasting large task binary with size 1949.5 KiB\n",
      "25/05/06 10:20:43 WARN DAGScheduler: Broadcasting large task binary with size 1955.7 KiB\n",
      "25/05/06 10:20:47 WARN DAGScheduler: Broadcasting large task binary with size 1949.5 KiB\n",
      "25/05/06 10:20:52 WARN DAGScheduler: Broadcasting large task binary with size 1961.6 KiB\n",
      "25/05/06 10:20:56 WARN DAGScheduler: Broadcasting large task binary with size 1608.5 KiB\n",
      "25/05/06 10:21:02 WARN DAGScheduler: Broadcasting large task binary with size 1958.1 KiB\n",
      "25/05/06 10:21:05 WARN DAGScheduler: Broadcasting large task binary with size 1608.5 KiB\n",
      "25/05/06 10:21:08 WARN DAGScheduler: Broadcasting large task binary with size 1962.1 KiB\n",
      "25/05/06 10:21:08 WARN DAGScheduler: Broadcasting large task binary with size 1640.3 KiB\n",
      "25/05/06 10:21:13 WARN DAGScheduler: Broadcasting large task binary with size 1960.8 KiB\n",
      "25/05/06 10:21:13 WARN DAGScheduler: Broadcasting large task binary with size 1640.3 KiB\n",
      "25/05/06 10:21:16 WARN DAGScheduler: Broadcasting large task binary with size 1962.7 KiB\n",
      "25/05/06 10:21:16 WARN DAGScheduler: Broadcasting large task binary with size 1641.4 KiB\n",
      "25/05/06 10:21:20 WARN DAGScheduler: Broadcasting large task binary with size 1961.3 KiB\n",
      "25/05/06 10:21:20 WARN DAGScheduler: Broadcasting large task binary with size 1641.4 KiB\n",
      "25/05/06 10:21:24 WARN DAGScheduler: Broadcasting large task binary with size 1963.9 KiB\n",
      "25/05/06 10:21:27 WARN DAGScheduler: Broadcasting large task binary with size 1961.9 KiB\n",
      "25/05/06 10:21:31 WARN DAGScheduler: Broadcasting large task binary with size 1966.3 KiB\n",
      "25/05/06 10:21:34 WARN DAGScheduler: Broadcasting large task binary with size 1963.1 KiB\n",
      "25/05/06 10:21:38 WARN DAGScheduler: Broadcasting large task binary with size 1968.8 KiB\n",
      "25/05/06 10:21:42 WARN DAGScheduler: Broadcasting large task binary with size 1965.2 KiB\n",
      "25/05/06 10:21:45 WARN DAGScheduler: Broadcasting large task binary with size 1969.3 KiB\n",
      "25/05/06 10:21:50 WARN DAGScheduler: Broadcasting large task binary with size 1967.8 KiB\n",
      "25/05/06 10:21:52 WARN DAGScheduler: Broadcasting large task binary with size 1969.9 KiB\n",
      "25/05/06 10:21:56 WARN DAGScheduler: Broadcasting large task binary with size 1968.4 KiB\n",
      "25/05/06 10:21:59 WARN DAGScheduler: Broadcasting large task binary with size 1971.1 KiB\n",
      "25/05/06 10:22:02 WARN DAGScheduler: Broadcasting large task binary with size 1968.9 KiB\n",
      "25/05/06 10:22:06 WARN DAGScheduler: Broadcasting large task binary with size 1973.4 KiB\n",
      "25/05/06 10:22:09 WARN DAGScheduler: Broadcasting large task binary with size 1970.2 KiB\n",
      "25/05/06 10:22:13 WARN DAGScheduler: Broadcasting large task binary with size 1976.0 KiB\n",
      "25/05/06 10:22:16 WARN DAGScheduler: Broadcasting large task binary with size 1972.3 KiB\n",
      "25/05/06 10:22:20 WARN DAGScheduler: Broadcasting large task binary with size 1976.5 KiB\n",
      "25/05/06 10:22:24 WARN DAGScheduler: Broadcasting large task binary with size 1974.9 KiB\n",
      "25/05/06 10:22:27 WARN DAGScheduler: Broadcasting large task binary with size 1977.0 KiB\n",
      "25/05/06 10:22:31 WARN DAGScheduler: Broadcasting large task binary with size 1975.3 KiB\n",
      "25/05/06 10:22:34 WARN DAGScheduler: Broadcasting large task binary with size 1978.2 KiB\n",
      "25/05/06 10:22:37 WARN DAGScheduler: Broadcasting large task binary with size 1976.0 KiB\n",
      "25/05/06 10:22:40 WARN DAGScheduler: Broadcasting large task binary with size 1980.6 KiB\n",
      "25/05/06 10:22:43 WARN DAGScheduler: Broadcasting large task binary with size 1977.1 KiB\n",
      "25/05/06 10:22:48 WARN DAGScheduler: Broadcasting large task binary with size 1983.3 KiB\n",
      "25/05/06 10:22:51 WARN DAGScheduler: Broadcasting large task binary with size 1979.5 KiB\n",
      "25/05/06 10:22:54 WARN DAGScheduler: Broadcasting large task binary with size 1983.7 KiB\n",
      "25/05/06 10:22:59 WARN DAGScheduler: Broadcasting large task binary with size 1982.3 KiB\n",
      "25/05/06 10:23:02 WARN DAGScheduler: Broadcasting large task binary with size 1984.3 KiB\n",
      "25/05/06 10:23:05 WARN DAGScheduler: Broadcasting large task binary with size 1982.8 KiB\n",
      "25/05/06 10:23:09 WARN DAGScheduler: Broadcasting large task binary with size 1985.5 KiB\n",
      "25/05/06 10:23:11 WARN DAGScheduler: Broadcasting large task binary with size 1983.3 KiB\n",
      "25/05/06 10:23:15 WARN DAGScheduler: Broadcasting large task binary with size 1987.6 KiB\n",
      "25/05/06 10:23:18 WARN DAGScheduler: Broadcasting large task binary with size 1984.6 KiB\n",
      "25/05/06 10:23:22 WARN DAGScheduler: Broadcasting large task binary with size 1640.3 KiB\n",
      "25/05/06 10:23:25 WARN DAGScheduler: Broadcasting large task binary with size 1986.9 KiB\n",
      "25/05/06 10:23:25 WARN DAGScheduler: Broadcasting large task binary with size 1641.4 KiB\n",
      "25/05/06 10:23:29 WARN DAGScheduler: Broadcasting large task binary with size 1640.3 KiB\n",
      "25/05/06 10:23:30 WARN DAGScheduler: Broadcasting large task binary with size 1641.4 KiB\n",
      "25/05/06 10:23:30 WARN DAGScheduler: Broadcasting large task binary with size 1630.7 KiB\n",
      "25/05/06 10:23:30 WARN DAGScheduler: Broadcasting large task binary with size 1630.8 KiB\n",
      "25/05/06 10:23:30 WARN DAGScheduler: Broadcasting large task binary with size 1634.9 KiB\n",
      "25/05/06 10:23:31 WARN DAGScheduler: Broadcasting large task binary with size 1895.2 KiB\n",
      "25/05/06 10:23:44 WARN DAGScheduler: Broadcasting large task binary with size 1896.1 KiB\n",
      "25/05/06 10:23:47 WARN DAGScheduler: Broadcasting large task binary with size 1896.8 KiB\n",
      "25/05/06 10:23:51 WARN DAGScheduler: Broadcasting large task binary with size 1898.0 KiB\n",
      "25/05/06 10:23:55 WARN DAGScheduler: Broadcasting large task binary with size 1900.4 KiB\n",
      "25/05/06 10:24:00 WARN DAGScheduler: Broadcasting large task binary with size 1907.8 KiB\n",
      "25/05/06 10:24:06 WARN DAGScheduler: Broadcasting large task binary with size 1908.3 KiB\n",
      "25/05/06 10:24:10 WARN DAGScheduler: Broadcasting large task binary with size 1908.9 KiB\n",
      "25/05/06 10:24:14 WARN DAGScheduler: Broadcasting large task binary with size 1910.1 KiB\n",
      "25/05/06 10:24:18 WARN DAGScheduler: Broadcasting large task binary with size 1912.4 KiB\n",
      "25/05/06 10:24:23 WARN DAGScheduler: Broadcasting large task binary with size 1915.4 KiB\n",
      "25/05/06 10:24:29 WARN DAGScheduler: Broadcasting large task binary with size 1916.0 KiB\n",
      "25/05/06 10:24:33 WARN DAGScheduler: Broadcasting large task binary with size 1916.6 KiB\n",
      "25/05/06 10:24:37 WARN DAGScheduler: Broadcasting large task binary with size 1917.7 KiB\n",
      "25/05/06 10:24:41 WARN DAGScheduler: Broadcasting large task binary with size 1920.1 KiB\n",
      "25/05/06 10:24:46 WARN DAGScheduler: Broadcasting large task binary with size 1922.8 KiB\n",
      "25/05/06 10:24:52 WARN DAGScheduler: Broadcasting large task binary with size 1923.3 KiB\n",
      "25/05/06 10:24:56 WARN DAGScheduler: Broadcasting large task binary with size 1923.9 KiB\n",
      "25/05/06 10:25:00 WARN DAGScheduler: Broadcasting large task binary with size 1925.1 KiB\n",
      "25/05/06 10:25:04 WARN DAGScheduler: Broadcasting large task binary with size 1926.9 KiB\n",
      "25/05/06 10:25:09 WARN DAGScheduler: Broadcasting large task binary with size 1929.2 KiB\n",
      "25/05/06 10:25:15 WARN DAGScheduler: Broadcasting large task binary with size 1929.7 KiB\n",
      "25/05/06 10:25:19 WARN DAGScheduler: Broadcasting large task binary with size 1930.3 KiB\n",
      "25/05/06 10:25:23 WARN DAGScheduler: Broadcasting large task binary with size 1931.5 KiB\n",
      "25/05/06 10:25:27 WARN DAGScheduler: Broadcasting large task binary with size 1933.9 KiB\n",
      "25/05/06 10:25:32 WARN DAGScheduler: Broadcasting large task binary with size 1936.7 KiB\n",
      "25/05/06 10:25:38 WARN DAGScheduler: Broadcasting large task binary with size 1937.2 KiB\n",
      "25/05/06 10:25:42 WARN DAGScheduler: Broadcasting large task binary with size 1937.8 KiB\n",
      "25/05/06 10:25:45 WARN DAGScheduler: Broadcasting large task binary with size 1939.0 KiB\n",
      "25/05/06 10:25:50 WARN DAGScheduler: Broadcasting large task binary with size 1941.4 KiB\n",
      "25/05/06 10:25:54 WARN DAGScheduler: Broadcasting large task binary with size 1944.3 KiB\n",
      "25/05/06 10:26:00 WARN DAGScheduler: Broadcasting large task binary with size 1944.8 KiB\n",
      "25/05/06 10:26:04 WARN DAGScheduler: Broadcasting large task binary with size 1945.4 KiB\n",
      "25/05/06 10:26:08 WARN DAGScheduler: Broadcasting large task binary with size 1946.6 KiB\n",
      "25/05/06 10:26:12 WARN DAGScheduler: Broadcasting large task binary with size 1949.0 KiB\n",
      "25/05/06 10:26:17 WARN DAGScheduler: Broadcasting large task binary with size 1951.9 KiB\n",
      "25/05/06 10:26:23 WARN DAGScheduler: Broadcasting large task binary with size 1952.3 KiB\n",
      "25/05/06 10:26:27 WARN DAGScheduler: Broadcasting large task binary with size 1953.0 KiB\n",
      "25/05/06 10:26:31 WARN DAGScheduler: Broadcasting large task binary with size 1954.1 KiB\n",
      "25/05/06 10:26:35 WARN DAGScheduler: Broadcasting large task binary with size 1956.2 KiB\n",
      "25/05/06 10:26:40 WARN DAGScheduler: Broadcasting large task binary with size 1958.6 KiB\n",
      "25/05/06 10:26:45 WARN DAGScheduler: Broadcasting large task binary with size 1959.1 KiB\n",
      "25/05/06 10:26:49 WARN DAGScheduler: Broadcasting large task binary with size 1959.7 KiB\n",
      "25/05/06 10:26:53 WARN DAGScheduler: Broadcasting large task binary with size 1960.9 KiB\n",
      "25/05/06 10:26:57 WARN DAGScheduler: Broadcasting large task binary with size 1963.2 KiB\n",
      "25/05/06 10:27:02 WARN DAGScheduler: Broadcasting large task binary with size 1966.0 KiB\n",
      "25/05/06 10:27:07 WARN DAGScheduler: Broadcasting large task binary with size 1966.4 KiB\n",
      "25/05/06 10:27:11 WARN DAGScheduler: Broadcasting large task binary with size 1967.0 KiB\n",
      "25/05/06 10:27:15 WARN DAGScheduler: Broadcasting large task binary with size 1968.3 KiB\n",
      "25/05/06 10:27:19 WARN DAGScheduler: Broadcasting large task binary with size 1970.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GBTRegressionModel: uid=GBTRegressor_2054749e2086, numTrees=10, numFeatures=9436"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "grid = ParamGridBuilder()\\\n",
    "    .addGrid(gbt.maxDepth, [3, 5])\\\n",
    "    .addGrid(gbt.minInstancesPerNode, [1, 2])\\\n",
    "    .build()\n",
    "cv = CrossValidator(estimator = gbt, \n",
    "                    estimatorParamMaps = grid, \n",
    "                    evaluator = evaluator2_rmse,\n",
    "                    parallelism = 5,\n",
    "                    numFolds=3)\n",
    "\n",
    "cvModel = cv.fit(train_data)\n",
    "bestModel = cvModel.bestModel\n",
    "bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1524320f-8764-4ed4-b8ff-9d0a43b2ee29",
   "metadata": {},
   "source": [
    "## Best model 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ccafef7f-5a51-4370-8572-447f06b92248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Param(parent='GBTRegressor_2054749e2086', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval.'): False,\n",
      " Param(parent='GBTRegressor_2054749e2086', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext.'): 10,\n",
      " Param(parent='GBTRegressor_2054749e2086', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size.'): 256,\n",
      " Param(parent='GBTRegressor_2054749e2086', name='labelCol', doc='label column name.'): 'label',\n",
      " Param(parent='GBTRegressor_2054749e2086', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.0,\n",
      " Param(parent='GBTRegressor_2054749e2086', name='seed', doc='random seed.'): -6720047389965400372,\n",
      " Param(parent='GBTRegressor_2054749e2086', name='lossType', doc='Loss function which GBT tries to minimize (case-insensitive). Supported options: squared, absolute'): 'squared',\n",
      " Param(parent='GBTRegressor_2054749e2086', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5,\n",
      " Param(parent='GBTRegressor_2054749e2086', name='maxIter', doc='max number of iterations (>= 0).'): 10,\n",
      " Param(parent='GBTRegressor_2054749e2086', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 32,\n",
      " Param(parent='GBTRegressor_2054749e2086', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0,\n",
      " Param(parent='GBTRegressor_2054749e2086', name='stepSize', doc='Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator.'): 0.1,\n",
      " Param(parent='GBTRegressor_2054749e2086', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: variance'): 'variance',\n",
      " Param(parent='GBTRegressor_2054749e2086', name='subsamplingRate', doc='Fraction of the training data used for learning each decision tree, in range (0, 1].'): 1.0,\n",
      " Param(parent='GBTRegressor_2054749e2086', name='predictionCol', doc='prediction column name.'): 'prediction',\n",
      " Param(parent='GBTRegressor_2054749e2086', name='leafCol', doc='Leaf indices column name. Predicted leaf index of each instance in each tree by preorder.'): '',\n",
      " Param(parent='GBTRegressor_2054749e2086', name='featuresCol', doc='features column name.'): 'features',\n",
      " Param(parent='GBTRegressor_2054749e2086', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 2,\n",
      " Param(parent='GBTRegressor_2054749e2086', name='featureSubsetStrategy', doc=\"The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto'\"): 'all',\n",
      " Param(parent='GBTRegressor_2054749e2086', name='validationTol', doc='Threshold for stopping early when fit with validation is used. If the error rate on the validation input changes by less than the validationTol, then learning will stop early (before `maxIter`). This parameter is ignored when fit without validation is used.'): 0.01}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "model2 = bestModel\n",
    "pprint(model2.extractParamMap())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336a3f34-cd0b-4e7e-a0ff-8670bee1e2ef",
   "metadata": {},
   "source": [
    "## Save the model to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "098bb094-67c3-4a8b-a52a-95a1f7c7bc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get: `model2/model1/data/_SUCCESS': File exists                                 \n",
      "get: `model2/model1/metadata/_SUCCESS': File exists\n",
      "get: `model2/model1/metadata/part-00000': File exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.write().overwrite().save(\"project/big_data_project/models/model2\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -get project/big_data_project/models/model2 model2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2220eee3-99d3-44cc-82ab-b941fd5e5e7c",
   "metadata": {},
   "source": [
    "## Predict for test data using best model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "53789d15-33a1-43b2-8e8e-df7627df5de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/06 10:27:40 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "[Stage 4012:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+-----------------+\n",
      "|            features|label|     indexedFeatures|       prediction|\n",
      "+--------------------+-----+--------------------+-----------------+\n",
      "|(9436,[0,3082,309...| 56.0|(9436,[0,3082,309...|87.54431936748031|\n",
      "|(9436,[0,3082,309...| 65.0|(9436,[0,3082,309...|  89.655530427895|\n",
      "|(9436,[0,3082,309...| 99.0|(9436,[0,3082,309...|87.50828110921341|\n",
      "|(9436,[0,3082,309...| 55.0|(9436,[0,3082,309...|79.06578543883641|\n",
      "|(9436,[0,3082,309...|175.0|(9436,[0,3082,309...|87.50828110921341|\n",
      "|(9436,[0,3082,309...| 70.0|(9436,[0,3082,309...|88.36509874556303|\n",
      "|(9436,[0,3082,309...| 98.0|(9436,[0,3082,309...| 89.6194921696281|\n",
      "|(9436,[0,3082,309...| 80.0|(9436,[0,3082,309...| 89.6194921696281|\n",
      "|(9436,[0,3082,309...| 85.0|(9436,[0,3082,309...|78.84892703881185|\n",
      "|(9436,[0,3082,309...| 70.0|(9436,[0,3082,309...| 89.6194921696281|\n",
      "|(9436,[0,3082,309...| 85.0|(9436,[0,3082,309...|79.06578543883641|\n",
      "|(9436,[0,3082,309...| 58.0|(9436,[0,3082,309...|87.54431936748031|\n",
      "|(9436,[0,3082,309...|130.0|(9436,[0,3082,309...|88.36509874556303|\n",
      "|(9436,[0,3082,309...| 83.0|(9436,[0,3082,309...|90.04313469605482|\n",
      "|(9436,[0,3082,309...| 50.0|(9436,[0,3082,309...| 89.6194921696281|\n",
      "|(9436,[0,3082,309...|108.0|(9436,[0,3082,309...|88.36509874556303|\n",
      "|(9436,[0,3082,309...| 65.0|(9436,[0,3082,309...|79.06578543883641|\n",
      "|(9436,[0,3082,309...| 75.0|(9436,[0,3082,309...|79.06578543883641|\n",
      "|(9436,[0,3082,309...| 54.0|(9436,[0,3082,309...|90.04313469605482|\n",
      "|(9436,[0,3082,309...| 50.0|(9436,[0,3082,309...|88.36509874556303|\n",
      "+--------------------+-----+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions = model2.transform(test_data)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "52df5365-5d86-407f-b71a-878fff80b152",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/06 10:27:56 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.select(\"label\", \"prediction\")\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .save(\"project/big_data_project/output/model2_predictions.csv\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -cat project/big_data_project/output/model2_predictions.csv/*.csv > ../output/model2_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f9e394-a65a-4cb3-b9c8-1036fd3a443a",
   "metadata": {},
   "source": [
    "## Evaluate the best model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2e83b832-0088-4e95-95fc-768bc13d70ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/06 11:17:26 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/06 11:17:41 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "[Stage 4112:============================>                           (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 69.88522182315187\n",
      "R^2 on test data = 0.749297634487301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator \n",
    "\n",
    "# Evaluate the performance of the model\n",
    "evaluator2_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator2_r2 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "rmse22 = evaluator2_rmse.evaluate(predictions)\n",
    "r22 = evaluator2_r2.evaluate(predictions)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = {}\".format(rmse2))\n",
    "print(\"R^2 on test data = {}\".format(r22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "517bfb55-f541-4f8e-9e81-7a8b062b6cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save metrics \n",
    "metrics_filename = \"GBT_tune\"\n",
    "metrics_data = [{\"RMSE\": rmse22, \"R2\": r22, \"model\": \"GBT_tune\"}]\n",
    "metrics_df = spark.createDataFrame(metrics_data)\n",
    "metrics_df.write.format(\"json\").mode(\"overwrite\").save(f\"project/big_data_project/output/{metrics_filename}\")\n",
    "run(\"hdfs dfs -get project/big_data_project/output/GBT_tune ../output/model2_tune\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bd7b48-f536-495a-ac38-90ea1b9014af",
   "metadata": {},
   "source": [
    "# Compare best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8b91f3c3-50ca-4ce1-9dfa-dba7449fbc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------+-----------------+------------------+\n",
      "|model                                                                           |RMSE             |R2                |\n",
      "+--------------------------------------------------------------------------------+-----------------+------------------+\n",
      "|LinearRegressionModel: uid=LinearRegression_a9df76969288, numFeatures=9436      |72.84485892524282|0.7276135190670916|\n",
      "|GBTRegressionModel: uid=GBTRegressor_2054749e2086, numTrees=10, numFeatures=9436|69.88522182315187|0.749297634487301 |\n",
      "+--------------------------------------------------------------------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = [[str(model1),rmse1, r21], [str(model2),rmse2, r22]]\n",
    "\n",
    "df = spark.createDataFrame(models, [\"model\", \"RMSE\", \"R2\"])\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e008d30b-9489-44a2-84a0-95bcd9608319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .save(\"project/big_data_project/output/evaluation.csv\")\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -cat project/big_data_project/output/evaluation.csv/*.csv > ../output/evaluation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "09f11a50-6fa4-4221-8537-627038c7a2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b3707816-1ed2-470b-87a7-98d37213fdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sth\n"
     ]
    }
   ],
   "source": [
    "print(\"sth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ab6517-5b00-4ce3-ace2-eeae0dab7620",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc-autonumbering": true,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
